[["はじめに.html", "エコノミストのためのR はじめに なぜRを使うのか？ 本書でできること・できないこと 目次 参考文献・ウェブサイト 変更履歴", " エコノミストのためのR 服部直樹 2023-09-20 はじめに 本書は、エコノミストがRを使用して経済・金融データ分析を行うための方法をまとめたものです。 Rによるプログラミング、データ分析、計量経済学について解説した書籍は、世の中に沢山あります。しかし、エコノミストがよく使用するコードや分析方法を体系的に整理し、いつでも・どこからでもコードにアクセスできるようにしたサービスはまだ無かったので、本書を作成しました。 なお、本書のソースコードはGitHubリポジトリで管理しています。改善点があれば、イシューやプルリクエストでお知らせいただければ幸いです。 なぜRを使うのか？ データ分析を行うことができるソフトウェアは、R以外にも、EViews、Stata、Pythonといった選択肢があります。では、なぜRを使用するのでしょうか。 無料： Rは無料で使用することができます。オフィスでも、自宅でも、所属組織が変わっても同じソフトウェアを使って作業することができるため、わざわざ違うソフトウェアの使用方法を新たに覚える必要がありません。また、無料で誰でも使えるということは、世界中に多くのユーザーがいて、書籍やオンラインに情報が沢山あるということでもあります。本書もその一つです。 豊富なパッケージ： Rはデータ分析を行うための追加パッケージが大量に開発されており、用途に応じてパッケージをインストールすることで、様々なデータ分析手法を使用することが可能です。オーソドックスな計量経済分析はもちろん、機械学習、自然言語処理、地理データ分析など、Rで実施可能な分析は多岐にわたります。 データ分析に特化： RとPythonはよく比較されますが、Pythonが汎用プログラミング言語である一方、Rはデータ分析に特化しているという違いがあります。特に、Rの統合開発環境であるRStudioは、コードの記述と実行に加え、変数の内容を一覧表示したり、グラフをプロットしたり、ディレクトリやヘルプを表示したり、といった分析を補助する機能を一つの画面で操作することができ、ストレスなく作業できます。データ分析を行うのであれば、まずRを選択すべきでしょう。一方、高度な深層学習を行ったり、分析した結果を用いてアプリケーションを作成したりするのであれば、Pythonが向いているといえます。 直感的なデータ操作： Rのパッケージの一つであるtidyverseでは、直感的かつ効率的なデータ操作（データハンドリング）を行うことができます。データセットを読み込んだ後、変数の選択、フィルタ、集計、結合、新たな変数の計算、異常値の処理といった前処理をシームレスに実施し、そのまま図表作成や分析に進むことができます。また、データセットの読み込みも、読み込み元がローカルかウェブかを問わず容易に行うことができます。 強力なグラフ作成機能： Rのggplot2パッケージは強力なグラフ作成機能をもち、折れ線グラフ、棒グラフ、散布図といったおなじみのグラフだけでなく、ヒートマップ、QQプロット、ステップグラフ、ドットプロット、箱ひげ図、ヴァイオリングラフ、ジッターグラフ、複合グラフ（ファセット）などの高度なグラフを作成することができます。グラフは日本語表記にも対応しており、フォーマットを設定すれば、そのままレポートにも使用可能です。 本書でできること・できないこと 本書では、エコノミストがRを使用してデータ分析を行う方法を順序だてて学ぶことができます。 Rの設定、Rの演算やオブジェクトの取り扱い方といった基本的な使用方法、tidyverseパッケージを使ってデータを自由に操作する方法、季節調整やトレンドなどの時系列データ特有の処理、ggplot2パッケージを使ったグラフ作成、データの分析方針を立てるための探索的データ分析（EDA）、計量経済学の基礎である線形単回帰・重回帰について解説しています。すべてRのコードを記載していますので、自分の環境で結果を再現することが可能です。 一方、本書はデータ分析の手法とコードの解説を主な目的にしているため、厳密な数学的解説は行っていません。手法の背景にある数学的解説が必要な場合は、参考文献を参照してください。また、データ分析の手法について、エコノミストが日常業務で使用することが比較的少ない機械学習、深層学習、自然言語処理の手法は、現時点では取り上げていません。 目次 1. Rの設定 Rを使用するにあたり、まず必要な環境構築を行います。 RとRStudioのインストール、バージョン確認、Rの機能を拡張するパッケージのインストールとインポート、グラフ作成の事前設定に加え、本書で使用するサンプルデータセットを紹介します。 また、オフィスや大学などのプロキシ環境下でRを使用する際に必要な通信設定についても解説します。 2. Rの基本的な使用方法 Rで作業を始める前に、プログラミング言語としてのRの基本的な使用方法を簡単に紹介します。 プログラムの実行方法、ショートカットキー、演算方法、ベクトル・行列・データフレーム・リスト、文字列といった各種データの取り扱い方法、if文やforループなどの制御構文、データの読み込み・書き出し方法、オブジェクトのセーブ・ロード方法について解説します。 この章をまず一読したうえで次章以降へ進み、後から必要に応じてリファレンス的に使用するのが良いでしょう。 3. tidyverseによるデータ操作 Rで直感的なデータ操作用を行うパッケージであるtidyverseの使用方法を紹介します。 変数選択や各種条件によるフィルタ、並べ替えなどの基本的な操作に加え、新たな変数の作成、集計、重複処理、欠損値処理、補完処理、サンプリング、縦型・横型の変換といったデータ前処理の手法についても解説します。 読み込んだデータを分析に「使える」ようにするために必須の内容です。 4. ggplot2によるグラフ作成 ggplot2パッケージを主に使用したグラフ作成方法を紹介します。 最初に、GUI形式でどのようなグラフを作成するか検討する方法を紹介したうえで、ggplot2によるコード記述形式のグラフ作成方法を解説します。 取り上げるグラフは、1次元・2次元の度数分布、密度グラフ、ヒストグラム、QQプロット、散布図、バブルチャート、棒グラフ、円グラフ、折れ線グラフ、ステップグラフ、面グラフ、箱ひげ図（ボックスプロット）、ヴァイオリングラフ、ジッターグラフ、ドットプロット、複合グラフ（ファセット）など、多岐にわたります。 また、散布図や折れ線グラフに系列名などのテキストアノテーションを行う方法、グラフの見栄えを整える上で欠かせない色、軸、凡例、フォントなどの設定方法についても紹介しています。 5. 探索的データ分析（EDA） 本格的なデータ分析の方針を立てるために必要な、探索的データ分析（EDA）の方法を紹介します。 具体的には、データの中身や分布、変数間の関係性・相関を一括して確認する方法について解説します。EDAは軽視されがちですが、極めて重要なプロセスであり、データ分析の成否はEDAが決めるといっても過言ではありません。 6. 線形回帰 計量経済分析の最も基本的な手法である線形回帰の内容と、Rによる実行方法を紹介します。 まず、前提として、統計学・計量経済学で使用される基本的な用語を説明します。次に、線形回帰の推定方法である最小2乗法と、統計的仮説検定について解説したうえで、実務で頻繁に用いる水準モデル、対数モデル、変化率モデル、多項式モデル、交互作用モデル、ダミー変数モデルについて、それぞれの内容と係数の解釈方法を説明します。 この部分は、なるべく直感的に理解できるような説明を心がけました。最後に、それらのモデルをRで推定する方法を、西山 他（2019）の実例を解く形で紹介します。 7. 時系列データ操作 経済・金融分析に欠かせない時系列データの操作方法を紹介します。 時系列データ用のts形式に特有の操作、日付型データ関数、ラグ・リード系列や変化率系列の計算方法、頻度変換、季節調整（X13、STL分解）、トレンド（線形トレンド、区分線形トレンド、HPフィルタ、一般化加法モデル）、構造変化などのトピックを取り上げています。 8. 時系列分析 時系列データを用いた基本的な分析手法である、ボックス＝ジェンキンス法とVARモデルを紹介します。 時系列分析を行う上で必須の定常性、単位根、系列相関、共和分の概念と検定方法を解説し、それらを用いた分析手法として、ARMA・ARIMAモデルなどの1変数時系列モデルであるボックス＝ジェンキンス法と、多変数時系列モデルであるVARモデルを紹介します。 時系列分析はエコノミストが経済・金融分析を行う上で欠かせないツールであり、本書では、単位根検定や共和分検定のシステマチックな検定フロー、構造VAR・VECモデル、一般化インパルス応答など、入門・中級のテキストが十分カバーしていないテーマを取り扱っています。 9. パネルデータ分析 現在作成中です。 参考文献・ウェブサイト Rプログラミング 入門・初級 馬場真哉（2020）『R言語ではじめるプログラミングとデータ分析』ソシム株式会社 松村優哉、湯谷啓明、紀ノ定保礼、前田和寛（2021）『改訂2版 Rユーザのための RStudio［実践］入門ーtidyverseによるモダンな分析フローの世界ー』技術評論社 中級 ハドリー・ウィッカム、ギャレット・グロールマンド（2017）『Rではじめるデータサイエンス』オライリー・ジャパン ウィッカム、グロールマンド（2017）の原著版 “R for Data Science”は、こちらのウェブサイトで全文閲覧できます。 松村優哉、瓜生真也、吉村広志（2022）『Rユーザのためのtidymodels［実践］入門ーモダンな統計・機械学習モデリングの世界ー』技術評論社 統計学・計量経済学 入門・初級 山本勲（2015）『実証分析のための計量経済学ー正しい手法と結果の読み方ー』中央経済社 中級 Hanck, C., Arnold, M., Gerber, A., and Schmelzer, M., (2021) “Introduction to Econometrics with R” 仲嶋亮（2020）「「誘導型推定」v.s.「構造型推定」」 経済セミナー編集部 編『新版 進化する経済学の実証分析』日本評論社 西山慶彦、新谷元嗣、川口大司、奥井亮（2019）『計量経済学』有斐閣 西山 他（2019）の実証例のRコードは、北川梨津（2020）「西山 他『計量経済学』のためのR」が提供しています。 マクリン謙一郎（2022）「再現性問題における統計学の役割と責任」日本評論社『経済セミナー』2022年6・7月号 通巻 726号 上級 北岡孝義、高橋青天、溜川健一、矢野順治（2013）『EViewsで学ぶ実証分析の方法』日本評論社 末石直也（2015）『計量経済学ーミクロデータ分析へのいざないー』日本評論社 時系列データ分析 入門・初級 熊本方雄（2022）「金融時系列データ分析を用いた論文の書き方」 経済セミナー編集部 編『経済論文の書き方』日本評論社 第9章 横内大介、青木義充（2014）『時系列データ分析』技術評論社 中級 Hyndman, R.J., and Athanasopoulos, G. (2018) “Forecasting: principles and practice, 2nd edition”, OTexts: Melbourne, Australia. アイリーン・ニールセン（2021）『実践 時系列解析ー統計と機械学習による予測ー』オライリー・ジャパン Sax, C., and Eddelbuettel, D., (2018) “Seasonal Adjustment by X-13ARIMA-SEATS in R” 奥本佳伸（2016）「季節調整法プログラム センサス局法X-13ARIMA-SEATSを日本のいくつかの経済統計データに適用した結果とその検討」千葉大学『経済研究』第30巻第4号 馬場真哉（2018）『時系列分析と状態空間モデルの基礎ーRとStanで学ぶ理論と実装ー』プレアデス出版 肥後雅博、中田（黒田）祥子（1998）「経済変数から基調的変動を抽出する時系列手法について」日本銀行金融研究所『金融研究』1998.12 上級 ウォルター・エンダース（2019）『実証のための計量時系列分析（原著第4版）』有斐閣 訳者の新谷元嗣、藪友良による本書日本語版のサポートページには、練習問題の解答例、データのExcelファイルに加えて、EViewsを用いて教科書の内容を再現するための資料が掲載されています。また、RのコードはDavid Gabauerのウェブサイトで提供されています。 宮尾龍蔵（2006）『マクロ金融政策の時系列分析ー政策効果の理論と実証ー』日本経済新聞出版 村尾博（2019）『Rで学ぶVAR実証分析ー時系列分析の基礎から予測までー』オーム社 村尾（2019）の内容のエッセンスやRコードは村尾氏個人のウェブサイトで閲覧することができます。まずはウェブサイトを確認してから書籍を購入することをおすすめします。 変更履歴 2023年6月18日 メジャーアップデート 第8章「時系列分析」を追加。 2023年6月18日 マイナーアップデート 第1章「Rの設定」の「プロキシサーバーの設定」を「通信関連の設定」に変更し、「SSL設定」を追加。 第1章「Rの設定」の「パッケージ」に、fppパッケージを追加。「パッケージ」に「インストール時のエラー」の項目を追加。 第1章「Rの設定」の「サンプルデータセット」に、fppパッケージのusconsumptionデータセットを追加。 第2章「Rの基本的な使用方法」の「データの書き出し」に、write.csv()関数で出力したCSVファイルが文字化けする際の対処方法に関する記述を追加。 第7章「時系列データ操作」の「日付型データ関数」に、「日付型データの計算」の項目を追加。 2023年4月6日 マイナーアップデート 参考文献にエンダース（2019）と、村尾（2019）のサポートページを追加。 第1章「Rの設定」の「プロキシサーバーの設定」に、Sys.getenv()関数を追加。「パッケージ」に、stringiパッケージを追加。 第2章「Rの基本的な使用方法」の「リスト」に、リスト要素の追加に関する記述を追加。「文字列操作」に、文字列の変換と文字列によるプログラム実行に関する記述を追加。「データの読み込み」と「データの書き出し」を分割し、「データの読み込み」に、複数データの読み込みとExcelの日付値の修正に関する記述を追加。 第4章「ggplot2によるグラフ作成」の「凡例」に、labs()関数を追加。「グラフの保存」のheightを9.00から8.50に修正。 各章で、参考文献の「西山 他（2019）」を誤って記載していた箇所を修正。 2023年3月18日 マイナーアップデート 参考文献の分類方法を「Rプログラミング」、「統計学・計量経済学」、「時系列データ分析」の3つに再編。 参考文献に、北岡 他（2013）、熊本（2022）、仲嶋（2020）、馬場（2018）、松村 他（2022）、宮尾（2006）を追加。 第1章「Rの設定」の「パッケージ」に、ggfortify、forecast、tseriesパッケージを追加。 第1章「Rの設定」の「サンプルデータセット」に、Seatbeltsを追加し、その他のサンプルデータセットの説明内容を一部修正。 第2章「Rの基本的な使用方法」の「主要ショートカットキー」に、Macのキーを追加。 第2章「Rの基本的な使用方法」の「データの読み込み・書き出し」に、readr::read_delim()関数を追加。 2023年2月6日 マイナーアップデート 参考文献にNielsen, A. (2021) 『実践 時系列解析』を追加。 第1章「Rの設定」の「パッケージ」に、lmtest、sandwich、vars、urcaパッケージを追加し、seasonalviewパッケージを削除。 第1章「Rの設定」の「グラフのフォント設定」に、Macの設定を追加。また、Windowsの設定に、theme_set(theme_light(base_family = \"YUGO\"))を追加。以降のすべての章のコードチャンクでWindowsの設定をコメントアウト。 第4章「ggplot2によるグラフ作成」で、family = \"YUGO\"を削除。 第6章「線形回帰」で、線形回帰モデルの推定に使用する関数をestimatr::lm_robust()関数からstats::lm()関数とlmtest::coeftest()関数の組み合わせに変更。 第7章「時系列データ操作」の「トレンド」に、HPフィルタの結果をプロットするコードを追加。 2022年10月22日 マイナーアップデート 複数行にわたる関数のコーディングスタイルを変更。 第2章「Rの基本的な使用方法」の「制御構文」に、nextによる処理のスキップを追加。 第3章「tidyverseによるデータ操作」の「列の作成・修正」に、文字列型の列を因子型に変換するコードを追加。「集計」から「カウント」を独立させ、要素の種類の数をカウントするコードを追加。「重複処理」に、重複削除のパターンを追加。 第5章「探索的データ分析」の「データの中身（記述統計量）」に、文字列型の変数を因子型に変換して要素数を出力するコードを追加。 第6章「線形回帰」の「最小2乗法（OLS）」で、最小2乗法の仮定にとp値に関する記述を修正。「Rの単回帰モデル推定」に、実証例4.1の回帰診断プロット作成方法を追加。 第6章「線形回帰」の「Rの重回帰モデル推定」で、西村 実証例5.7の説明を追加。 第7章「時系列データ操作」に、「時系列データとは」を項目を追加。 第7章「時系列データ操作」の「日付型データ関数」に、四半期ベースの時系列インデックスに関する記述を追加。 2022年9月7日 マイナーアップデート 章立てを変更。「時系列データ操作」を「tidyverseによるデータ操作」の後から「線形回帰」の後に移動させ、第1章「Rの設定」、第2章「Rの基本的な使用方法」、第3章「tidyverseによるデータ操作」、第4章「ggplot2によるグラフ作成」、第5章「探索的データ分析」、第6章「線形回帰」、第7章「時系列データ操作」とした。 第1章「Rの設定」の「パッケージ」に、tsibble、strucchange、nycflights13の各パッケージを追加。 第6章「線形回帰」の「Rの単回帰モデル推定」に、estimatr::lm_robust()関数で標準誤差の種類を指定する方法としてHC3に関する記述、lm()関数とestimatr::lm_robust()関数の違いに関する記述を追加。 第7章「時系列データ操作」に、「ts形式データ」、「ts形式データの変換」、「時系列関数」、「構造変化」の項目を追加。 第7章「時系列データ操作」の「トレンド推定」に、線形トレンドと区分線形トレンドに関する記述を追加。 2022年7月17日 マイナーアップデート 第1章「Rの設定」の「サンプルデータセット」に、日本の四半期別GDPを追加。 第2章「Rの基本的な使用方法」の「データフレーム」に、str()関数を追加。 第3章「tidyverseによるデータ操作」に、「行のスライスとサンプリング」の項目を追加。加えて、サンプリングに使用する関数をdplyr::sample_*()関数ファミリーからdplyr::slice_sample()関数に変更。 第3章「tidyverseによるデータ操作」の「行のフィルタ」に、複数列の一括条件指定を追加。 第3章「tidyverseによるデータ操作」の「集計」に、要素数のカウントを追加。 第5章「ggplot2によるグラフ作成」で、「補助線」、「設定：パネル目盛線」、「設定：凡例」、「設定：タイトル・キャプション」、「設定：フォント・マージン」をセクションとして独立。 第5章「ggplot2によるグラフ作成」の「実例」に、折れ線グラフと積み上げ棒グラフを追加。 2022年7月9日 マイナーアップデート 第1章「Rの設定」の「R起動時に実行する設定」に、.Rprofileファイルに関する記述を追加。 第2章「Rの基本的な使用方法」に、「第2章の準備」、「文字列操作」、「ファイル操作」の項目を追加。 第7章「線形回帰」の「最小2乗法（OLS）」に、HAC標準誤差に関する記述を追加。 「変更履歴」を追加。 2022年6月26日 ローンチ 第1章「Rの設定」、第2章「Rの基本的な使用方法」、第3章「tidyverseによるデータ操作」、第4章「時系列データ操作」、第5章「ggplot2によるグラフ作成」、第6章「探索的データ分析」、第7章「線形回帰」を公開。 "],["rの設定-1.html", "1 Rの設定 1.1 設定全般 1.2 通信関連の設定 1.3 パッケージ 1.4 グラフの設定 1.5 サンプルデータセット 1.6 R起動時に実行する設定", " 1 Rの設定 第1章「Rの設定」では、Rを使用する際の各種設定について解説します。Rのインストール、プロキシサーバーの設定、パッケージのインストール・インポート、グラフの設定など、Rの使用に欠かせない様々な設定について記載しています。 また、本書で使用するサンプルデータセットも、この章で紹介しています。特に、ウェブから読み込む外部データセットについては、この章で取得方法を記載していますので、確認してください。 この章の最後に、Rを起動時に実行すべき設定をまとめています。実際にRを使用する際にご利用ください。 1.1 設定全般 1.1.1 Rのインストール RはThe Comprehensive R Archive Networkからダウンロードしてインストールしてください。2020年にバージョン4へメジャーアップデートが行われており、本書でもバージョン4を使用しています。 Rの統合開発環境であるRStudioは、開発元のrstudio.comからインストールできます。上部メニューのDOWNLOADからダウンロードページに進み、RStudio DesktopのFree版をダウンロードしてインストールしてください。 Windows環境でRの一部のパッケージを使用する際、Rtoolsというツールが必要な場合がありますので、こちらのウェブサイトからRのバージョンに合ったものをダウンロードしてインストールします。 1.1.2 Rのバージョン確認 Rのバージョン情報を出力します。 version ## _ ## platform aarch64-apple-darwin20 ## arch aarch64 ## os darwin20 ## system aarch64, darwin20 ## status ## major 4 ## minor 2.2 ## year 2022 ## month 10 ## day 31 ## svn rev 83211 ## language R ## version.string R version 4.2.2 (2022-10-31) ## nickname Innocent and Trusting 1.1.3 セッション情報の確認 Rのバージョンに加え、使用環境、パッケージ情報を出力します。 sessionInfo() ## R version 4.2.2 (2022-10-31) ## Platform: aarch64-apple-darwin20 (64-bit) ## Running under: macOS Ventura 13.4.1 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] rstudioapi_0.14 magrittr_2.0.3 knitr_1.41 MASS_7.3-58.1 ## [5] lattice_0.20-45 R6_2.5.1 rlang_1.0.6 fastmap_1.1.0 ## [9] stringr_1.5.0 tools_4.2.2 grid_4.2.2 nlme_3.1-160 ## [13] xfun_0.35 urca_1.3-3 cli_3.4.1 jquerylib_0.1.4 ## [17] htmltools_0.5.4 vars_1.5-6 yaml_2.3.6 lmtest_0.9-40 ## [21] digest_0.6.31 lifecycle_1.0.3 bookdown_0.31 vctrs_0.5.1 ## [25] sass_0.4.4 glue_1.6.2 cachem_1.0.6 strucchange_1.5-3 ## [29] evaluate_0.19 rmarkdown_2.19 sandwich_3.0-2 stringi_1.7.8 ## [33] compiler_4.2.2 bslib_0.4.2 jsonlite_1.8.4 zoo_1.8-11 1.1.4 オブジェクトを全削除 変数をすべて削除してワークスペースを初期化します。 rm(list = ls()) 1.1.5 警告の非表示 Rが出力する警告（Warning）を非表示にします。 options(warn = -1) 1.1.6 関数が出力する警告やメッセージの非表示 関数が出力する警告やメッセージはoptions(warn = -1)で非表示にできないため、関数毎に、必要に応じてsuppressWarnings()関数やsuppressMessages()関数を使用します。 suppressWarnings(警告を出力する関数) suppressMessages(メッセージを出力する関数) 1.2 通信関連の設定 1.2.1 プロキシ設定 オフィスや大学などのプロキシ環境ではプロキシサーバーの設定を行ってください。なお、下記のコードは認証プロキシには非対応です。 # プロキシサーバーとポートを記入 proxy_url &lt;- &quot;http://proxyserver:port/&quot; # Rのシステム環境変数を設定 Sys.setenv(&quot;http_proxy&quot; = proxy_url) Sys.setenv(&quot;https_proxy&quot; = proxy_url) # Rのダウンロードオプションを設定 options(download.file.method = &quot;libcurl&quot;) options(timeout = NA) 設定したプロキシサーバーの値を確認するには、次のコードを実行します。 Sys.getenv(c(&quot;http_proxy&quot;, &quot;https_proxy&quot;)) ## http_proxy https_proxy ## &quot;&quot; &quot;&quot; 1.2.2 SSL設定 Windows版のR 4.2以降のバージョンでは、ファイルのダウンロードにおける通信方法が変更されたことが影響し、主にプロキシ環境でSSLエラーが生じHTTPSサイトにアクセスできないことがあります。 そうしたSSLエラーが生じた場合は、次のコマンドを実行することでエラーが解消する可能性があります。詳細はRのディベロッパー用ウェブサイトにおける2022年8月6日のニュースを参照してください。 Sys.setenv(&quot;R_LIBCURL_SSL_REVOKE_BEST_EFFORT&quot; = TRUE) 1.3 パッケージ 1.3.1 使用するパッケージ一覧 tidyverse系 lubridate：日付処理 magrittr：パイプ処理 stringi：文字列操作 tidyverse：モダンなデータ分析用パッケージセット tsibble：ts形式の時系列データをデータフレーム形式で処理 readxl：Excelファイルの読み込み 図表系 esquisse：shinyを使用したGUI形式の直感的な図表作成 geofacet：地図形式の複合グラフ（ファセット）配置 ggfortify：統計解析結果の可視化 ggplotgui：shinyを使用したGUI形式の直感的な図表作成 ggpubr：論文形式の図表作成 ggsci：科学系論文の雑誌別カラーパレット ggrepel：散布図のラベル付与 hexbin：geom_hex()関数を使用するために必要なパッケージ lemon：複合グラフ（ファセット）の軸・目盛り表示 RColorBrewer：カラーパレット 統計系 corrplot：相関係数行列の計算・可視化 corrr：相関係数行列の計算・可視化 DataExplorer：探索的データ分析の一括実行 estimatr：線形回帰モデルの推定 forecast：単変量時系列データ分析 GGally：ペアプロットなどのデータ可視化 lmtest：線型回帰モデルの検定や診断 mFilter：HPフィルタ mgcv：一般化加法モデル（GAM） plm：パネルデータモデル psych：心理統計パッケージ sandwich：線型回帰モデルの標準誤差 seasonal：コマンド形式の季節調整 sigmoid：シグモイド関数 SmartEDA：探索的データ分析の一括実行 strucchange：構造変化の検定 tidyquant：金融時系列データ分析 tseries：時系列分析用の関数 vars：VARモデル urca：単位根検定・共和分検定 データパッケージ fpp：様々なサンプルデータセットを格納したパッケージ fpp2：様々なサンプルデータセットを格納したパッケージ nycflights13：航空便情報サンプルデータセット その他 docstring：関数の説明文を作成 openxlsx：Excelのxlsxファイルの読み込み・編集・書き出し pblapply：プログレスバーを表示するapply()関数ファミリー 1.3.2 インストールとインポート パッケージのインストールはinstall.packages()関数、インストールしたパッケージのインポート（呼び出し）はlibrary()関数で実行します。一度インストールすれば、その後はインポートするだけでパッケージを使用することができます。 なお、パッケージのインストールの際、ウィンドウやコンソールに “Do you want to install from sources the package which needs compilation?” というメッセージが出ることがあります。その場合は、まず “No” を選択し、それでもうまくいかなければ “Yes” を選択してください。 # パッケージ一覧 packages &lt;- c( # tidyverse系 &quot;lubridate&quot;, &quot;magrittr&quot;, &quot;stringi&quot;, &quot;tidyverse&quot;, &quot;tsibble&quot;, &quot;readxl&quot;, # 図表系 &quot;esquisse&quot;, # Rバージョン3系では実行不可のためコメントアウトする &quot;geofacet&quot;, &quot;ggfortify&quot;, &quot;ggplotgui&quot;, # Rバージョン3系では実行不可のためコメントアウトする &quot;ggpubr&quot;, &quot;ggsci&quot;, &quot;ggrepel&quot;, &quot;hexbin&quot;, &quot;lemon&quot;, &quot;RColorBrewer&quot;, # 統計系 &quot;corrplot&quot;, &quot;corrr&quot;, &quot;DataExplorer&quot;, &quot;estimatr&quot;, &quot;forecast&quot;, &quot;GGally&quot;, &quot;lmtest&quot;, &quot;mFilter&quot;, &quot;mgcv&quot;, &quot;plm&quot;, &quot;psych&quot;, # Rバージョン3系では実行不可のためコメントアウトする &quot;sandwich&quot;, &quot;seasonal&quot;, &quot;sigmoid&quot;, &quot;SmartEDA&quot;, &quot;strucchange&quot;, &quot;tidyquant&quot;, &quot;tseries&quot;, &quot;vars&quot;, &quot;urca&quot;, # データパッケージ &quot;fpp&quot;, &quot;fpp2&quot;, &quot;nycflights13&quot;, # その他 &quot;docstring&quot;, &quot;openxlsx&quot;, &quot;pbapply&quot; ) # インストールしていないパッケージがあればインストール new_packages &lt;- packages[!(packages %in% installed.packages()[,&quot;Package&quot;])] if(length(new_packages)) { install.packages(new_packages) } # パッケージをインポート for (pkg in packages) { library(pkg, character.only = TRUE) } # 変数を削除 rm( new_packages, packages, pkg ) Rのパッケージは開発者が随時アップデートしていますが、ユーザーの環境にパッケージのアップデートが自動で反映されることはありません。パッケージをアップデートする場合は、次のコードを実行してください。 # パッケージをすべてアップデート update.packages() # アップデートの対象になる古いバージョンのパッケージ一覧を出力 old.packages() # パッケージを指定してアップデート install.packages(&quot;tidyverse&quot;) 1.3.3 インストール時のエラー パッケージをインストールする際に、unable to accessやcannot open URLといったエラーメッセージが表示され、パッケージがインストールできないことがあります（特に、プロキシ環境下でこうしたエラーが生じやすいようです）。こうしたエラーが発生する場合、パッケージのインストールに関する通信設定を変更すると、エラーが解消する可能性があります。 RStudioのファイルメニュー &gt; Tools &gt; Global Options を選択し、Options画面でPackagesをクリックし、Managementタブの下の方にある「Use secure download method for HTTP」のチェックボックスを外してください。 詳細は、RStudioコミュニティのイシューを参照してください。 1.4 グラフの設定 レポートやスライドに掲載するためのグラフは、主にRのggplot2パッケージで作成します。 1.4.1 グラフのテーマ設定 本書では、実務上Excelと併用することを想定しているため、Excelで出力されるグラフに似たテーマであるtheme_light()を設定します。ggplot2パッケージにデフォルトで用意されている様々なテーマについては、公式ウェブサイトを参照してください。 theme_set(theme_light()) 1.4.2 グラフのフォント設定 Windows上でRを使用する場合は、日本語フォントをグラフ上で表示するために以下の設定が必要です。ここでは代表的なフォントとしてMeiryo UIとYu Gothic UIを設定します。それぞれのフォントにMEIRYO、YUGOというキーを割り当て、ggplot2パッケージでグラフを作成する際にキーを指定します。 windowsFonts(&quot;MEIRYO&quot; = windowsFont(&quot;Meiryo UI&quot;)) windowsFonts(&quot;YUGO&quot; = windowsFont(&quot;Yu Gothic UI&quot;)) theme_set(theme_light(base_family = &quot;YUGO&quot;)) Mac上でRを使用する場合は、上記のtheme_set()関数の中でフォントを指定します。 theme_set(theme_light(base_family = &quot;HiraKakuProN-W3&quot;)) もしも上記のフォント設定がうまく機能しない場合は、こちらのウェブサイトを参照して解決するか試してください。 1.5 サンプルデータセット 本書で使用するサンプルデータセットの一覧です。Rの本体もしくはパッケージから呼び出せるデータセットと、ウェブから取得する外部データセットがあります。 1.5.1 Rのサンプルデータセット Rのサンプルデータセットは、help(データセット名)を実行するとヘルプ画面で詳細を確認できます。 iris datasetsパッケージに含まれるirisデータセットは、データ分析で良く使用される有名なデータです。3種類のアヤメの萼（がく）と花弁の長さ・幅のデータを格納しています。データ構造はdata.frame形式です。 # データの先頭行をコンソールに出力 head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa mtcars datasetsパッケージに含まれるmtcarsデータセットは、米国で1973〜1974年に発売された32車種のデザインや諸元に関するデータです。燃費、排気量、馬力、重量などのデータが格納されています。データ構造はdata.frame形式です。 # データの先頭行をコンソールに出力 head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 mpg ggplot2パッケージに含まれるmpgデータセットは、米国で1999～2008年に発売された38車種の燃費関連データです。自動車メーカー名、モデル名、排気量、都市部と高速道路の燃費などのデータが格納されています。データ構造はtibble形式です。 データ出所リンク mpg ## # A tibble: 234 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto… f 18 29 p comp… ## 2 audi a4 1.8 1999 4 manu… f 21 29 p comp… ## 3 audi a4 2 2008 4 manu… f 20 31 p comp… ## 4 audi a4 2 2008 4 auto… f 21 30 p comp… ## 5 audi a4 2.8 1999 6 auto… f 16 26 p comp… ## 6 audi a4 2.8 1999 6 manu… f 18 26 p comp… ## 7 audi a4 3.1 2008 6 auto… f 18 27 p comp… ## 8 audi a4 quattro 1.8 1999 4 manu… 4 18 26 p comp… ## 9 audi a4 quattro 1.8 1999 4 auto… 4 16 25 p comp… ## 10 audi a4 quattro 2 2008 4 manu… 4 20 28 p comp… ## # … with 224 more rows diamonds ggplot2パッケージに含まれるdiamondsデータセットは、約54,000個のダイヤモンドの価格、カラット、色、大きさなどの情報を格納しています。データ構造はtibble形式です。 diamonds ## # A tibble: 53,940 × 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 ## 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 ## 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 ## 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 ## 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 ## # … with 53,930 more rows economics ggplot2パッケージに含まれるeconomicsデータセットは、米国の消費・雇用関連の時系列データです（月次、1967年7月〜2015年4月）。日付（date）、個人消費支出（pce）、人口（pop）、個人貯蓄率（psavert）、失業期間の中央値（uempmed）、失業者数（unemploy）のデータが格納されています。データ構造はtibble形式です。 なお、各系列の出所は米国のセントルイス連銀が運営するウェブサイトのFREDを参照してください。 個人消費支出 人口 個人貯蓄率 失業期間の中央値 失業者数（unemploy） economics ## # A tibble: 574 × 6 ## date pce pop psavert uempmed unemploy ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1967-07-01 507. 198712 12.6 4.5 2944 ## 2 1967-08-01 510. 198911 12.6 4.7 2945 ## 3 1967-09-01 516. 199113 11.9 4.6 2958 ## 4 1967-10-01 512. 199311 12.9 4.9 3143 ## 5 1967-11-01 517. 199498 12.8 4.7 3066 ## 6 1967-12-01 525. 199657 11.8 4.8 3018 ## 7 1968-01-01 531. 199808 11.7 5.1 2878 ## 8 1968-02-01 534. 199920 12.3 4.5 3001 ## 9 1968-03-01 544. 200056 11.7 4.1 2877 ## 10 1968-04-01 544 200208 12.3 4.6 2709 ## # … with 564 more rows unemp seasonalパッケージに含まれるunempデータセットは、米国の失業者数（単位：千人、季節調整前の原数値）の時系列データです（月次、1990年1月〜2016年11月）。データ構造はts形式です。 データ出所リンク # データの先頭行をコンソールに出力 head(seasonal::unemp) ## Jan Feb Mar Apr May Jun ## 1990 7413 7296 6852 6620 6533 6884 usconsumption fppパッケージに含まれるusconsumptionデータセットは、米国のマクロ経済時系列データ（四半期、1970年1-3月期〜2010年10-12月期）です。実質個人消費支出（consumption、前期比、％）、実質可処分所得（income、前期比、％）のデータが格納されています。データ構造はts形式です。 # データの先頭行をコンソールに出力 head(fpp::usconsumption) ## consumption income ## 1970 Q1 0.6122769 0.496540 ## 1970 Q2 0.4549298 1.736460 ## 1970 Q3 0.8746730 1.344881 ## 1970 Q4 -0.2725144 -0.328146 ## 1971 Q1 1.8921870 1.965432 ## 1971 Q2 0.9133782 1.490757 gasoline fpp2パッケージに含まれるgasolineデータセットは、米国のガソリン生産量（単位：百万バレル／日）の時系列データです（週次、1991年2月2日〜2017年１月20日）。データ構造はts形式です。 # データの先頭行をコンソールに出力 head(fpp2::gasoline) ## Time Series: ## Start = 1991.1 ## End = 1991.19582477755 ## Frequency = 52.1785714285714 ## [1] 6.621 6.433 6.582 7.224 6.875 6.947 Canada varsパッケージに含まれるCanadaデータセットは、カナダのマクロ経済時系列データ（四半期、1980年1-3月期〜2000年10-12月期）です。雇用者数（e、自然対数×100）、実質労働生産性（prod、自然対数×100）、実質賃金（rw、自然対数×100）、失業率（U、％）のデータが格納されています。データ構造はts形式です。 # データを呼び出し data(Canada) # データの先頭行をコンソールに出力 head(Canada) ## e prod rw U ## 1980 Q1 929.6105 405.3665 386.1361 7.53 ## 1980 Q2 929.8040 404.6398 388.1358 7.70 ## 1980 Q3 930.3184 403.8149 390.5401 7.47 ## 1980 Q4 931.4277 404.2158 393.9638 7.27 ## 1981 Q1 932.6620 405.0467 396.7647 7.37 ## 1981 Q2 933.5509 404.4167 400.0217 7.13 Seatbelts datasetsパッケージに含まれるSeatbeltsデータセットは、英国の交通事故死者数の時系列データです（月次、1969年1月〜1984年12月）。前席における死傷者数（front）、ガソリン価格（PetrolPrice）、前席のシートベルト着用を義務付ける法律の施行有無を表すフラグ（law）などのデータが格納されています。データ構造はts形式です。 head(Seatbelts) ## DriversKilled drivers front rear kms PetrolPrice VanKilled law ## Jan 1969 107 1687 867 269 9059 0.1029718 12 0 ## Feb 1969 97 1508 825 265 7685 0.1023630 6 0 ## Mar 1969 102 1507 806 319 9963 0.1020625 12 0 ## Apr 1969 87 1385 814 407 10955 0.1008733 8 0 ## May 1969 119 1632 991 454 11823 0.1010197 10 0 ## Jun 1969 106 1511 945 427 12391 0.1005812 13 0 1.5.2 外部データセット ウェブから外部データセットを読み込みます。オフィスや大学などのプロキシ環境では、次のプロキシサーバーの設定を実行してください。 # プロキシサーバーとポートを記入 proxy_url &lt;- &quot;http://proxyserver:port/&quot; # Rのシステム環境変数を設定 Sys.setenv(&quot;http_proxy&quot; = proxy_url) Sys.setenv(&quot;https_proxy&quot; = proxy_url) # Rのダウンロードオプションを設定 options(download.file.method = &quot;libcurl&quot;) options(timeout = NA) OWIDのCOVID-19データセット Our World in Dataの新型コロナウイルス関連データセットです。データの詳細はOur World in Dataのウェブサイトを参照してください。 Hannah Ritchie, Edouard Mathieu, Lucas Rodés-Guirao, Cameron Appel, Charlie Giattino, Esteban Ortiz-Ospina, Joe Hasell, Bobbie Macdonald, Diana Beltekian and Max Roser (2020) - “Coronavirus Pandemic (COVID-19)”. Published online at OurWorldInData.org. Retrieved from: ‘https://ourworldindata.org/coronavirus’ [Online Resource] # Our World in Dataの新型コロナデータをtibble形式で読み込み data_owid &lt;- readr::read_csv(file = &quot;https://covid.ourworldindata.org/data/owid-covid-data.csv&quot;, # ファイルパス／URL col_names = TRUE, # ヘッダー（列名データ）の有無 col_types = NULL, # 各列の型の指定（c：文字列型、d：数値型、D：日付型、l：論理値型） skip = 0) # 読み込み時に上からスキップする行数 # 使用するデータを絞り込み data_owid %&lt;&gt;% dplyr::select(continent, location, date, total_cases, new_cases, new_cases_smoothed, total_deaths, new_deaths, total_cases_per_million, new_cases_per_million, total_deaths_per_million, new_deaths_per_million, people_fully_vaccinated, stringency_index) #%&gt;% #dplyr::filter(location %in% c(&quot;Japan&quot;, &quot;United States&quot;, &quot;United Kingdom&quot;, &quot;Germany&quot;, &quot;France&quot;, &quot;Italy&quot;)) # データをコンソールに出力 data_owid ## # A tibble: 341,376 × 14 ## continent location date total…¹ new_c…² new_c…³ total…⁴ new_d…⁵ total…⁶ ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Asia Afghani… 2020-01-03 NA 0 NA NA 0 NA ## 2 Asia Afghani… 2020-01-04 NA 0 NA NA 0 NA ## 3 Asia Afghani… 2020-01-05 NA 0 NA NA 0 NA ## 4 Asia Afghani… 2020-01-06 NA 0 NA NA 0 NA ## 5 Asia Afghani… 2020-01-07 NA 0 NA NA 0 NA ## 6 Asia Afghani… 2020-01-08 NA 0 0 NA 0 NA ## 7 Asia Afghani… 2020-01-09 NA 0 0 NA 0 NA ## 8 Asia Afghani… 2020-01-10 NA 0 0 NA 0 NA ## 9 Asia Afghani… 2020-01-11 NA 0 0 NA 0 NA ## 10 Asia Afghani… 2020-01-12 NA 0 0 NA 0 NA ## # … with 341,366 more rows, 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​total_cases, ²​new_cases, ³​new_cases_smoothed, ## # ⁴​total_deaths, ⁵​new_deaths, ⁶​total_cases_per_million 日本の産業別就業者数 総務省が公表する労働力調査の長期時系列データのうち、2002年1月以降の産業別就業者数（表番号：1-c-3）を格納したデータセットです。詳細はe-Statの該当ページを参照してください。 # 総務省の労働力調査データをdata.frame形式で読み込み data_labor &lt;- openxlsx::read.xlsx(xlsxFile = &quot;https://www.e-stat.go.jp/stat-search/file-download?statInfId=000031831374&amp;fileKind=0&quot;, # ファイルパス／URL sheet = 1, # シートインデックス／シート名 startRow = 10, # 読み込み開始行 colNames = FALSE, # 列名データの有無 rowNames = FALSE, # 行名データの有無 rows = NULL, # 読み込む列（NULLですべて読み込み） cols = NULL) # 読み込む行（NULLですべて読み込み） # 不要な列を削除 data_labor %&lt;&gt;% dplyr::select(-X1, -X2, -X3, -X6) # 列名を追加 colnames(data_labor) &lt;- c(&quot;総数&quot;, &quot;農林&quot;, &quot;建設&quot;, &quot;製造&quot;, &quot;情報通信&quot;, &quot;運輸・郵便&quot;, &quot;卸・小売&quot;, &quot;金融・保険&quot;, &quot;不動産・物品賃貸&quot;, &quot;学術・専門・技術&quot;, &quot;宿泊・飲食&quot;, &quot;生活関連・娯楽&quot;, &quot;教育&quot;, &quot;医療・福祉&quot;, &quot;複合サービス&quot;, &quot;その他サービス&quot;, &quot;公務&quot;) # NAを含む行を削除 data_labor %&lt;&gt;% tidyr::drop_na(everything()) # すべての列を数値型に変換 data_labor %&lt;&gt;% dplyr::mutate(across(everything(), as.double)) # 日付列を追加 data_labor %&lt;&gt;% dplyr::mutate(date = seq(from = as.Date(&quot;2002-01-01&quot;), to = as.Date(&quot;2002-01-01&quot;) + months(dim(data_labor)[1] - 1), by = &quot;1 month&quot;), .before = &quot;総数&quot;) # tibble形式に変換 data_labor %&lt;&gt;% tibble::as_tibble() # データをコンソールに出力 data_labor ## # A tibble: 259 × 18 ## date 総数 農林 建設 製造 情報通信 運輸・…¹ 卸・小…² 金融…³ 不動産…⁴ ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2002-01-01 6267 224 589 1210 153 331 1131 165 95 ## 2 2002-02-01 6248 227 611 1201 156 324 1075 164 96 ## 3 2002-03-01 6297 242 628 1238 156 323 1081 161 99 ## 4 2002-04-01 6333 278 624 1213 159 315 1121 167 95 ## 5 2002-05-01 6356 307 607 1194 157 322 1134 171 96 ## 6 2002-06-01 6373 311 594 1223 159 330 1115 170 108 ## 7 2002-07-01 6374 294 622 1199 160 328 1116 180 110 ## 8 2002-08-01 6371 288 620 1207 164 326 1085 176 103 ## 9 2002-09-01 6353 286 621 1183 171 333 1081 174 100 ## 10 2002-10-01 6355 271 642 1176 160 333 1116 172 102 ## # … with 249 more rows, 8 more variables: `学術・専門・技術` &lt;dbl&gt;, ## # `宿泊・飲食` &lt;dbl&gt;, `生活関連・娯楽` &lt;dbl&gt;, 教育 &lt;dbl&gt;, `医療・福祉` &lt;dbl&gt;, ## # 複合サービス &lt;dbl&gt;, その他サービス &lt;dbl&gt;, 公務 &lt;dbl&gt;, and abbreviated ## # variable names ¹​`運輸・郵便`, ²​`卸・小売`, ³​`金融・保険`, ## # ⁴​`不動産・物品賃貸` 日本の県内総生産 内閣府が公表する年次の名目県内総生産データセットです（生産側、2008SNA、平成23年基準計数）。詳細は内閣府のウェブサイトを参照してください。ここではデータをdata.frame形式で読み込み、前処理して縦型のtibble形式に変換します。 # 内閣府の県内総生産データをdata.frame形式で読み込み data_gdp_pref &lt;- openxlsx::read.xlsx(xlsxFile = &quot;https://www.esri.cao.go.jp/jp/sna/data/data_list/kenmin/files/contents/tables/2018/soukatu1.xlsx&quot;, # ファイルパス／URL sheet = 1, # シートインデックス／シート名 startRow = 5, # 読み込み開始行 colNames = TRUE, # 列名データの有無 rowNames = FALSE, # 行名データの有無 rows = 5:53, # 読み込む列（NULLですべて読み込み） cols = NULL) # 読み込む行（NULLですべて読み込み） # データの型変換等を行い、縦型に変形したうえで、変化率系列を作成し、tibble形式に変換 data_gdp_pref %&lt;&gt;% dplyr::mutate(across(`2006`:`2018`, as.double)) %&gt;% dplyr::rename(pref_code = X1, pref_name = X2) %&gt;% dplyr::select(-X16) %&gt;% tidyr::pivot_longer(cols = c(-&quot;pref_code&quot;, -&quot;pref_name&quot;), names_to = &quot;year&quot;, values_to = &quot;gdp_nominal&quot;) %&gt;% dplyr::mutate(across(c(pref_code, year), as.double)) %&gt;% dplyr::arrange(pref_code, year) %&gt;% dplyr::group_by(pref_name) %&gt;% dplyr::mutate(gdp_nominal_pchg = 100 * (gdp_nominal / dplyr::lag(gdp_nominal, n = 1) - 1)) %&gt;% dplyr::ungroup() %&gt;% tibble::as_tibble() # データをコンソールに出力 data_gdp_pref ## # A tibble: 611 × 5 ## pref_code pref_name year gdp_nominal gdp_nominal_pchg ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 北海道 2006 19316568 NA ## 2 1 北海道 2007 19137599 -0.927 ## 3 1 北海道 2008 18457858 -3.55 ## 4 1 北海道 2009 18219113 -1.29 ## 5 1 北海道 2010 18122675 -0.529 ## 6 1 北海道 2011 18071493 -0.282 ## 7 1 北海道 2012 17923502 -0.819 ## 8 1 北海道 2013 18242119 1.78 ## 9 1 北海道 2014 18579766 1.85 ## 10 1 北海道 2015 19128504 2.95 ## # … with 601 more rows 日本の四半期別GDP 内閣府が公表する日本の四半期別GDPデータセットです（支出側、実質季節調整値）。2022年6月8日に公表された2022年1～3月期2次速報値のリンクからデータを取得しており、期間は1994年1～3月期から2022年1～3月期です。詳細は内閣府のウェブサイトを参照してください。ここではデータをtibble形式で読み込んで前処理を行います。 # 内閣府のGDP実質季節調整値データをtibble形式で読み込み data_gdp_jp &lt;- readr::read_csv(file = &quot;https://www.esri.cao.go.jp/jp/sna/data/data_list/sokuhou/files/2022/qe221_2/tables/gaku-jk2212.csv&quot;, # ファイルパス／URL（拡張子が必要） col_names = TRUE, # ヘッダー（列名データ）の有無 col_types = NULL, # 各列の型の指定（c：文字列型、d：数値型、D：日付型、l：論理値型） col_select = c(2, 3, 6, 7, 8, 9, 10, 11, 12, 15), # 読み込む列の指定（列名、インデックス） skip = 2, # 読み込み時に上からスキップする行数 locale = locale(encoding = &quot;CP932&quot;)) # Windows標準（Shift JIS）で作成されたファイルは&quot;CP932&quot;、utf-8で作成されたファイルは&quot;UTF-8&quot; ## New names: ## Rows: 120 Columns: 10 ## ── Column specification ## ──────────────────────────────────────────────────────────────────────────────────────────────────────────────── Delimiter: &quot;,&quot; chr ## (10): 国内総生産(支出側), 民間最終消費支出, 民間住宅, 民間企業設備, 民間在庫変動, 政府最終消費支出, 公的固定資本形成, 公的... ## ℹ Use `spec()` to retrieve the full column specification for this data. ℹ Specify the column types or set `show_col_types = FALSE` to ## quiet this message. ## • `` -&gt; `...1` ## • `` -&gt; `...4` ## • `` -&gt; `...5` ## • `` -&gt; `...13` ## • `` -&gt; `...14` ## • `` -&gt; `...16` ## • `` -&gt; `...20` ## • `` -&gt; `...21` ## • `` -&gt; `...23` ## • `` -&gt; `...27` ## • `` -&gt; `...29` ## • `` -&gt; `...31` ## • `` -&gt; `...32` ## • `` -&gt; `...33` # 不要な行を削除 data_gdp_jp %&lt;&gt;% dplyr::slice(-(1:4)) %&gt;% tidyr::drop_na(everything()) # データ型を文字列型から数値型に変換 data_gdp_jp %&lt;&gt;% dplyr::mutate(across(everything(), ~ str_replace_all(., &quot;,&quot;, &quot;&quot;) %&gt;% as.double())) # 列名を変更し、日付列を追加 data_gdp_jp %&lt;&gt;% dplyr::rename(`国内総生産` = `国内総生産(支出側)`, `純輸出` = `財貨・サービス`) %&gt;% dplyr::mutate(`日付` = seq(from = as.Date(&quot;1994-01-01&quot;), to = as.Date(&quot;2022-01-01&quot;), by = &quot;quarter&quot;), .before = `国内総生産`) # データをコンソールに出力 data_gdp_jp ## # A tibble: 113 × 11 ## 日付 国内総生産 民間…¹ 民間…² 民間企…³ 民間…⁴ 政府最…⁵ 公的…⁶ 公的在…⁷ ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1994-01-01 446308. 247535. 29812 66259. 4229 71357. 47106. -542. ## 2 1994-04-01 443744. 248758. 31118. 66065 -2884. 72244. 48020. 623. ## 3 1994-07-01 448945 250619. 33628. 65999. -280. 72789 46303. 806. ## 4 1994-10-01 447169. 250780 32218. 67029. -1602. 72950. 45589. 853. ## 5 1995-01-01 452113 252928. 31122. 68648. 2254. 74470. 43642. -27.7 ## 6 1995-04-01 456276. 254884. 30382. 71088 1117. 74611. 45422. 199. ## 7 1995-07-01 461709. 256356. 29742. 72334. 2106. 75543 48926 173. ## 8 1995-10-01 462860. 258171. 30431. 73525. 1033. 75789 49134. 235. ## 9 1996-01-01 466623. 257412. 31806. 73285 1643 76459 51647. 597. ## 10 1996-04-01 472494. 260510 33278. 75804 2979. 76392. 50920 191. ## # … with 103 more rows, 2 more variables: 純輸出 &lt;dbl&gt;, 開差 &lt;dbl&gt;, and ## # abbreviated variable names ¹​民間最終消費支出, ²​民間住宅, ³​民間企業設備, ## # ⁴​民間在庫変動, ⁵​政府最終消費支出, ⁶​公的固定資本形成, ⁷​公的在庫変動 西山 他（2019） 本書の第7章以降の計量経済学パートでは、西山 他（2019）で使用されているサンプルデータセットを使用します。サンプルデータセットは、西山 他（2019）のサポートウェブサイトで配布されています。 1.6 R起動時に実行する設定 ここまでに紹介した各種設定のうち、Rを起動したときに毎回実行する必要があるものをひとまとめにしました。 file.edit(\".Rprofile\")でプロジェクトフォルダの直下に.Rprofileファイルを作成し、その中に次のコードを記載して保存しておくと、Rの起動時に.Rprofileファイルの内容が自動的に実行されるので便利です。 # 全変数を削除 rm(list = ls()) # 警告の非表示 options(warn = -1) # プロキシ設定（使用環境に応じproxyserverとportを記入。筆者の実行環境では不要のためコメントアウト） # proxy_url &lt;- &quot;http://proxyserver:port/&quot; # # Sys.setenv(&quot;http_proxy&quot; = proxy_url) # Sys.setenv(&quot;https_proxy&quot; = proxy_url) # # options(download.file.method = &quot;libcurl&quot;) # options(timeout = NA) # パッケージのインストールとインポート packages &lt;- c( # tidyverse系 &quot;lubridate&quot;, &quot;magrittr&quot;, &quot;stringi&quot;, &quot;tidyverse&quot;, &quot;tsibble&quot;, &quot;readxl&quot;, # 図表系 &quot;esquisse&quot;, # Rバージョン3系では実行不可のためコメントアウトする &quot;geofacet&quot;, &quot;ggfortify&quot;, &quot;ggplotgui&quot;, # Rバージョン3系では実行不可のためコメントアウトする &quot;ggpubr&quot;, &quot;ggsci&quot;, &quot;ggrepel&quot;, &quot;hexbin&quot;, &quot;lemon&quot;, &quot;RColorBrewer&quot;, # 統計系 &quot;corrplot&quot;, &quot;corrr&quot;, &quot;DataExplorer&quot;, &quot;estimatr&quot;, &quot;forecast&quot;, &quot;GGally&quot;, &quot;lmtest&quot;, &quot;mFilter&quot;, &quot;mgcv&quot;, &quot;plm&quot;, &quot;psych&quot;, # Rバージョン3系では実行不可のためコメントアウトする &quot;sandwich&quot;, &quot;seasonal&quot;, &quot;sigmoid&quot;, &quot;SmartEDA&quot;, &quot;strucchange&quot;, &quot;tidyquant&quot;, &quot;tseries&quot;, &quot;vars&quot;, &quot;urca&quot;, # データパッケージ &quot;fpp&quot;, &quot;fpp2&quot;, &quot;nycflights13&quot;, # その他 &quot;docstring&quot;, &quot;openxlsx&quot;, &quot;pbapply&quot; ) new_packages &lt;- packages[!(packages %in% installed.packages()[,&quot;Package&quot;])] if(length(new_packages)) { install.packages(new_packages) } for (pkg in packages) { library(pkg, character.only = TRUE) } rm( new_packages, packages, pkg ) # Windowsのグラフ設定（筆者の実行環境では不要のためコメントアウト） # windowsFonts(&quot;MEIRYO&quot; = windowsFont(&quot;Meiryo UI&quot;)) # windowsFonts(&quot;YUGO&quot; = windowsFont(&quot;Yu Gothic UI&quot;)) # theme_set(theme_light(base_family = &quot;YUGO&quot;)) # Macのグラフ設定 theme_set(theme_light(base_family = &quot;HiraKakuProN-W3&quot;)) "],["rの基本的な使用方法-1.html", "2 Rの基本的な使用方法 2.1 第2章の準備 2.2 主要ショートカットキー 2.3 基本操作 2.4 演算 2.5 ベクトル 2.6 行列 2.7 データフレーム 2.8 リスト 2.9 文字列操作 2.10 制御構文 2.11 ファイル操作 2.12 データの読み込み 2.13 データの書き出し 2.14 オブジェクトのセーブ・ロード", " 2 Rの基本的な使用方法 第2章「Rの基本的な使用方法」では、Rの基本的な使用方法について解説します。本節の内容は基礎部分のみのため、詳細は、馬場（2020）、松村 他（2021）、Wickham &amp; Grolemund（2017）などを参照してください。 RStudioでコードを編集するパネルをソース（Source）と呼びます。RStudioの左上のパネルがソースで、コンソール（Console）パネルの上にあります。ソースが表示されていない場合は、FileメニューからNew Fileに進み、R Scriptを選択すると、ソースが表示されます。 ソースにコードを記述し、記述したコードの行にカーソルがある状態でCtrl + Enterを押すと、コンソールに実行結果が表示されます。 2.1 第2章の準備 2.1.1 パッケージのインポート library(magrittr) library(stringi) library(tidyverse) library(openxlsx) 2.2 主要ショートカットキー RStudioでよく使用されるショートカットキーです。Macの場合はCtrlをCmdに、AltをOptにそれぞれ置き換えてください。 Alt + Shift + K： キーボードショートカットを表示 2.2.1 編集 Ctrl + S： 保存 Ctrl + A： すべて選択 Ctrl + Shift + R： セクション区切りを挿入 Ctrl + Shift + C： 選択範囲をコメントアウト／コメントアウト解除 Ctrl + Shift + M： パイプオペレータ%&gt;%を挿入 2.2.2 実行 Ctrl + Enter： カーソルがある行／選択している部分のコードを実行 Ctrl + Alt + T： カーソルがあるセクションのコードをすべて実行 Ctrl + Alt + R： すべてのコードを実行 2.2.3 その他 F1： カーソルがある関数のヘルプを表示 2.3 基本操作 # 代入は &lt;- か = x &lt;- 2 x ## [1] 2 # シャープでコメントアウト（実行されない） 2.4 演算 2.4.1 四則計算 # 足し算 1 + 1 ## [1] 2 # 引き算 3 - 1 ## [1] 2 # 掛け算 2 * 3 ## [1] 6 # 割り算 10 / 5 ## [1] 2 # 割り算の整数の商 10 %/% 3 ## [1] 3 # 割り算の余り 10 %% 3 ## [1] 1 # べき乗 3 ** 2 ## [1] 9 3 ^ 2 ## [1] 9 2.4.2 数値計算 # 自然対数 log(10) ## [1] 2.302585 # ネイピア数のべき乗 exp(1) ## [1] 2.718282 # 平方根 sqrt(2) ## [1] 1.414214 # 絶対値 abs(-5) ## [1] 5 2.4.3 一致・大小関係 # 一致 2 == 2 ## [1] TRUE # 不一致 3 != 2 ## [1] TRUE # より大きい 3 &gt; 2 ## [1] TRUE # 以上 3 &gt;= 3 ## [1] TRUE # より小さい 2 &lt; 3 ## [1] TRUE # 以下 2 &lt;= 2 ## [1] TRUE 2.4.4 包含・集合関係 # 包含関係 1:5 %in% c(1, 2, 5) ## [1] TRUE TRUE FALSE FALSE TRUE # 和集合 union(seq(0, 20, 2), seq(0, 20, 3)) ## [1] 0 2 4 6 8 10 12 14 16 18 20 3 9 15 # 共通部分 intersect(seq(0, 20, 2), seq(0, 20, 3)) ## [1] 0 6 12 18 # 差分 setdiff(seq(0, 20, 2), seq(0, 20, 3)) ## [1] 2 4 8 10 14 16 20 2.5 ベクトル 2.5.1 ベクトルの作成 ベクトルは複数の要素を一つにまとめたデータ構造で、c()関数で作成します。一つのベクトルには単一のデータ型のみ格納でき、数値型や文字列型のデータを混在させることはできません。 # 数値型ベクトルの作成 vec_1 &lt;- c(11, 12, 13, 14, 15) vec_1 ## [1] 11 12 13 14 15 # 文字列型ベクトルの作成 vec_2 &lt;- c(&quot;Hello&quot;, &quot;World&quot;) vec_2 ## [1] &quot;Hello&quot; &quot;World&quot; # 数値型データと文字列型データを混在させると、すべて文字列型に変換される c(1, 2, &quot;A&quot;, &quot;B&quot;) ## [1] &quot;1&quot; &quot;2&quot; &quot;A&quot; &quot;B&quot; 2.5.2 ベクトル要素へのアクセス ベクトルでは、ベクトル[要素のインデックス]の形でインデックスを指定して各要素にアクセスすることができます。なお、Pythonなど他のプログラミング言語のインデックスは0から始まりますが、Rのインデックスは1から始まる点に注意してください。 # ベクトルの要素へのアクセス vec_1[1] ## [1] 11 vec_1[5] ## [1] 15 vec_1[2:4] ## [1] 12 13 14 rev(vec_1)[1] ## [1] 15 2.5.3 規則性があるベクトルの作成 規則性があるベクトルを作成するには、コロン（:）、seq()関数、rep()関数を使用します。 # 1から10までの等差数列 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 # 0から20までの2つ置きの数列 seq(from = 0, to = 20, by = 2) ## [1] 0 2 4 6 8 10 12 14 16 18 20 # 1から10までを4等分する等差数列 seq(from = 1, to = 10, length.out = 4) ## [1] 1 4 7 10 # 要素の繰り返し rep(x = 2, times = 5) ## [1] 2 2 2 2 2 # ベクトルの繰り返し rep(x = c(1, 2), times = 3) ## [1] 1 2 1 2 1 2 # ベクトルの各要素の繰り返し rep(x = c(1, 2), each = 3) ## [1] 1 1 1 2 2 2 2.5.4 ベクトルの演算 ベクトルの演算を行うと、ベクトルの対応する要素どうしが計算され、計算結果としてベクトルが出力されます。ベクトルの長さが異なる場合は、短い方のベクトルが使いまわされて長さを合わせます。 *演算子はベクトルの要素どうしの掛け算を行う点に注意してください。ベクトルの内積を計算するには%*%演算子を使用します。 vec_3 &lt;- c(1, 2, 3) vec_4 &lt;- c(5, 6, 7) # ベクトル要素の足し算 vec_3 + vec_4 ## [1] 6 8 10 # ベクトル要素の引き算 vec_3 - vec_4 ## [1] -4 -4 -4 # ベクトル要素の掛け算 vec_3 * vec_4 ## [1] 5 12 21 # ベクトル要素の割り算 vec_3 / vec_4 ## [1] 0.2000000 0.3333333 0.4285714 # ベクトルの内積 vec_3 %*% vec_4 ## [,1] ## [1,] 38 2.6 行列 2.6.1 行列の作成 行列はmatrix()関数を使用し、ベクトルを複数の列・行に分割する形で作成します。 # ベクトルを複数列に分割して行列を作成 mat_1 &lt;- matrix( data = 1:10, ncol = 2 ) mat_1 ## [,1] [,2] ## [1,] 1 6 ## [2,] 2 7 ## [3,] 3 8 ## [4,] 4 9 ## [5,] 5 10 # ベクトルを複数行に分割して行列を作成 mat_2 &lt;- matrix( data = 1:10, nrow = 2 ) mat_2 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 3 5 7 9 ## [2,] 2 4 6 8 10 2.6.2 行列の要素へのアクセス 行列の要素にアクセスするには、ベクトルと同様に行列[要素の行インデックス, 要素の列インデックス]の形でインデックスを指定します。 # 行列の3行目・1列目の要素を取得 mat_1[3, 1] ## [1] 3 # 行列の4行目全体を取得 mat_1[4, ] ## [1] 4 9 # 行列の2列目全体を取得 mat_1[, 2] ## [1] 6 7 8 9 10 # 行列の1～3行目を取得 mat_1[1:3, ] ## [,1] [,2] ## [1,] 1 6 ## [2,] 2 7 ## [3,] 3 8 # 行列の1～3行目以外を取得 mat_1[-1:-3, ] ## [,1] [,2] ## [1,] 4 9 ## [2,] 5 10 2.6.3 行列の演算 行列の演算を行うと、行列の対応する要素どうしが計算され、計算結果として行列が出力されます。 *演算子は行列の要素どうしの掛け算を行う点に注意してください。行列の積を計算するには%*%演算子を使用します。 mat_2 &lt;- matrix( data = 1:4, ncol = 2 ) mat_3 &lt;- matrix( data = 5:8, ncol = 2 ) # 行列要素の足し算 mat_2 + mat_3 ## [,1] [,2] ## [1,] 6 10 ## [2,] 8 12 # 行列要素の引き算 mat_2 - mat_3 ## [,1] [,2] ## [1,] -4 -4 ## [2,] -4 -4 # 行列要素の掛け算 mat_2 * mat_3 ## [,1] [,2] ## [1,] 5 21 ## [2,] 12 32 # 行列要素の割り算 mat_2 / mat_3 ## [,1] [,2] ## [1,] 0.2000000 0.4285714 ## [2,] 0.3333333 0.5000000 # 各行の和 rowSums(mat_2) ## [1] 4 6 # 各列の和 colSums(mat_2) ## [1] 3 7 # 行列の積 mat_2 %*% mat_3 ## [,1] [,2] ## [1,] 23 31 ## [2,] 34 46 # 行列の転置 t(mat_2) ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 # 逆行列 solve(mat_2) ## [,1] [,2] ## [1,] -2 1.5 ## [2,] 1 -0.5 2.7 データフレーム データフレームは、同じ長さの列ベクトルを複数まとめた行列形式のデータ構造で、実務で最も頻繁に使用します。Excelのスプレッドシートのイメージに近く、実際にCSV形式のファイルをRに読み込むとデータフレーム形式のオブジェクトが作成されます。 なお、データフレームには、Rにもともと備わっているdata.frame形式と、tidyverseパッケージによって導入されたtibble形式の2種類があります。両者にはいくつか違いがありますが、data.frame形式を使いやすくしたものがtibble形式と言えます。詳細はtibbleパッケージの公式ウェブサイトを参照してください。 2.7.1 データフレームの作成 tibble形式のデータフレームは、tibble()関数を使用して列名 = 要素の形で作成します。データフレームではすべての列ベクトルの要素数が同じになるようにします。 df_1 &lt;- tibble( x = 1:5, y = 6:10, z = x ^ 2 + y ) df_1 ## # A tibble: 5 × 3 ## x y z ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 6 7 ## 2 2 7 11 ## 3 3 8 17 ## 4 4 9 25 ## 5 5 10 35 2.7.2 データフレームの要素へのアクセス データフレームでは、データフレーム$列名の形でドルマークを使用して各列ベクトルにアクセスすることができます。また、データフレーム$列名[要素インデックス]で各列ベクトルの要素にアクセスできます。 df_1$x ## [1] 1 2 3 4 5 df_1$z[3] ## [1] 17 その他に、角括弧を使用して要素にアクセスすることもできます。ここで、一重の角括弧と二重の角括弧では実行結果が異なる点に注意してください。一重角括弧では、データフレームの一部を分割したものとして、結果がtibble形式で出力されます。一方、二重角括弧では、tibble形式の中に格納されているベクトルや単一の数値といった要素そのものが出力されます。 df_1[&quot;x&quot;] ## # A tibble: 5 × 1 ## x ## &lt;int&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 df_1[1] ## # A tibble: 5 × 1 ## x ## &lt;int&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 df_1[2, 2] ## # A tibble: 1 × 1 ## y ## &lt;int&gt; ## 1 7 df_1[[&quot;x&quot;]] ## [1] 1 2 3 4 5 df_1[[1]] ## [1] 1 2 3 4 5 df_1[[2, 2]] ## [1] 7 2.7.3 その他のデータフレームの機能 df_2 &lt;- tibble( letters = LETTERS, numbers = 1:26 ) df_2 ## # A tibble: 26 × 2 ## letters numbers ## &lt;chr&gt; &lt;int&gt; ## 1 A 1 ## 2 B 2 ## 3 C 3 ## 4 D 4 ## 5 E 5 ## 6 F 6 ## 7 G 7 ## 8 H 8 ## 9 I 9 ## 10 J 10 ## # … with 16 more rows # データフレームの上部のみ表示 head(df_2) ## # A tibble: 6 × 2 ## letters numbers ## &lt;chr&gt; &lt;int&gt; ## 1 A 1 ## 2 B 2 ## 3 C 3 ## 4 D 4 ## 5 E 5 ## 6 F 6 # データフレームの下部のみ表示 tail(df_2) ## # A tibble: 6 × 2 ## letters numbers ## &lt;chr&gt; &lt;int&gt; ## 1 U 21 ## 2 V 22 ## 3 W 23 ## 4 X 24 ## 5 Y 25 ## 6 Z 26 # データフレームの列名（変数）一覧を出力 colnames(df_2) ## [1] &quot;letters&quot; &quot;numbers&quot; # データフレームの列名（変数）と内容の一覧を出力 str(df_2) ## tibble [26 × 2] (S3: tbl_df/tbl/data.frame) ## $ letters: chr [1:26] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; ... ## $ numbers: int [1:26] 1 2 3 4 5 6 7 8 9 10 ... 2.8 リスト リストは、単一の数値、文字列、ベクトル、データフレームなど、様々な種類のデータを格納することができる容器のようなものです。リストそのものをリストに格納することもできます。リストはlist()関数で作成します。リストに格納する各要素には、それぞれ名前を付けることができます。 2.8.1 リストの作成 list_1 &lt;- list( number = 1, vector = c(10, 11, 12), matrix = matrix(1:9, ncol = 3) ) list_1 ## $number ## [1] 1 ## ## $vector ## [1] 10 11 12 ## ## $matrix ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 2.8.2 リストの要素へのアクセス リストでは、データフレームと同様にリスト$要素名の形でドルマークを使用して各要素にアクセスすることができます。 list_1$number ## [1] 1 list_1$matrix ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 また、ベクトルや行列と同様に、リスト[[要素インデックス]]でも各要素にアクセスすることができます。ここで、角括弧が二重である点に注意してください。 list_1[[3]] ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 2.8.3 リストに要素を追加 リストに新しい要素を追加するには、append()関数を用います。 list_1 &lt;- append(list_1, list(string = &quot;add&quot;)) list_1 ## $number ## [1] 1 ## ## $vector ## [1] 10 11 12 ## ## $matrix ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## $string ## [1] &quot;add&quot; 2.9 文字列操作 Rでは、tidyverseに含まれるstringrパッケージを使用して、様々な文字列操作を行うことができます。 stringrパッケージの文字列操作関数は、複数の文字列型データを格納したベクトル形式のデータ構造（データフレームの列など）に対し、一括して文字列操作の処理を適用することを念頭に設計されています。これにより、forループなどを使用することなく、複数の文字列に対する操作を高速に実行することができます。 2.9.1 文字列の連結 複数の文字列を連結するにはstr_c()関数を使用します。 # 単語を連結 str_c(&quot;日本&quot;, &quot;の&quot;, &quot;GDP&quot;, &quot;成長率&quot;, &quot;は&quot;, &quot;2％&quot;) ## [1] &quot;日本のGDP成長率は2％&quot; # 区切り文字を使用して単語を連結 str_c(&quot;directory&quot;, &quot;subdirectory&quot;, &quot;file&quot;, sep = &quot;/&quot;) ## [1] &quot;directory/subdirectory/file&quot; # 文字列ベクトルと単一文字列を連結 str_c(c(&quot;日本&quot;, &quot;米国&quot;, &quot;欧州&quot;), &quot;の成長率&quot;) ## [1] &quot;日本の成長率&quot; &quot;米国の成長率&quot; &quot;欧州の成長率&quot; # 文字列ベクトルに格納されている複数の文字列を連結 str_c(c(&quot;日本&quot;, &quot;米国&quot;, &quot;欧州&quot;), collapse = &quot;&quot;) ## [1] &quot;日本米国欧州&quot; 2.9.2 文字列の切り出し 文字列から一部を切り出すには、str_sub()関数を使用し、切り出す箇所の開始・終了位置（文字数）を指定します。 str_sub(string = c(&quot;日本の成長率&quot;, &quot;米国の成長率&quot;, &quot;欧州の成長率&quot;), # 切り出す対象の文字列／文字列ベクトル start = 1, # 開始位置の文字数 end = 2 # 終了位置の文字数 ) ## [1] &quot;日本&quot; &quot;米国&quot; &quot;欧州&quot; 2.9.3 文字列の検出・置換 ある文字列を含んでいるかを検出するには、str_detect()関数を使用し、pattern引数に検出条件を指定します。str_detect()関数は検出結果に応じてTRUEかFALSEを出力します。 str_detect(string = c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;, &quot;pinapple&quot;), # 検出対象の文字列ベクトル pattern = &quot;e&quot; # 検出条件 ) ## [1] TRUE FALSE TRUE TRUE より複雑な条件で文字列を検出する場合は、文字列ベクトルを変数としたデータフレームにstr_detect()関数を適用するのがよいでしょう。 具体的には、データフレームの行をフィルタするdplyr::filter()関数の中でstr_detect()関数を使用して、検出条件に合致するデータを抽出します。なお、データフレームの操作に関する詳細は、第3章「tidyverseによるデータ操作」を参照してください。 ここでは、都道府県名の変数をもつデータフレームをサンプルデータとして作成し、str_detect()関数を用いてデータの抽出を行います。 # サンプルデータを作成 data_pref_names &lt;- tibble::tibble( pref_index = 1:47, pref_name = c(&quot;北海道&quot;, &quot;青森県&quot;, &quot;岩手県&quot;, &quot;宮城県&quot;, &quot;秋田県&quot;, &quot;山形県&quot;, &quot;福島県&quot;, &quot;茨城県&quot;, &quot;栃木県&quot;, &quot;群馬県&quot;, &quot;埼玉県&quot;, &quot;千葉県&quot;, &quot;東京都&quot;, &quot;神奈川県&quot;, &quot;新潟県&quot;, &quot;富山県&quot;, &quot;石川県&quot;, &quot;福井県&quot;, &quot;山梨県&quot;, &quot;長野県&quot;, &quot;岐阜県&quot;, &quot;静岡県&quot;, &quot;愛知県&quot;, &quot;三重県&quot;, &quot;滋賀県&quot;, &quot;京都府&quot;, &quot;大阪府&quot;, &quot;兵庫県&quot;, &quot;奈良県&quot;, &quot;和歌山県&quot;, &quot;鳥取県&quot;, &quot;島根県&quot;, &quot;岡山県&quot;, &quot;広島県&quot;, &quot;山口県&quot;, &quot;徳島県&quot;, &quot;香川県&quot;, &quot;愛媛県&quot;, &quot;高知県&quot;, &quot;福岡県&quot;, &quot;佐賀県&quot;, &quot;長崎県&quot;, &quot;熊本県&quot;, &quot;大分県&quot;, &quot;宮崎県&quot;, &quot;鹿児島県&quot;, &quot;沖縄県&quot;) ) # 単一文字列を検出 data_pref_names %&gt;% dplyr::filter(str_detect(string = pref_name, pattern = &quot;県&quot;)) ## # A tibble: 43 × 2 ## pref_index pref_name ## &lt;int&gt; &lt;chr&gt; ## 1 2 青森県 ## 2 3 岩手県 ## 3 4 宮城県 ## 4 5 秋田県 ## 5 6 山形県 ## 6 7 福島県 ## 7 8 茨城県 ## 8 9 栃木県 ## 9 10 群馬県 ## 10 11 埼玉県 ## # … with 33 more rows # 複数文字列をor条件で検出するには、|演算子を使用 data_pref_names %&gt;% dplyr::filter(str_detect(string = pref_name, pattern = &quot;都|府&quot;)) ## # A tibble: 3 × 2 ## pref_index pref_name ## &lt;int&gt; &lt;chr&gt; ## 1 13 東京都 ## 2 26 京都府 ## 3 27 大阪府 # 複数文字列をand条件で検出するには、str_detect()関数を&amp;演算子で連結 data_pref_names %&gt;% dplyr::filter(str_detect(string = pref_name, pattern = &quot;都&quot;) &amp; str_detect(string = pref_name, pattern = &quot;府&quot;)) ## # A tibble: 1 × 2 ## pref_index pref_name ## &lt;int&gt; &lt;chr&gt; ## 1 26 京都府 文字列の置換はstr_replace()関数で行います。置換の該当箇所が複数ある場合、最初の該当箇所のみ置換する場合はstr_replace()関数を、すべての該当箇所を置換する場合はstr_replace_all()関数を使用します。どちらの関数も、pattern引数に置換元の文字列を、replacement引数に置換先の文字列を指定します。 複数の「置換元文字列＆置換先文字列」の組み合わせを設定する場合は、pattern引数に名前付きベクトルc(\"pattern1\" = \"replacement1\", \"pattern2\" = \"replacement2\")の形で「置換元文字列＆置換先文字列」の組み合わせを指定します。 なお、置換先の文字列として、何もない文字列（ダブルクオーテーションの中に何も含まない文字列\"\"）を指定すると、置換元の文字列を削除したことと同じ処理になります。 # 都道府県名一覧の&quot;県&quot;を&quot;&quot;で置換（削除） str_replace_all(string = data_pref_names$pref_name, # 置換対象の文字列 pattern = &quot;県&quot;, # 置換元の文字列 replacement = &quot;&quot; # 置換先の文字列 ) ## [1] &quot;北海道&quot; &quot;青森&quot; &quot;岩手&quot; &quot;宮城&quot; &quot;秋田&quot; &quot;山形&quot; &quot;福島&quot; &quot;茨城&quot; ## [9] &quot;栃木&quot; &quot;群馬&quot; &quot;埼玉&quot; &quot;千葉&quot; &quot;東京都&quot; &quot;神奈川&quot; &quot;新潟&quot; &quot;富山&quot; ## [17] &quot;石川&quot; &quot;福井&quot; &quot;山梨&quot; &quot;長野&quot; &quot;岐阜&quot; &quot;静岡&quot; &quot;愛知&quot; &quot;三重&quot; ## [25] &quot;滋賀&quot; &quot;京都府&quot; &quot;大阪府&quot; &quot;兵庫&quot; &quot;奈良&quot; &quot;和歌山&quot; &quot;鳥取&quot; &quot;島根&quot; ## [33] &quot;岡山&quot; &quot;広島&quot; &quot;山口&quot; &quot;徳島&quot; &quot;香川&quot; &quot;愛媛&quot; &quot;高知&quot; &quot;福岡&quot; ## [41] &quot;佐賀&quot; &quot;長崎&quot; &quot;熊本&quot; &quot;大分&quot; &quot;宮崎&quot; &quot;鹿児島&quot; &quot;沖縄&quot; # 都道府県名一覧の&quot;県&quot;、&quot;都&quot;、&quot;府&quot;を&quot;&quot;で置換（削除） # この場合、&quot;京都府&quot;の&quot;都&quot;と&quot;府&quot;が置換されるため、適切でない str_replace_all(string = data_pref_names$pref_name, # 置換対象の文字列 pattern = c(&quot;県&quot; = &quot;&quot;, &quot;都&quot; = &quot;&quot;, &quot;府&quot; = &quot;&quot;) # 複数の置換条件を格納した名前付きベクトル ) ## [1] &quot;北海道&quot; &quot;青森&quot; &quot;岩手&quot; &quot;宮城&quot; &quot;秋田&quot; &quot;山形&quot; &quot;福島&quot; &quot;茨城&quot; ## [9] &quot;栃木&quot; &quot;群馬&quot; &quot;埼玉&quot; &quot;千葉&quot; &quot;東京&quot; &quot;神奈川&quot; &quot;新潟&quot; &quot;富山&quot; ## [17] &quot;石川&quot; &quot;福井&quot; &quot;山梨&quot; &quot;長野&quot; &quot;岐阜&quot; &quot;静岡&quot; &quot;愛知&quot; &quot;三重&quot; ## [25] &quot;滋賀&quot; &quot;京&quot; &quot;大阪&quot; &quot;兵庫&quot; &quot;奈良&quot; &quot;和歌山&quot; &quot;鳥取&quot; &quot;島根&quot; ## [33] &quot;岡山&quot; &quot;広島&quot; &quot;山口&quot; &quot;徳島&quot; &quot;香川&quot; &quot;愛媛&quot; &quot;高知&quot; &quot;福岡&quot; ## [41] &quot;佐賀&quot; &quot;長崎&quot; &quot;熊本&quot; &quot;大分&quot; &quot;宮崎&quot; &quot;鹿児島&quot; &quot;沖縄&quot; # 都道府県名一覧の&quot;県&quot;、&quot;都&quot;、&quot;府&quot;を&quot;&quot;で置換（削除）する正しい処理 str_replace_all(string = data_pref_names$pref_name, # 置換対象の文字列 pattern = c(&quot;県&quot; = &quot;&quot;, &quot;東京都&quot; = &quot;東京&quot;, &quot;府&quot; = &quot;&quot;) # 複数の置換条件を格納した名前付きベクトル ) ## [1] &quot;北海道&quot; &quot;青森&quot; &quot;岩手&quot; &quot;宮城&quot; &quot;秋田&quot; &quot;山形&quot; &quot;福島&quot; &quot;茨城&quot; ## [9] &quot;栃木&quot; &quot;群馬&quot; &quot;埼玉&quot; &quot;千葉&quot; &quot;東京&quot; &quot;神奈川&quot; &quot;新潟&quot; &quot;富山&quot; ## [17] &quot;石川&quot; &quot;福井&quot; &quot;山梨&quot; &quot;長野&quot; &quot;岐阜&quot; &quot;静岡&quot; &quot;愛知&quot; &quot;三重&quot; ## [25] &quot;滋賀&quot; &quot;京都&quot; &quot;大阪&quot; &quot;兵庫&quot; &quot;奈良&quot; &quot;和歌山&quot; &quot;鳥取&quot; &quot;島根&quot; ## [33] &quot;岡山&quot; &quot;広島&quot; &quot;山口&quot; &quot;徳島&quot; &quot;香川&quot; &quot;愛媛&quot; &quot;高知&quot; &quot;福岡&quot; ## [41] &quot;佐賀&quot; &quot;長崎&quot; &quot;熊本&quot; &quot;大分&quot; &quot;宮崎&quot; &quot;鹿児島&quot; &quot;沖縄&quot; 2.9.4 文字列の変換 アルファベットの大文字・小文字の変換や、日本語文における英数字の全角・半角の変換には、stringrパッケージのアルファベット変換用関数や、stringiパッケージのstri_trans_general()関数を用います。 # アルファベットの大文字・小文字を変換 sentence_en &lt;- &quot;The quick brown dog&quot; # 全て大文字に変換 str_to_upper(sentence_en) ## [1] &quot;THE QUICK BROWN DOG&quot; # 全て小文字に変換 str_to_lower(sentence_en) ## [1] &quot;the quick brown dog&quot; # 各単語の最初の文字を大文字に変換 str_to_title(sentence_en) ## [1] &quot;The Quick Brown Dog&quot; # 文章の最初の文字を大文字に変換 str_to_sentence(sentence_en) ## [1] &quot;The quick brown dog&quot; stri_trans_general()関数では、下記の例の他にも様々な変換を行うことができます。詳細はこちらのウェブサイトを参照してください。 # 日本語文の全角・半角を変換 sentence_jp &lt;- &quot;日本の第２四半期のGDP成長率は前期比＋０．５％だった&quot; # 半角から全角へ変換 stri_trans_general(sentence_jp, id = &quot;Halfwidth-Fullwidth&quot;) ## [1] &quot;日本の第２四半期のＧＤＰ成長率は前期比＋０．５％だった&quot; # 全角から半角へ変換 stri_trans_general(sentence_jp, id = &quot;Fullwidth-Halfwidth&quot;) ## [1] &quot;日本の第2四半期のGDP成長率は前期比+0.5%だった&quot; 2.9.5 文字列のパディング パディングとは、固定長の文字列を扱う際に、文字数（桁数）が足りない部分を埋めることです。主に、ファイルやオブジェクトの名称に数字で連番をつける時に用います。パディングにはstr_pad()関数を使用します。 # 数字の左側にゼロをパディングして3桁にする str_pad(string = 1:20, # 元の文字列（ここでは数値ベクトルを指定） width = 3, # 桁数 pad = &quot;0&quot;, # パディングに使用する文字 side = &quot;left&quot; # パディングする側 ) ## [1] &quot;001&quot; &quot;002&quot; &quot;003&quot; &quot;004&quot; &quot;005&quot; &quot;006&quot; &quot;007&quot; &quot;008&quot; &quot;009&quot; &quot;010&quot; &quot;011&quot; &quot;012&quot; ## [13] &quot;013&quot; &quot;014&quot; &quot;015&quot; &quot;016&quot; &quot;017&quot; &quot;018&quot; &quot;019&quot; &quot;020&quot; # ファイル名に1～12の2桁連番を追加する例 str_c(&quot;file_&quot;, str_pad(string = 1:12, width = 2, pad = &quot;0&quot;)) ## [1] &quot;file_01&quot; &quot;file_02&quot; &quot;file_03&quot; &quot;file_04&quot; &quot;file_05&quot; &quot;file_06&quot; &quot;file_07&quot; ## [8] &quot;file_08&quot; &quot;file_09&quot; &quot;file_10&quot; &quot;file_11&quot; &quot;file_12&quot; 2.9.6 文字列によるプログラム実行 文字列を使った特殊な操作として、文字列によるプログラムの実行方法を紹介します。 例えば、リストに要素を追加する際の要素名として、変数に格納した値を使いたい場合、通常のプログラムを実行すると変数名が要素名として使用されてしまいます。 そこで、eval()関数とparse()関数を組み合わせ、eval(parse(text = 文字列によるプログラム文))を実行すると、上記の意図せざる結果を避けることができます。 # 通常のプログラム文：変数名がリストの要素名になってしまう list_str &lt;- list() for (letter in c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) { list_str &lt;- append(list_str, list(letter = 111)) } list_str ## $letter ## [1] 111 ## ## $letter ## [1] 111 ## ## $letter ## [1] 111 # 文字列のプログラム文：変数に格納されている値がリストの要素名になる list_str &lt;- list() for (letter in c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) { # 実行するプログラムを文字列で作成 text &lt;- str_c(&quot;list_str &lt;- append(list_str, list(&quot;, letter, &quot;= 111))&quot;) # 文字列のプログラム文を実行 eval(parse(text = text)) } list_str ## $a ## [1] 111 ## ## $b ## [1] 111 ## ## $c ## [1] 111 2.10 制御構文 制御構文には、条件分岐を行うifやelse、同じ操作の繰り返しを行うforやwhile、エラー処理を行うtryがあります。 2.10.1 if/else文 if文は、if (条件) {処理}の形で記述します。else ifで追加条件、elseで「その他すべて」の条件を意味します。 x &lt;- 5 if (x &lt; 2) { print(&quot;A&quot;) } else if (x &gt;= 2 &amp; x &lt; 6) { print(&quot;B&quot;) } else { print(&quot;C&quot;) } ## [1] &quot;B&quot; 2.10.2 forループ for文は、for (変数 in 変数に逐次代入する要素) {処理}の形で記述します。変数はカウンタではない点に注意してください。 for (i in 1:5) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 for (letter in letters[1:6]) { print(letter) } ## [1] &quot;a&quot; ## [1] &quot;b&quot; ## [1] &quot;c&quot; ## [1] &quot;d&quot; ## [1] &quot;e&quot; ## [1] &quot;f&quot; 変数をカウンタとして使用したい場合は、seq_along()関数をfor (変数 in seq_along(変数に逐次代入する要素を格納したベクトル等))の形で用いると、自動的に要素数に応じたインデックスを変数に代入してくれます。 for (i in seq_along(letters[1:6])) { print(str_c(i, &quot;_&quot;, letters[1:6][i])) } ## [1] &quot;1_a&quot; ## [1] &quot;2_b&quot; ## [1] &quot;3_c&quot; ## [1] &quot;4_d&quot; ## [1] &quot;5_e&quot; ## [1] &quot;6_f&quot; 2.10.3 whileループ while文は、while (繰り返し処理を続ける条件) {処理}の形で記述します。while文を使う際は、「繰り返し処理を続ける条件」が有限回数で終わるように処理内容を工夫します（そうしないと無限ループになります）。例えば、下記のように条件を「カウンタが正の値」にしておき、処理の中でカウンタが減少するように書くのが一般的です。 count &lt;- 3 while (count &gt; 0) { print(count) count &lt;- count - 1 } ## [1] 3 ## [1] 2 ## [1] 1 2.10.4 breakによる繰り返しの終了 while文の処理の中でbreak()関数を用い、ループを強制的に終了させることができます。 vec_break &lt;- c(10, 20, 30, 40, 50) index &lt;- 1 while (TRUE) { # indexがvec_breakの要素数を超えると繰り返しを終了 if (index &gt; length(vec_break)) { break() } print(vec_break[index]) index &lt;- index + 1 } ## [1] 10 ## [1] 20 ## [1] 30 ## [1] 40 ## [1] 50 2.10.5 nextによる処理のスキップ for文やwhile文の中でnest()関数を用いると、処理をスキップして次の処理に移ります。 for (i in 1:5) { # iが3のとき処理をスキップ if (i == 3) next() print(i) } ## [1] 1 ## [1] 2 ## [1] 4 ## [1] 5 2.10.6 tryによるエラー処理 エラーを起こす可能性がある処理をtry()関数のexpr = {}内に記述することで、エラーが発生しても処理を続けることができます。 list_try &lt;- list(1, 2, 3, &quot;4&quot;, 5) for (i in seq_along(list_try)) { try( expr = { # エラーの可能性がある処理 x &lt;- log(list_try[[i]]) print(x) }, silent = FALSE # TRUEにするとエラーの内容を非表示 ) } ## [1] 0 ## [1] 0.6931472 ## [1] 1.098612 ## Error in log(list_try[[i]]) : ## non-numeric argument to mathematical function ## [1] 1.609438 2.11 ファイル操作 Rでは、ファイル操作関数を使用してディレクトリ（フォルダ）やファイルを直接操作することができます。 なお、WindowsやPythonではディレクトリの階層を表す記号としてバックスラッシュ：\\（日本語環境では円マークとして表示される場合もある）を用いますが、Rではスラッシュ：/を用いますので、注意してください。 2.11.1 ディレクトリ関連 # 作業ディレクトリの指定 setwd(dir = &quot;directory&quot;) # 作業ディレクトリのパスを出力 getwd() # ディレクトリを新規作成 dir.create(path = &quot;./directory&quot;) 2.11.2 ファイル関連 # ディレクトリの中にあるファイル一覧を出力 list.files(path = &quot;directory&quot;) # ファイルのコピー file.copy(from = &quot;directory/file_1.csv&quot;, to = &quot;directory/file_2.csv&quot; ) 2.11.3 ファイルのダウンロード ウェブからファイルをダウンロードする場合は、download.file()関数を使用します。プロキシ環境ではあらかじめプロキシ設定を行ってください。 download.file(url = &quot;https://www.sample.com/file.csv&quot;, # ダウンロード元のURL destfile = &quot;directory/file.csv&quot; # ダウンロードしたファイルを格納するファイルパス ) 制御構文のforループなどと組み合わせると、複数のファイルを自動でダウンロードできます。ただし、ダウンロード元のサーバーに過度な負荷をかけないよう、Sys.sleep()関数でスリープタイムを設定しておきましょう。 # forループで複数のファイルを自動ダウンロードする例 website &lt;- &quot;https://www.sample.com/&quot; file_name &lt;- &quot;file&quot; file_nums &lt;- 1:3 for (i in file_nums) { url &lt;- str_c(website, file_name, &quot;_&quot;, i, &quot;.csv&quot;), destfile &lt;- str_c(&quot;directory/&quot;, file_name, &quot;_&quot;, str_pad(i, width = 2, pad = &quot;0&quot;), &quot;.csv&quot;) download.file(url = url, destfile = destfile) Sys.sleep(time = 1) # スリープタイム（指定した秒数の間、処理を停止） } 2.12 データの読み込み 2.12.1 CSVデータ CSVデータの読み込みには、readrパッケージのread_csv()関数を使用します。ローカルネットワークのファイルパス、ウェブのURLどちらからの読み込みにも対応しています。 CSVが使用している文字コードにより文字化けする場合あるため、locale引数で適切な文字コードを指定してください。 data &lt;- readr::read_csv(file = &quot;directory/file.csv&quot;, # ファイルパス／URL（拡張子が必要） col_names = TRUE, # ヘッダー（列名データ）の有無／列名指定 col_types = NULL, # 各列の型の指定（c：文字列型、d：数値型、D：日付型、l：論理値型） col_select = NULL, # 読み込む列の指定（列名、インデックス） skip = 0, # 読み込み時に上からスキップする行数 locale = locale(encoding = &quot;CP932&quot;) # Windows標準（Shift JIS）で作成されたファイルは&quot;CP932&quot;、utf-8で作成されたファイルは&quot;UTF-8&quot; ) CSVファイルにおけるデータの区切り文字はカンマ「,」ですが、中には区切り文字としてカンマ以外の文字（例えばセミコロン「;」など）が使われている場合もあります。そうした特殊なCSVファイルはread_csv()関数では上手く読み込めないことがあるため、その際はread_delim()関数を使用し、delim引数に区切り文字を指定します。 data &lt;- readr::read_delim(file = &quot;directory/file.csv&quot;, # ファイルパス／URL（拡張子が必要） delim = &quot;;&quot; # 区切り文字（ここでは例としてセミコロンを指定） col_names = TRUE, # ヘッダー（列名データ）の有無／列名指定 col_types = NULL, # 各列の型の指定（c：文字列型、d：数値型、D：日付型、l：論理値型） col_select = NULL, # 読み込む列の指定（列名、インデックス） skip = 0, # 読み込み時に上からスキップする行数 locale = locale(encoding = &quot;CP932&quot;) # Windows標準（Shift JIS）で作成されたファイルは&quot;CP932&quot;、utf-8で作成されたファイルは&quot;UTF-8&quot; ) 2.12.2 xls・xlsx形式データ readrパッケージのread_excel()関数は、Excelのxls形式、xlsx形式どちらも読み込み可能です。ただし、読み込み元としてpath引数に指定できるのはローカルネットワーク内のファイルパスだけで、ウェブのURLからの読み込みはできません。 data &lt;- readxl::read_excel(path = &quot;directory/file.xls&quot;, # ファイルパス（拡張子が必要、URLは不可） sheet = NULL, # シートインデックス／シート名 col_names = TRUE, # ヘッダー（列名データ）の有無／列名指定 col_types = NULL, # 各列の型の指定（c：文字列型、d：数値型、D：日付型、l：論理値型） skip = 0 # 読み込み時に上からスキップする行数 ) 2.12.3 xlsx形式データ openxlsxパッケージのread.xlsx()関数であれば、ローカルネットワーク内のファイルパスとウェブのURLどちらからでも読み込みが可能です。ただし、読み込めるファイル形式はxlsx形式のみで、xls形式には対応していません。 data &lt;- openxlsx::read.xlsx(xlsxFile = &quot;directory/file.xlsx&quot;, # ファイルパス／URL（拡張子が必要） sheet = 1, # シートインデックス／シート名 startRow = 5, # 読み込み開始行 colNames = TRUE, # 列名データの有無 rowNames = FALSE, # 行名データの有無 rows = 5:53, # 読み込む列（NULLですべて読み込み） cols = NULL # 読み込む行（NULLですべて読み込み） ) なお、openxlsxパッケージのgetSheetNames()関数を使うと、Excelファイル内のシート名の一覧を取得することができます。 sheet_names &lt;- openxlsx::getSheetNames(file = &quot;directory/file.xlsx&quot;) 2.12.4 複数データの読み込み 複数のCSV形式ファイルや、xlsx形式ファイルの中の複数のシートのデータを読み込んで結合する場合は、読み込み対象のファイル名やシート名の一覧をあらかじめ取得し、特定の関数やループを使って連続して一気にデータを読み込むことができます。 共通の列名をもつ複数のCSVファイルを読み込んで縦方向に結合する場合は、readrパッケージのread_csv()関数を使用します。 # 読み込み対象のファイル名一覧を取得 file_names &lt;- fs::dir_ls(path = &quot;data_read/csv_row&quot;, # CSVファイルを格納しているフォルダのパスを入力 glob = &quot;*.csv&quot; # CSVファイル名を読み込む対象として指定（変更する必要なし） ) # 複数のCSVファイルを連続して読み込み縦方向に結合 data &lt;- readr::read_csv(file = file_names, # 読み込み対象のファイル名一覧を格納したベクトル id = &quot;file_path&quot;, # ファイルパスの列を追加する場合は列名を指定 col_names = TRUE, # ヘッダー（列名データ）の有無／列名指定 col_types = NULL, # 各列の型の指定（c：文字列型、d：数値型、D：日付型、l：論理値型） col_select = NULL, # 読み込む列の指定（列名、インデックス） skip = 0, # 読み込み時に上からスキップする行数 locale = locale(encoding = &quot;UTF-8&quot;) # Windows標準（Shift JIS）で作成されたファイルは&quot;CP932&quot;、utf-8で作成されたファイルは&quot;UTF-8&quot; ) ## Rows: 30 Columns: 4 ## ── Column specification ──────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): 日付 ## dbl (2): 変数1, 変数2 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. data ## # A tibble: 30 × 4 ## file_path 日付 変数1 変数2 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 data_read/csv_row/dfr_1.csv 2000/1/1 0.945 0.927 ## 2 data_read/csv_row/dfr_1.csv 2000/1/2 0.208 0.942 ## 3 data_read/csv_row/dfr_1.csv 2000/1/3 0.386 0.631 ## 4 data_read/csv_row/dfr_1.csv 2000/1/4 0.360 0.690 ## 5 data_read/csv_row/dfr_1.csv 2000/1/5 0.218 0.411 ## 6 data_read/csv_row/dfr_1.csv 2000/1/6 0.630 0.227 ## 7 data_read/csv_row/dfr_1.csv 2000/1/7 0.661 0.461 ## 8 data_read/csv_row/dfr_1.csv 2000/1/8 0.358 0.214 ## 9 data_read/csv_row/dfr_1.csv 2000/1/9 0.175 0.917 ## 10 data_read/csv_row/dfr_1.csv 2000/1/10 0.674 0.519 ## # … with 20 more rows 行が共通する複数のCSVファイルを読み込んで横方向に結合する場合は、purrrパッケージのmap_dfc()関数を使用します。なお、横方向に結合するため、行数を統一しておく必要があります。 # 読み込み対象のファイル名一覧を取得 file_names &lt;- fs::dir_ls(path = &quot;data_read/csv_column&quot;, # CSVファイルを格納しているフォルダのパスを入力 glob = &quot;*.csv&quot; # CSVファイル名を読み込む対象として指定（変更する必要なし） ) data &lt;- purrr::map_dfc(file_names, readr::read_csv, col_names = TRUE, # ヘッダー（列名データ）の有無／列名指定 col_types = NULL, # 各列の型の指定（c：文字列型、d：数値型、D：日付型、l：論理値型） col_select = NULL, # 読み込む列の指定（列名、インデックス） skip = 0, # 読み込み時に上からスキップする行数 locale = locale(encoding = &quot;UTF-8&quot;) # Windows標準（Shift JIS）で作成されたファイルは&quot;CP932&quot;、utf-8で作成されたファイルは&quot;UTF-8&quot; ) ## Rows: 5 Columns: 2 ## ── Column specification ──────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): 属性1, 属性2 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## Rows: 5 Columns: 2 ## ── Column specification ──────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): 属性3 ## dbl (1): 変数1 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## Rows: 5 Columns: 2 ## ── Column specification ──────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (2): 変数2, 変数3 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. data ## # A tibble: 5 × 6 ## 属性1 属性2 属性3 変数1 変数2 変数3 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A あ ア 0.914 0.540 0.203 ## 2 B い イ 0.972 0.150 0.992 ## 3 C う ウ 0.788 0.750 0.531 ## 4 D え エ 0.0367 0.384 0.146 ## 5 E お オ 0.750 0.00701 0.926 Excelファイルの複数のシートに格納されているデータを読み込んで結合する場合は、ループを使用します。ここでは、共通の属性列（日付列など）をもつデータをdplyr::full_join()関数で横方向に結合します。属性列の内容が同じ行を結合するため、必ずしも行数が統一されている必要はありません。 なお、dplyr::full_join()関数などのデータフレーム結合関数の詳細は、第3章「tidyverseによるデータ操作」を参照してください。 # Excelファイルのシート名一覧を取得 sheet_names &lt;- openxlsx::getSheetNames(file = &quot;data_read/xlsx/df.xlsx&quot;) # シート名のループ for (i in seq_along(sheet_names)) { # 最初のシートの読み込み時にデータフレームを作成 if (i == 1) { data &lt;- readxl::read_excel(path = &quot;data_read/xlsx/df.xlsx&quot;, # ファイルパス（拡張子が必要、URLは不可） sheet = sheet_names[i], # シートインデックス／シート名 col_names = TRUE, # ヘッダー（列名データ）の有無／列名指定 col_types = NULL, # 各列の型の指定（c：文字列型、d：数値型、D：日付型、l：論理値型） skip = 0 # 読み込み時に上からスキップする行数 ) # 2回目以降のシート読み込み時は日付列で全要素結合 } else { data %&lt;&gt;% dplyr::full_join(readxl::read_excel(path = &quot;data_read/xlsx/df.xlsx&quot;, # ファイルパス（拡張子が必要、URLは不可） sheet = sheet_names[i], # シートインデックス／シート名 col_names = TRUE, # ヘッダー（列名データ）の有無／列名指定 col_types = NULL, # 各列の型の指定（c：文字列型、d：数値型、D：日付型、l：論理値型） skip = 0 # 読み込み時に上からスキップする行数 ), by = &quot;日付&quot; # 結合に用いる列名を指定 ) } } data ## # A tibble: 10 × 7 ## 日付 変数1 変数2 変数3 変数4 変数5 変数6 ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2000-01-01 00:00:00 0.148 0.512 0.523 0.949 0.0863 0.762 ## 2 2000-01-02 00:00:00 0.682 0.154 0.529 0.883 0.00264 0.617 ## 3 2000-01-03 00:00:00 0.835 0.150 0.336 0.249 0.631 0.735 ## 4 2000-01-04 00:00:00 0.914 0.622 0.332 0.890 0.327 0.421 ## 5 2000-01-05 00:00:00 0.113 0.829 0.605 0.434 0.00965 0.505 ## 6 2000-01-06 00:00:00 0.494 0.539 0.155 0.977 0.948 0.895 ## 7 2000-01-07 00:00:00 0.720 0.356 0.968 0.496 0.299 0.363 ## 8 2000-01-08 00:00:00 0.553 0.838 0.826 0.497 0.930 0.721 ## 9 2000-01-09 00:00:00 0.0735 0.383 0.157 0.488 0.666 0.0194 ## 10 2000-01-10 00:00:00 0.402 0.877 0.0423 0.0308 0.765 0.0864 2.12.5 Excelの日付値の修正 Excelのファイルを読み込むと、Excelでは日付として表示されていたデータが、Rではシリアル値（単なる数字）に変換されてしまう場合があります。as.Date()関数でorigin引数にシリアル値の開始日を設定すると、Rでも正しく日付が表示されます。 # 1998年7月5日を表すシリアル値を日付に変換 as.Date(35981, origin = &quot;1899-12-30&quot;) ## [1] &quot;1998-07-05&quot; なお、上記のコードはWindows版のExcelで作成したファイルを読み込む際に有効です。Mac版のExcelで作成したファイルは開始日の設定が異なります。 # Mac版のExcelで1998年7月5日を表すシリアル値を日付に変換 as.Date(34519, origin = &quot;1904-01-01&quot;) 2.13 データの書き出し 2.13.1 CSVデータの書き出し CSVデータを書き出す（CSVファイルとして保存する）には、write.csv()関数を使用します。書き出したCSVデータが文字化けしている場合は、fileEncoding引数に適切な文字コードを指定してください。 write.csv(..., # 書き出すオブジェクト名（クオーテーションは不要） file = &quot;directory/file.csv&quot;, # 書き出し先のファイルパス（拡張子が必要） row.names = FALSE, # 行番号を付与するか fileEncoding = &quot;CP932&quot; # Windows標準（Shift JIS）は&quot;CP932&quot;、utf-8は&quot;UTF-8&quot; ) 2.14 オブジェクトのセーブ・ロード 2.14.1 セーブ RのオブジェクトをRData形式で保存します。 save(..., # セーブするオブジェクト名（クオーテーションは不要） file = &quot;directory/file.RData&quot; # セーブ先のファイルパス（拡張子が必要） ) 2.14.2 ロード 保存したRData形式のオブジェクトを読み込みます。 load(file = &quot;directory/file.RData&quot;) # ロード元ファイルパス（拡張子が必要） "],["tidyverseによるデータ操作-1.html", "3 tidyverseによるデータ操作 3.1 第3章の準備 3.2 tidyverseとは 3.3 列の選択 3.4 列名の変更 3.5 行のスライスとサンプリング 3.6 行のフィルタ 3.7 行の並べ替え 3.8 列の作成・修正 3.9 グループ化 3.10 集計 3.11 カウント 3.12 縦型・横型の変換 3.13 データの結合 3.14 重複処理 3.15 欠損値処理 3.16 補完処理", " 3 tidyverseによるデータ操作 第3章「tidyverseによるデータ操作」では、tidyverseのdplyrパッケージとtidyrパッケージを使用した、直感的かつ効率的なデータ操作について解説します。 3.1 第3章の準備 3.1.1 パッケージのインポート library(magrittr) library(tidyverse) library(tidyquant) 3.1.2 外部データセットの取得 この章では、外部データセットとして以下のデータを使用します。第1章のコードを使用してあらかじめウェブからデータセットを取得してください。 OWIDのCOVID-19データセット： data_owid 3.2 tidyverseとは tidyverseは、Rでデータを直感的・効率的に操作・可視化するために開発された様々なパッケージを、ひとまとめにしたものです。本節では、tidyverseに含まれるパッケージのうち、データ操作に関わるdplyrパッケージとtidyrパッケージを主に使用します。 dplyr：データに様々な操作を加えるパッケージ tidyr：データをtidy dataに変形するためのパッケージ tidy dataとは、tidyverseの開発者であるHadley Wickham氏が提唱した概念で、機械処理しやすいデータ構造のことを言います。具体的には、以下の条件を満たすデータです。 1つの列が1つの変数を表す 1つの行が1つのレコードを表す 1つのテーブルが1つのデータセットだけを含む 3.2.1 tidy data（縦型・横型データ） 例えば、国別・産業別GDPの時系列データであれば、国の列、産業の列、時点の列、データ（GDP）の列、の4列で構成されるデータがtidy dataです。こうしたデータを縦型データとも呼びます。パネルデータは一般的に縦型データの構造になっています。 一方、国や産業が横方向に並んでいる場合（例：日本・製造業のGDPの列、日本・飲食業のGDPの列、米国・金融業のGDPの列…など）は、tidy data（縦型データ）ではなく、横型データと呼ばれます。 tidy dataは属性条件によるフィルタがかけやすいなど、データ処理が列方向に一括して行えるため、機械処理に適しています。tidy dataの詳細については、松村 他（2021）を参照してください。 3.2.2 tidyverseの特徴 dplyrパッケージやtidyrパッケージの関数は、もとのデータに対して変更を一切加えません。データを操作した結果を残しておくためには、結果をオブジェクトに代入する必要があります。なお、代入先をもとのデータのオブジェクトにすると、データの内容が書き換えられます。 tidyverseでは、magrittrパッケージの機能の一つであるパイプ%&gt;%が多用されます。パイプは、データに対して適用した関数の結果を、次の関数へと受け渡すものです。パイプを連続して使用することで、処理の途中の結果をいちいちオブジェクトに代入することなく、一括して複数の処理を行うことができます。 このコードは、mpgデータセットに対する複数の関数の処理をパイプでつなげたものです。 # mpgデータセットからmanufacturer列とcty列を選択し、manufacturer列でフィルタしたうえで、 # cty列を10倍し、結果をresultオブジェクトに保存する例 result &lt;- mpg %&gt;% dplyr::select(manufacturer, cty) %&gt;% dplyr::filter(manufacturer == &quot;toyota&quot;) %&gt;% dplyr::mutate(cty_10 = cty * 10) print(result) ## # A tibble: 34 × 3 ## manufacturer cty cty_10 ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 toyota 15 150 ## 2 toyota 16 160 ## 3 toyota 15 150 ## 4 toyota 15 150 ## 5 toyota 16 160 ## 6 toyota 14 140 ## 7 toyota 21 210 ## 8 toyota 21 210 ## 9 toyota 21 210 ## 10 toyota 21 210 ## # … with 24 more rows 3.3 列の選択 データセットから列（変数）を選択するには、dplyr::select()関数を使用します。 3.3.1 列名で選択 data_owid %&gt;% dplyr::select(location, date, new_cases) ## # A tibble: 341,376 × 3 ## location date new_cases ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; ## 1 Afghanistan 2020-01-03 0 ## 2 Afghanistan 2020-01-04 0 ## 3 Afghanistan 2020-01-05 0 ## 4 Afghanistan 2020-01-06 0 ## 5 Afghanistan 2020-01-07 0 ## 6 Afghanistan 2020-01-08 0 ## 7 Afghanistan 2020-01-09 0 ## 8 Afghanistan 2020-01-10 0 ## 9 Afghanistan 2020-01-11 0 ## 10 Afghanistan 2020-01-12 0 ## # … with 341,366 more rows 3.3.2 列名を格納したベクトルで選択 cols &lt;- c(&quot;location&quot;, &quot;date&quot;, &quot;new_cases&quot;) data_owid %&gt;% dplyr::select(cols) ## # A tibble: 341,376 × 3 ## location date new_cases ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; ## 1 Afghanistan 2020-01-03 0 ## 2 Afghanistan 2020-01-04 0 ## 3 Afghanistan 2020-01-05 0 ## 4 Afghanistan 2020-01-06 0 ## 5 Afghanistan 2020-01-07 0 ## 6 Afghanistan 2020-01-08 0 ## 7 Afghanistan 2020-01-09 0 ## 8 Afghanistan 2020-01-10 0 ## 9 Afghanistan 2020-01-11 0 ## 10 Afghanistan 2020-01-12 0 ## # … with 341,366 more rows 3.3.3 列を非選択（削除） data_owid %&gt;% dplyr::select(-location) ## # A tibble: 341,376 × 13 ## continent date total_…¹ new_c…² new_c…³ total…⁴ new_d…⁵ total…⁶ new_c…⁷ ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Asia 2020-01-03 NA 0 NA NA 0 NA 0 ## 2 Asia 2020-01-04 NA 0 NA NA 0 NA 0 ## 3 Asia 2020-01-05 NA 0 NA NA 0 NA 0 ## 4 Asia 2020-01-06 NA 0 NA NA 0 NA 0 ## 5 Asia 2020-01-07 NA 0 NA NA 0 NA 0 ## 6 Asia 2020-01-08 NA 0 0 NA 0 NA 0 ## 7 Asia 2020-01-09 NA 0 0 NA 0 NA 0 ## 8 Asia 2020-01-10 NA 0 0 NA 0 NA 0 ## 9 Asia 2020-01-11 NA 0 0 NA 0 NA 0 ## 10 Asia 2020-01-12 NA 0 0 NA 0 NA 0 ## # … with 341,366 more rows, 4 more variables: total_deaths_per_million &lt;dbl&gt;, ## # new_deaths_per_million &lt;dbl&gt;, people_fully_vaccinated &lt;dbl&gt;, ## # stringency_index &lt;dbl&gt;, and abbreviated variable names ¹​total_cases, ## # ²​new_cases, ³​new_cases_smoothed, ⁴​total_deaths, ⁵​new_deaths, ## # ⁶​total_cases_per_million, ⁷​new_cases_per_million 3.3.4 列名に特定の文字列を含む列を選択 data_owid %&gt;% dplyr::select(location, date, contains(&quot;cases&quot;)) ## # A tibble: 341,376 × 7 ## location date total_cases new_cases new_cases_smoo…¹ total…² new_c…³ ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 2020-01-03 NA 0 NA NA 0 ## 2 Afghanistan 2020-01-04 NA 0 NA NA 0 ## 3 Afghanistan 2020-01-05 NA 0 NA NA 0 ## 4 Afghanistan 2020-01-06 NA 0 NA NA 0 ## 5 Afghanistan 2020-01-07 NA 0 NA NA 0 ## 6 Afghanistan 2020-01-08 NA 0 0 NA 0 ## 7 Afghanistan 2020-01-09 NA 0 0 NA 0 ## 8 Afghanistan 2020-01-10 NA 0 0 NA 0 ## 9 Afghanistan 2020-01-11 NA 0 0 NA 0 ## 10 Afghanistan 2020-01-12 NA 0 0 NA 0 ## # … with 341,366 more rows, and abbreviated variable names ¹​new_cases_smoothed, ## # ²​total_cases_per_million, ³​new_cases_per_million 3.3.5 列名が特定の文字列から始まる列を選択 data_owid %&gt;% dplyr::select(location, date, starts_with(&quot;new_cases&quot;)) ## # A tibble: 341,376 × 5 ## location date new_cases new_cases_smoothed new_cases_per_million ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 2020-01-03 0 NA 0 ## 2 Afghanistan 2020-01-04 0 NA 0 ## 3 Afghanistan 2020-01-05 0 NA 0 ## 4 Afghanistan 2020-01-06 0 NA 0 ## 5 Afghanistan 2020-01-07 0 NA 0 ## 6 Afghanistan 2020-01-08 0 0 0 ## 7 Afghanistan 2020-01-09 0 0 0 ## 8 Afghanistan 2020-01-10 0 0 0 ## 9 Afghanistan 2020-01-11 0 0 0 ## 10 Afghanistan 2020-01-12 0 0 0 ## # … with 341,366 more rows 3.3.6 列名が特定の文字列で終わる列を選択 data_owid %&gt;% dplyr::select(location, date, ends_with(c(&quot;cases&quot;, &quot;deaths&quot;))) ## # A tibble: 341,376 × 6 ## location date total_cases new_cases total_deaths new_deaths ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 2020-01-03 NA 0 NA 0 ## 2 Afghanistan 2020-01-04 NA 0 NA 0 ## 3 Afghanistan 2020-01-05 NA 0 NA 0 ## 4 Afghanistan 2020-01-06 NA 0 NA 0 ## 5 Afghanistan 2020-01-07 NA 0 NA 0 ## 6 Afghanistan 2020-01-08 NA 0 NA 0 ## 7 Afghanistan 2020-01-09 NA 0 NA 0 ## 8 Afghanistan 2020-01-10 NA 0 NA 0 ## 9 Afghanistan 2020-01-11 NA 0 NA 0 ## 10 Afghanistan 2020-01-12 NA 0 NA 0 ## # … with 341,366 more rows 3.3.7 特定の型の列を選択 data_owid %&gt;% dplyr::select(where(is.character) | where(is.Date)) ## # A tibble: 341,376 × 3 ## continent location date ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 Asia Afghanistan 2020-01-03 ## 2 Asia Afghanistan 2020-01-04 ## 3 Asia Afghanistan 2020-01-05 ## 4 Asia Afghanistan 2020-01-06 ## 5 Asia Afghanistan 2020-01-07 ## 6 Asia Afghanistan 2020-01-08 ## 7 Asia Afghanistan 2020-01-09 ## 8 Asia Afghanistan 2020-01-10 ## 9 Asia Afghanistan 2020-01-11 ## 10 Asia Afghanistan 2020-01-12 ## # … with 341,366 more rows 3.4 列名の変更 データセットの列名（変数名）を変更するときは、dplyr::rename()関数を使用します。 data_owid %&gt;% dplyr::rename(country = location) ## # A tibble: 341,376 × 14 ## continent country date total…¹ new_c…² new_c…³ total…⁴ new_d…⁵ total…⁶ ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Asia Afghani… 2020-01-03 NA 0 NA NA 0 NA ## 2 Asia Afghani… 2020-01-04 NA 0 NA NA 0 NA ## 3 Asia Afghani… 2020-01-05 NA 0 NA NA 0 NA ## 4 Asia Afghani… 2020-01-06 NA 0 NA NA 0 NA ## 5 Asia Afghani… 2020-01-07 NA 0 NA NA 0 NA ## 6 Asia Afghani… 2020-01-08 NA 0 0 NA 0 NA ## 7 Asia Afghani… 2020-01-09 NA 0 0 NA 0 NA ## 8 Asia Afghani… 2020-01-10 NA 0 0 NA 0 NA ## 9 Asia Afghani… 2020-01-11 NA 0 0 NA 0 NA ## 10 Asia Afghani… 2020-01-12 NA 0 0 NA 0 NA ## # … with 341,366 more rows, 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​total_cases, ²​new_cases, ³​new_cases_smoothed, ## # ⁴​total_deaths, ⁵​new_deaths, ⁶​total_cases_per_million 3.5 行のスライスとサンプリング 3.5.1 行のスライス データセットから行番号を指定して特定の行を取り出す（スライスする）には、dplyr::slice()関数の中で行インデックスを指定します。 df_2 &lt;- tibble( letters = LETTERS, numbers = 1:26 ) df_2 %&gt;% dplyr::slice(6:10) ## # A tibble: 5 × 2 ## letters numbers ## &lt;chr&gt; &lt;int&gt; ## 1 F 6 ## 2 G 7 ## 3 H 8 ## 4 I 9 ## 5 J 10 特定の行を切り捨てて、それ以外の行を取り出すには、dplyr::slice()関数の中でマイナスのインデックスを指定します。 df_2 %&gt;% dplyr::slice(-(6:10)) ## # A tibble: 21 × 2 ## letters numbers ## &lt;chr&gt; &lt;int&gt; ## 1 A 1 ## 2 B 2 ## 3 C 3 ## 4 D 4 ## 5 E 5 ## 6 K 11 ## 7 L 12 ## 8 M 13 ## 9 N 14 ## 10 O 15 ## # … with 11 more rows 3.5.2 サンプリング dplyr::slice()関数ファミリーであるdplyr::slice_sample()関数を使用すると、データセットからデータをランダムサンプリングすることができます。ここでは、53940サンプルあるdiamondsデータセットからサンプリングを行います。 diamonds ## # A tibble: 53,940 × 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 ## 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 ## 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 ## 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 ## 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 ## # … with 53,930 more rows 抽出するサンプル数は、dplyr::slice_sample()関数のn引数で指定します。 diamonds %&gt;% dplyr::slice_sample(n = 1000) # サンプル数 ## # A tibble: 1,000 × 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.92 Premium J SI1 62.9 58 2858 6.22 6.18 3.9 ## 2 1 Premium F SI2 61.6 62 4333 6.37 6.3 3.9 ## 3 0.9 Ideal G VS1 61.8 56 4931 6.19 6.24 3.84 ## 4 1.02 Ideal I VS2 61.5 57 4662 6.49 6.45 3.98 ## 5 0.23 Very Good E VVS1 62.1 58 530 3.9 3.93 2.43 ## 6 0.59 Ideal F SI1 62.3 54 1424 5.37 5.41 3.36 ## 7 1 Very Good D SI2 63.3 56 4704 6.38 6.35 4.03 ## 8 1.03 Premium F VVS1 59.7 60 10546 6.63 6.57 3.94 ## 9 1.1 Very Good I SI2 62.6 59 4192 6.53 6.57 4.1 ## 10 1.01 Fair F SI1 65.9 58 4276 6.16 6.1 4.04 ## # … with 990 more rows 抽出するサンプル数を、データの個数ではなく、元データセットの何割、といった形で決める場合は、dplyr::slice_sample()関数のporp引数に割合を指定します。 diamonds %&gt;% dplyr::slice_sample(prop = 0.1) # サンプル割合 ## # A tibble: 5,394 × 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.01 Premium I VS2 61.1 60 17433 8.06 8.12 4.94 ## 2 0.41 Ideal E VVS2 61.9 56 1187 4.75 4.78 2.95 ## 3 0.74 Premium G VS1 62.9 60 2800 5.74 5.68 3.59 ## 4 2.01 Very Good J VS2 60.8 58 12968 8.08 8.16 4.94 ## 5 0.9 Premium I SI2 62.2 59 2826 6.11 6.07 3.79 ## 6 1.04 Premium I SI2 61.6 56 3960 6.56 6.49 4.02 ## 7 0.41 Ideal D VVS2 62.5 57 1105 4.72 4.75 2.96 ## 8 1 Ideal D SI1 62.7 57 6073 6.34 6.38 3.99 ## 9 1.51 Very Good D VS2 63.2 57 12311 7.27 7.25 4.59 ## 10 1.52 Good F VS2 57.8 59 12283 7.58 7.5 4.36 ## # … with 5,384 more rows 3.6 行のフィルタ データセットの行のフィルタ（特定の条件を満たすデータの抽出）を行うには、dplyr::filter()関数を使用します。 3.6.1 条件に一致する行 data_owid %&gt;% dplyr::filter(location == &quot;Japan&quot;) ## # A tibble: 1,350 × 14 ## continent location date total…¹ new_c…² new_c…³ total…⁴ new_d…⁵ total…⁶ ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Asia Japan 2020-01-03 NA 0 NA NA 0 NA ## 2 Asia Japan 2020-01-04 NA 0 NA NA 0 NA ## 3 Asia Japan 2020-01-05 NA 0 NA NA 0 NA ## 4 Asia Japan 2020-01-06 NA 0 NA NA 0 NA ## 5 Asia Japan 2020-01-07 NA 0 NA NA 0 NA ## 6 Asia Japan 2020-01-08 NA 0 0 NA 0 NA ## 7 Asia Japan 2020-01-09 NA 0 0 NA 0 NA ## 8 Asia Japan 2020-01-10 NA 0 0 NA 0 NA ## 9 Asia Japan 2020-01-11 NA 0 0 NA 0 NA ## 10 Asia Japan 2020-01-12 NA 0 0 NA 0 NA ## # … with 1,340 more rows, 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​total_cases, ²​new_cases, ³​new_cases_smoothed, ## # ⁴​total_deaths, ⁵​new_deaths, ⁶​total_cases_per_million 3.6.2 条件に一致しない行（NOT条件） NOT条件は!=演算子を使用します。 data_owid %&gt;% dplyr::filter(continent != &quot;Asia&quot;) ## # A tibble: 257,532 × 14 ## continent location date total…¹ new_c…² new_c…³ total…⁴ new_d…⁵ total…⁶ ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Europe Albania 2020-01-03 NA 0 NA NA 0 NA ## 2 Europe Albania 2020-01-04 NA 0 NA NA 0 NA ## 3 Europe Albania 2020-01-05 NA 0 NA NA 0 NA ## 4 Europe Albania 2020-01-06 NA 0 NA NA 0 NA ## 5 Europe Albania 2020-01-07 NA 0 NA NA 0 NA ## 6 Europe Albania 2020-01-08 NA 0 0 NA 0 NA ## 7 Europe Albania 2020-01-09 NA 0 0 NA 0 NA ## 8 Europe Albania 2020-01-10 NA 0 0 NA 0 NA ## 9 Europe Albania 2020-01-11 NA 0 0 NA 0 NA ## 10 Europe Albania 2020-01-12 NA 0 0 NA 0 NA ## # … with 257,522 more rows, 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​total_cases, ²​new_cases, ³​new_cases_smoothed, ## # ⁴​total_deaths, ⁵​new_deaths, ⁶​total_cases_per_million 3.6.3 複数条件 dplyr::filter()関数内で複数条件を指定すると、左から順番に条件が適用されます。 data_owid %&gt;% dplyr::filter(location == &quot;Japan&quot;, date &gt;= &quot;2021-01-01&quot;, date &lt;= &quot;2021-01-07&quot;) ## # A tibble: 7 × 14 ## continent location date total_…¹ new_c…² new_c…³ total…⁴ new_d…⁵ total…⁶ ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Asia Japan 2021-01-01 238012 7708 4005. 3460 46 1920. ## 2 Asia Japan 2021-01-02 240954 2942 3915. 3514 54 1944. ## 3 Asia Japan 2021-01-03 243847 2893 3791. 3548 34 1967. ## 4 Asia Japan 2021-01-04 247960 4113 3961. 3599 51 2000. ## 5 Asia Japan 2021-01-05 252317 4357 4171 3655 56 2036. ## 6 Asia Japan 2021-01-06 258393 6076 4542. 3719 64 2085. ## 7 Asia Japan 2021-01-07 265299 6906 4999. 3791 72 2140. ## # … with 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​total_cases, ²​new_cases, ³​new_cases_smoothed, ## # ⁴​total_deaths, ⁵​new_deaths, ⁶​total_cases_per_million 3.6.4 AND・OR条件を明示的に指定した複数条件 左から順番に条件を適用しないためには、&amp;演算子と|演算子で明示的にAND条件とOR条件を指定します。 data_owid %&gt;% dplyr::filter((date == &quot;2022-01-01&quot;) &amp; (location == &quot;Japan&quot; | location == &quot;United States&quot;)) ## # A tibble: 2 × 14 ## continent locat…¹ date total…² new_c…³ new_c…⁴ total…⁵ new_d…⁶ total…⁷ ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Asia Japan 2022-01-01 1.73e6 504 397. 18393 0 13992. ## 2 North Amer… United… 2022-01-01 5.41e7 584647 354503. 820389 1334 159978. ## # … with 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​location, ²​total_cases, ³​new_cases, ⁴​new_cases_smoothed, ## # ⁵​total_deaths, ⁶​new_deaths, ⁷​total_cases_per_million 3.6.5 %in%演算子によるOR条件 複数の値が格納されたベクトルと%in%演算子を用いて、OR条件で行をフィルタします。この場合は、日本と米国のレコードを抽出しています。 locations &lt;- c(&quot;Japan&quot;, &quot;United States&quot;) data_owid %&gt;% dplyr::filter(location %in% locations, date == &quot;2021-01-01&quot;) ## # A tibble: 2 × 14 ## continent locat…¹ date total…² new_c…³ new_c…⁴ total…⁵ new_d…⁶ total…⁷ ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Asia Japan 2021-01-01 2.38e5 7708 4005. 3460 46 1920. ## 2 North Amer… United… 2021-01-01 1.98e7 235818 183836. 355767 3763 58569. ## # … with 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​location, ²​total_cases, ³​new_cases, ⁴​new_cases_smoothed, ## # ⁵​total_deaths, ⁶​new_deaths, ⁷​total_cases_per_million 3.6.6 OR条件の否定 %in%演算子によるOR条件を!で否定します。この場合は、日本・米国以外を抽出しています。 data_owid %&gt;% dplyr::filter(!location %in% locations, date == &quot;2021-01-01&quot;) ## # A tibble: 250 × 14 ## continent locat…¹ date total…² new_c…³ new_c…⁴ total…⁵ new_d…⁶ total…⁷ ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Asia Afghan… 2021-01-01 52513 183 1.31e+2 2201 12 1277. ## 2 &lt;NA&gt; Africa 2021-01-01 2777642 29790 2.31e+4 65745 693 1947. ## 3 Europe Albania 2021-01-01 57727 581 4.87e+2 1174 4 20310. ## 4 Africa Algeria 2021-01-01 99610 299 3.72e+2 2756 5 2218. ## 5 Oceania Americ… 2021-01-01 NA 0 0 NA 0 NA ## 6 Europe Andorra 2021-01-01 8049 66 5 e+1 84 0 100810. ## 7 Africa Angola 2021-01-01 17553 120 7.49e+1 405 0 493. ## 8 North Ame… Anguil… 2021-01-01 13 0 2.86e-1 NA 0 819. ## 9 North Ame… Antigu… 2021-01-01 159 1 7.14e-1 5 0 1696. ## 10 South Ame… Argent… 2021-01-01 1677741 3422 8.34e+3 48337 66 36865. ## # … with 240 more rows, 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​location, ²​total_cases, ³​new_cases, ⁴​new_cases_smoothed, ## # ⁵​total_deaths, ⁶​new_deaths, ⁷​total_cases_per_million 3.6.7 関数による条件指定 data_owid %&gt;% dplyr::filter(date == max(date)) ## # A tibble: 18 × 14 ## continent locat…¹ date total…² new_c…³ new_c…⁴ total…⁵ new_d…⁶ total…⁷ ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 North Ame… Aruba 2023-09-18 NA NA NA NA NA NA ## 2 &lt;NA&gt; Asia 2023-09-18 NA NA NA NA NA NA ## 3 Asia Bangla… 2023-09-18 NA NA NA NA NA NA ## 4 Europe Bulgar… 2023-09-18 NA NA NA NA NA NA ## 5 Europe Croatia 2023-09-18 NA NA NA NA NA NA ## 6 Europe Czechia 2023-09-18 NA NA NA NA NA NA ## 7 &lt;NA&gt; Europe 2023-09-18 NA NA NA NA NA NA ## 8 &lt;NA&gt; Europe… 2023-09-18 NA NA NA NA NA NA ## 9 &lt;NA&gt; High i… 2023-09-18 NA NA NA NA NA NA ## 10 Asia India 2023-09-18 NA NA NA NA NA NA ## 11 Europe Lithua… 2023-09-18 NA NA NA NA NA NA ## 12 &lt;NA&gt; Lower … 2023-09-18 NA NA NA NA NA NA ## 13 Europe Nether… 2023-09-18 NA NA NA NA NA NA ## 14 &lt;NA&gt; North … 2023-09-18 NA NA NA NA NA NA ## 15 &lt;NA&gt; South … 2023-09-18 NA NA NA NA NA NA ## 16 &lt;NA&gt; Upper … 2023-09-18 NA NA NA NA NA NA ## 17 South Ame… Uruguay 2023-09-18 NA NA NA NA NA NA ## 18 &lt;NA&gt; World 2023-09-18 NA NA NA NA NA NA ## # … with 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​location, ²​total_cases, ³​new_cases, ⁴​new_cases_smoothed, ## # ⁵​total_deaths, ⁶​new_deaths, ⁷​total_cases_per_million 3.6.8 論理値を返す関数による条件指定 is.na()関数のように倫理値を返す関数は、==演算子がなくてもフィルタ条件として使用することができます。 data_owid %&gt;% dplyr::filter(is.na(new_cases)) ## # A tibble: 9,483 × 14 ## continent locat…¹ date total…² new_c…³ new_c…⁴ total…⁵ new_d…⁶ total…⁷ ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 North Ame… Antigu… 2021-04-30 1224 NA NA 32 0 13053. ## 2 South Ame… Argent… 2020-01-01 NA NA NA NA NA NA ## 3 South Ame… Argent… 2020-01-02 NA NA NA NA NA NA ## 4 South Ame… Argent… 2023-09-14 NA NA NA NA NA NA ## 5 South Ame… Argent… 2023-09-15 NA NA NA NA NA NA ## 6 South Ame… Argent… 2023-09-16 NA NA NA NA NA NA ## 7 South Ame… Argent… 2023-09-17 NA NA NA NA NA NA ## 8 North Ame… Aruba 2021-10-12 15534 NA NA 170 1 145915. ## 9 North Ame… Aruba 2023-09-14 NA NA NA NA NA NA ## 10 North Ame… Aruba 2023-09-15 NA NA NA NA NA NA ## # … with 9,473 more rows, 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​location, ²​total_cases, ³​new_cases, ⁴​new_cases_smoothed, ## # ⁵​total_deaths, ⁶​new_deaths, ⁷​total_cases_per_million 3.6.9 複数列の一括条件指定 複数の列に対して一括して条件を指定する場合、OR条件ならif_any()関数、AND条件ならif_all()関数を用います。.cols引数にstarts_with()、ends_with()、where()関数といった列を選択する関数を、.fns引数に条件式や論理値を返す関数を指定します。 条件式は~ {.}で表される無名関数（ラムダ式）の形で記述することができます。なお、波括弧の中のドットは、.cols引数に指定した各列の要素を一つずつドットの位置に代入することを意味します。 # 列名がcasesで終わる列の値がすべて1000以上である行を抽出 data_owid %&gt;% dplyr::filter(location == &quot;Japan&quot;, if_all(.cols = ends_with(&quot;cases&quot;), .fns = ~ {. &gt;= 1000}) ) ## # A tibble: 813 × 14 ## continent location date total…¹ new_c…² new_c…³ total…⁴ new_d…⁵ total…⁶ ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Asia Japan 2020-07-30 33049 1148 860 1004 3 267. ## 2 Asia Japan 2020-07-31 34372 1323 917. 1006 2 277. ## 3 Asia Japan 2020-08-01 35836 1464 1007. 1011 5 289. ## 4 Asia Japan 2020-08-03 38687 1998 1243. 1012 1 312. ## 5 Asia Japan 2020-08-04 39858 1171 1271 1016 4 322. ## 6 Asia Japan 2020-08-05 41129 1271 1318. 1022 6 332. ## 7 Asia Japan 2020-08-06 42263 1134 1316. 1026 4 341. ## 8 Asia Japan 2020-08-07 43815 1552 1349 1033 7 353. ## 9 Asia Japan 2020-08-08 45439 1624 1372. 1039 6 367. ## 10 Asia Japan 2020-08-09 46783 1344 1442 1040 1 377. ## # … with 803 more rows, 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​total_cases, ²​new_cases, ³​new_cases_smoothed, ## # ⁴​total_deaths, ⁵​new_deaths, ⁶​total_cases_per_million # 数値型の変数の何れかがNAである行を抽出 data_owid %&gt;% dplyr::filter(location == &quot;Japan&quot;, if_any(.cols = where(is.double), .fns = is.na) ) ## # A tibble: 698 × 14 ## continent location date total…¹ new_c…² new_c…³ total…⁴ new_d…⁵ total…⁶ ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Asia Japan 2020-01-03 NA 0 NA NA 0 NA ## 2 Asia Japan 2020-01-04 NA 0 NA NA 0 NA ## 3 Asia Japan 2020-01-05 NA 0 NA NA 0 NA ## 4 Asia Japan 2020-01-06 NA 0 NA NA 0 NA ## 5 Asia Japan 2020-01-07 NA 0 NA NA 0 NA ## 6 Asia Japan 2020-01-08 NA 0 0 NA 0 NA ## 7 Asia Japan 2020-01-09 NA 0 0 NA 0 NA ## 8 Asia Japan 2020-01-10 NA 0 0 NA 0 NA ## 9 Asia Japan 2020-01-11 NA 0 0 NA 0 NA ## 10 Asia Japan 2020-01-12 NA 0 0 NA 0 NA ## # … with 688 more rows, 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​total_cases, ²​new_cases, ³​new_cases_smoothed, ## # ⁴​total_deaths, ⁵​new_deaths, ⁶​total_cases_per_million 3.7 行の並べ替え データセットの行を並べ替えるには、dplyr::arrange()関数を使用します。 3.7.1 昇順ソート data_owid %&gt;% dplyr::arrange(new_cases) ## # A tibble: 341,376 × 14 ## continent location date total…¹ new_c…² new_c…³ total…⁴ new_d…⁵ total…⁶ ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Asia Afghani… 2020-01-03 NA 0 NA NA 0 NA ## 2 Asia Afghani… 2020-01-04 NA 0 NA NA 0 NA ## 3 Asia Afghani… 2020-01-05 NA 0 NA NA 0 NA ## 4 Asia Afghani… 2020-01-06 NA 0 NA NA 0 NA ## 5 Asia Afghani… 2020-01-07 NA 0 NA NA 0 NA ## 6 Asia Afghani… 2020-01-08 NA 0 0 NA 0 NA ## 7 Asia Afghani… 2020-01-09 NA 0 0 NA 0 NA ## 8 Asia Afghani… 2020-01-10 NA 0 0 NA 0 NA ## 9 Asia Afghani… 2020-01-11 NA 0 0 NA 0 NA ## 10 Asia Afghani… 2020-01-12 NA 0 0 NA 0 NA ## # … with 341,366 more rows, 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​total_cases, ²​new_cases, ³​new_cases_smoothed, ## # ⁴​total_deaths, ⁵​new_deaths, ⁶​total_cases_per_million 3.7.2 降順ソート data_owid %&gt;% dplyr::arrange(-new_cases) ## # A tibble: 341,376 × 14 ## continent location date total…¹ new_c…² new_c…³ total…⁴ new_d…⁵ total…⁶ ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &lt;NA&gt; World 2022-01-30 3.75e8 8401961 3.35e6 5715121 17569 47060. ## 2 &lt;NA&gt; World 2022-01-23 3.52e8 8346578 3.36e6 5647286 15843 44123. ## 3 &lt;NA&gt; World 2022-12-23 6.91e8 7852910 5.40e6 6687113 5093 86674. ## 4 &lt;NA&gt; Asia 2022-12-23 2.38e8 7213802 5.08e6 1515220 1282 50483. ## 5 &lt;NA&gt; World 2022-01-16 3.28e8 7197242 2.95e6 5587551 13372 41171. ## 6 &lt;NA&gt; World 2022-02-06 3.95e8 7078280 2.85e6 5790995 18447 49559. ## 7 &lt;NA&gt; High in… 2022-01-23 1.83e8 7065175 2.30e6 2138356 10513 146594. ## 8 &lt;NA&gt; Upper m… 2022-12-23 1.91e8 7038613 4.92e6 2549041 1189 75651. ## 9 &lt;NA&gt; High in… 2022-01-30 1.99e8 7006472 2.24e6 2174389 11212 159155. ## 10 Asia China 2022-12-23 5.04e7 6966046 4.84e6 36318 894 35380. ## # … with 341,366 more rows, 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​total_cases, ²​new_cases, ³​new_cases_smoothed, ## # ⁴​total_deaths, ⁵​new_deaths, ⁶​total_cases_per_million 3.7.3 降順ソート（日付型） 日付型のデータは-演算子で降順ソートができないため、desc()関数を使用します。 data_owid %&gt;% dplyr::arrange(desc(date)) ## # A tibble: 341,376 × 14 ## continent locat…¹ date total…² new_c…³ new_c…⁴ total…⁵ new_d…⁶ total…⁷ ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 North Ame… Aruba 2023-09-18 NA NA NA NA NA NA ## 2 &lt;NA&gt; Asia 2023-09-18 NA NA NA NA NA NA ## 3 Asia Bangla… 2023-09-18 NA NA NA NA NA NA ## 4 Europe Bulgar… 2023-09-18 NA NA NA NA NA NA ## 5 Europe Croatia 2023-09-18 NA NA NA NA NA NA ## 6 Europe Czechia 2023-09-18 NA NA NA NA NA NA ## 7 &lt;NA&gt; Europe 2023-09-18 NA NA NA NA NA NA ## 8 &lt;NA&gt; Europe… 2023-09-18 NA NA NA NA NA NA ## 9 &lt;NA&gt; High i… 2023-09-18 NA NA NA NA NA NA ## 10 Asia India 2023-09-18 NA NA NA NA NA NA ## # … with 341,366 more rows, 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​location, ²​total_cases, ³​new_cases, ⁴​new_cases_smoothed, ## # ⁵​total_deaths, ⁶​new_deaths, ⁷​total_cases_per_million 3.7.4 複数条件によるソート dplyr::arrange()関数内で複数条件を指定すると、左から順番に適用します。 data_owid %&gt;% dplyr::arrange(desc(date), -new_cases) ## # A tibble: 341,376 × 14 ## continent locat…¹ date total…² new_c…³ new_c…⁴ total…⁵ new_d…⁶ total…⁷ ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 North Ame… Aruba 2023-09-18 NA NA NA NA NA NA ## 2 &lt;NA&gt; Asia 2023-09-18 NA NA NA NA NA NA ## 3 Asia Bangla… 2023-09-18 NA NA NA NA NA NA ## 4 Europe Bulgar… 2023-09-18 NA NA NA NA NA NA ## 5 Europe Croatia 2023-09-18 NA NA NA NA NA NA ## 6 Europe Czechia 2023-09-18 NA NA NA NA NA NA ## 7 &lt;NA&gt; Europe 2023-09-18 NA NA NA NA NA NA ## 8 &lt;NA&gt; Europe… 2023-09-18 NA NA NA NA NA NA ## 9 &lt;NA&gt; High i… 2023-09-18 NA NA NA NA NA NA ## 10 Asia India 2023-09-18 NA NA NA NA NA NA ## # … with 341,366 more rows, 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​location, ²​total_cases, ³​new_cases, ⁴​new_cases_smoothed, ## # ⁵​total_deaths, ⁶​new_deaths, ⁷​total_cases_per_million 3.8 列の作成・修正 データセットの列を追加・修正するには、dplyr::mutate()関数を使用します。 まず、使用するサンプルデータを作成します。 data_owid_jp &lt;- data_owid %&gt;% dplyr::select(location, date, new_cases, new_deaths) %&gt;% dplyr::filter(location == &quot;Japan&quot;, date &gt;= &quot;2022-01-01&quot; ) 3.8.1 新たな列の作成 既存の列（変数）の計算結果として、新たな列を追加します。=演算子の左側が新たに作成する列名、右側が計算式です。作成する際、.before引数もしくは.after引数に既存の列名を指定すると、指定した列の前後に新たな列を挿入します。.before引数、.after引数を指定しなければ、新たな列は最右列に追加されます。 data_owid_jp %&gt;% dplyr::mutate(death_rate = new_deaths / new_cases, .after = &quot;date&quot; ) ## # A tibble: 621 × 5 ## location date death_rate new_cases new_deaths ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Japan 2022-01-01 0 504 0 ## 2 Japan 2022-01-02 0.00388 516 2 ## 3 Japan 2022-01-03 0.00128 783 1 ## 4 Japan 2022-01-04 0.000796 1256 1 ## 5 Japan 2022-01-05 0.000399 2506 1 ## 6 Japan 2022-01-06 0 4194 0 ## 7 Japan 2022-01-07 0.000334 5983 2 ## 8 Japan 2022-01-08 0.000252 7930 2 ## 9 Japan 2022-01-09 0.000123 8144 1 ## 10 Japan 2022-01-10 0.000313 6394 2 ## # … with 611 more rows 3.8.2 既存の列の修正 =演算子の左側に既存の列名を指定すると、当該列を修正します。 data_owid %&gt;% dplyr::mutate(location = stringr::str_c(location, &quot;_&quot;, continent)) ## # A tibble: 341,376 × 14 ## continent location date total…¹ new_c…² new_c…³ total…⁴ new_d…⁵ total…⁶ ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Asia Afghani… 2020-01-03 NA 0 NA NA 0 NA ## 2 Asia Afghani… 2020-01-04 NA 0 NA NA 0 NA ## 3 Asia Afghani… 2020-01-05 NA 0 NA NA 0 NA ## 4 Asia Afghani… 2020-01-06 NA 0 NA NA 0 NA ## 5 Asia Afghani… 2020-01-07 NA 0 NA NA 0 NA ## 6 Asia Afghani… 2020-01-08 NA 0 0 NA 0 NA ## 7 Asia Afghani… 2020-01-09 NA 0 0 NA 0 NA ## 8 Asia Afghani… 2020-01-10 NA 0 0 NA 0 NA ## 9 Asia Afghani… 2020-01-11 NA 0 0 NA 0 NA ## 10 Asia Afghani… 2020-01-12 NA 0 0 NA 0 NA ## # … with 341,366 more rows, 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​total_cases, ²​new_cases, ³​new_cases_smoothed, ## # ⁴​total_deaths, ⁵​new_deaths, ⁶​total_cases_per_million 3.8.3 条件付き系列の作成 dplyr::case_when()関数の中に、「既存の系列を用いた条件 ~ 条件を満たす場合にとる値」の形で条件式を書き、新たな系列を作成します。複数条件を指定した場合、左から順番に条件が適用されます。「その他すべて」の条件はTRUEで指定します。 # 単一条件を指定してダミー変数を作成 data_owid_jp %&gt;% dplyr::mutate(dummy = dplyr::case_when(new_cases &lt; 3000 ~ 1, TRUE ~ 0 ) ) ## # A tibble: 621 × 5 ## location date new_cases new_deaths dummy ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Japan 2022-01-01 504 0 1 ## 2 Japan 2022-01-02 516 2 1 ## 3 Japan 2022-01-03 783 1 1 ## 4 Japan 2022-01-04 1256 1 1 ## 5 Japan 2022-01-05 2506 1 1 ## 6 Japan 2022-01-06 4194 0 0 ## 7 Japan 2022-01-07 5983 2 0 ## 8 Japan 2022-01-08 7930 2 0 ## 9 Japan 2022-01-09 8144 1 0 ## 10 Japan 2022-01-10 6394 2 0 ## # … with 611 more rows # 複数条件を指定 data_owid_jp %&gt;% dplyr::mutate(case = dplyr::case_when(new_cases &lt; 1000 ~ &quot;A&quot;, (new_cases &gt;= 5000 &amp; new_cases &lt; 8000) ~ &quot;B&quot;, TRUE ~ &quot;other&quot; ) ) ## # A tibble: 621 × 5 ## location date new_cases new_deaths case ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Japan 2022-01-01 504 0 A ## 2 Japan 2022-01-02 516 2 A ## 3 Japan 2022-01-03 783 1 A ## 4 Japan 2022-01-04 1256 1 other ## 5 Japan 2022-01-05 2506 1 other ## 6 Japan 2022-01-06 4194 0 other ## 7 Japan 2022-01-07 5983 2 B ## 8 Japan 2022-01-08 7930 2 B ## 9 Japan 2022-01-09 8144 1 other ## 10 Japan 2022-01-10 6394 2 B ## # … with 611 more rows 3.8.4 複数列の一括処理 dplyr::mutate()関数内でacross()関数を用い、.cols引数に対象の列を、.fns引数に処理方法を指定して、一括処理します。 .fns引数には、通常の関数に加え、~ {.}で表す無名関数（ラムダ式）を指定することも可能です。なお、無名関数~ {.}の波括弧の中のドットは、.cols引数に指定した各列の要素を一つずつドットの位置に代入することを意味します。 # new_casesからnew_deathまでのすべての列の要素を1000で除す data_owid_jp %&gt;% dplyr::mutate(across(.cols = new_cases:new_deaths, .fns = ~ {. / 1000})) ## # A tibble: 621 × 4 ## location date new_cases new_deaths ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Japan 2022-01-01 0.504 0 ## 2 Japan 2022-01-02 0.516 0.002 ## 3 Japan 2022-01-03 0.783 0.001 ## 4 Japan 2022-01-04 1.26 0.001 ## 5 Japan 2022-01-05 2.51 0.001 ## 6 Japan 2022-01-06 4.19 0 ## 7 Japan 2022-01-07 5.98 0.002 ## 8 Japan 2022-01-08 7.93 0.002 ## 9 Japan 2022-01-09 8.14 0.001 ## 10 Japan 2022-01-10 6.39 0.002 ## # … with 611 more rows # new_casesからnew_deathまでのすべての列の要素の前期比変化率を計算 data_owid_jp %&gt;% dplyr::mutate(across(.cols = new_cases:new_deaths, .fns = ~ {. / dplyr::lag(., n = 1)})) ## # A tibble: 621 × 4 ## location date new_cases new_deaths ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Japan 2022-01-01 NA NA ## 2 Japan 2022-01-02 1.02 Inf ## 3 Japan 2022-01-03 1.52 0.5 ## 4 Japan 2022-01-04 1.60 1 ## 5 Japan 2022-01-05 2.00 1 ## 6 Japan 2022-01-06 1.67 0 ## 7 Japan 2022-01-07 1.43 Inf ## 8 Japan 2022-01-08 1.33 1 ## 9 Japan 2022-01-09 1.03 0.5 ## 10 Japan 2022-01-10 0.785 2 ## # … with 611 more rows # すべての列を文字列型に変換 data_owid_jp %&gt;% dplyr::mutate(across(.cols = everything(), .fns = as.character)) ## # A tibble: 621 × 4 ## location date new_cases new_deaths ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Japan 2022-01-01 504 0 ## 2 Japan 2022-01-02 516 2 ## 3 Japan 2022-01-03 783 1 ## 4 Japan 2022-01-04 1256 1 ## 5 Japan 2022-01-05 2506 1 ## 6 Japan 2022-01-06 4194 0 ## 7 Japan 2022-01-07 5983 2 ## 8 Japan 2022-01-08 7930 2 ## 9 Japan 2022-01-09 8144 1 ## 10 Japan 2022-01-10 6394 2 ## # … with 611 more rows # 文字列型の列を因子型に変換 data_owid_jp %&gt;% dplyr::mutate(across(.cols = where(is.character), .fns = as.factor)) ## # A tibble: 621 × 4 ## location date new_cases new_deaths ## &lt;fct&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Japan 2022-01-01 504 0 ## 2 Japan 2022-01-02 516 2 ## 3 Japan 2022-01-03 783 1 ## 4 Japan 2022-01-04 1256 1 ## 5 Japan 2022-01-05 2506 1 ## 6 Japan 2022-01-06 4194 0 ## 7 Japan 2022-01-07 5983 2 ## 8 Japan 2022-01-08 7930 2 ## 9 Japan 2022-01-09 8144 1 ## 10 Japan 2022-01-10 6394 2 ## # … with 611 more rows 3.9 グループ化 データセットを属性ごとにグループ化するには、dplyr::group_by()関数を使用します。グループ化するだけでは変化はありませんが、dplyr::filter()関数や、次のdplyr::summarise()関数とあわせて使用することで、より柔軟なデータセット操作が可能になります。 3.9.1 単一の列でグループ化 見た目は変わりませんが、データをprint()関数で出力するとデータの属性を示す冒頭箇所の2行目にGroupsが追加されています。 data_owid %&gt;% dplyr::group_by(location) %&gt;% print() ## # A tibble: 341,376 × 14 ## # Groups: location [255] ## continent location date total…¹ new_c…² new_c…³ total…⁴ new_d…⁵ total…⁶ ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Asia Afghani… 2020-01-03 NA 0 NA NA 0 NA ## 2 Asia Afghani… 2020-01-04 NA 0 NA NA 0 NA ## 3 Asia Afghani… 2020-01-05 NA 0 NA NA 0 NA ## 4 Asia Afghani… 2020-01-06 NA 0 NA NA 0 NA ## 5 Asia Afghani… 2020-01-07 NA 0 NA NA 0 NA ## 6 Asia Afghani… 2020-01-08 NA 0 0 NA 0 NA ## 7 Asia Afghani… 2020-01-09 NA 0 0 NA 0 NA ## 8 Asia Afghani… 2020-01-10 NA 0 0 NA 0 NA ## 9 Asia Afghani… 2020-01-11 NA 0 0 NA 0 NA ## 10 Asia Afghani… 2020-01-12 NA 0 0 NA 0 NA ## # … with 341,366 more rows, 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​total_cases, ²​new_cases, ³​new_cases_smoothed, ## # ⁴​total_deaths, ⁵​new_deaths, ⁶​total_cases_per_million 3.9.2 複数の列でグループ化 data_owid %&gt;% dplyr::group_by(continent, location) %&gt;% print() ## # A tibble: 341,376 × 14 ## # Groups: continent, location [255] ## continent location date total…¹ new_c…² new_c…³ total…⁴ new_d…⁵ total…⁶ ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Asia Afghani… 2020-01-03 NA 0 NA NA 0 NA ## 2 Asia Afghani… 2020-01-04 NA 0 NA NA 0 NA ## 3 Asia Afghani… 2020-01-05 NA 0 NA NA 0 NA ## 4 Asia Afghani… 2020-01-06 NA 0 NA NA 0 NA ## 5 Asia Afghani… 2020-01-07 NA 0 NA NA 0 NA ## 6 Asia Afghani… 2020-01-08 NA 0 0 NA 0 NA ## 7 Asia Afghani… 2020-01-09 NA 0 0 NA 0 NA ## 8 Asia Afghani… 2020-01-10 NA 0 0 NA 0 NA ## 9 Asia Afghani… 2020-01-11 NA 0 0 NA 0 NA ## 10 Asia Afghani… 2020-01-12 NA 0 0 NA 0 NA ## # … with 341,366 more rows, 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​total_cases, ²​new_cases, ³​new_cases_smoothed, ## # ⁴​total_deaths, ⁵​new_deaths, ⁶​total_cases_per_million 3.9.3 グループ化＆フィルタ グループ化と行のフィルタを組み合わせて、グループ別にフィルタを適用します。ここでは、グループ別の最大値を抽出しています。 data_owid %&gt;% dplyr::group_by(location) %&gt;% dplyr::filter(new_cases == max(new_cases, na.rm = TRUE)) ## # A tibble: 2,945 × 14 ## # Groups: location [246] ## continent locat…¹ date total…² new_c…³ new_c…⁴ total…⁵ new_d…⁶ total…⁷ ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Asia Afghan… 2021-06-17 9.65e4 3243 1.52e3 3842 159 2347. ## 2 &lt;NA&gt; Africa 2022-02-02 1.12e7 72463 3.39e4 240339 534 7839. ## 3 Europe Albania 2022-01-20 2.36e5 2832 1.97e3 3277 6 83202. ## 4 Africa Algeria 2022-01-26 2.41e5 2521 1.98e3 6516 8 5376. ## 5 Oceania Americ… 2022-03-25 3.22e3 955 3.08e2 1 0 72627. ## 6 Europe Andorra 2022-01-26 3.47e4 1676 5.89e2 145 1 434615. ## 7 Africa Angola 2022-01-02 8.24e4 5611 1.61e3 1772 15 2315. ## 8 North Ame… Anguil… 2022-05-11 2.98e3 196 2.8 e1 9 0 187945. ## 9 North Ame… Antigu… 2022-02-11 7.32e3 468 8.41e1 134 3 78072. ## 10 South Ame… Argent… 2022-01-11 6.72e6 135401 1.05e5 119760 268 147667. ## # … with 2,935 more rows, 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​location, ²​total_cases, ³​new_cases, ⁴​new_cases_smoothed, ## # ⁵​total_deaths, ⁶​new_deaths, ⁷​total_cases_per_million 3.9.4 グループ化の解除 dplyr::ungroup()関数でグループ化を解除します。グループ化の有無でフィルタや集計の結果が変わるため、思わぬ事故を防ぐためにも、所定の結果を得た後はグループ化を解除しておくことを推奨します。 data_owid %&gt;% dplyr::group_by(location) %&gt;% dplyr::ungroup() ## # A tibble: 341,376 × 14 ## continent location date total…¹ new_c…² new_c…³ total…⁴ new_d…⁵ total…⁶ ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Asia Afghani… 2020-01-03 NA 0 NA NA 0 NA ## 2 Asia Afghani… 2020-01-04 NA 0 NA NA 0 NA ## 3 Asia Afghani… 2020-01-05 NA 0 NA NA 0 NA ## 4 Asia Afghani… 2020-01-06 NA 0 NA NA 0 NA ## 5 Asia Afghani… 2020-01-07 NA 0 NA NA 0 NA ## 6 Asia Afghani… 2020-01-08 NA 0 0 NA 0 NA ## 7 Asia Afghani… 2020-01-09 NA 0 0 NA 0 NA ## 8 Asia Afghani… 2020-01-10 NA 0 0 NA 0 NA ## 9 Asia Afghani… 2020-01-11 NA 0 0 NA 0 NA ## 10 Asia Afghani… 2020-01-12 NA 0 0 NA 0 NA ## # … with 341,366 more rows, 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​total_cases, ²​new_cases, ³​new_cases_smoothed, ## # ⁴​total_deaths, ⁵​new_deaths, ⁶​total_cases_per_million 3.10 集計 グループ化したデータセットに対してdplyr::summarise()関数を使用すると、グループ別に集計操作を行うことができます。 3.10.1 グループ別の集計 # 指定した列のグループ別の平均値を計算 data_owid %&gt;% dplyr::group_by(location) %&gt;% dplyr::summarise(new_cases_mean = mean(new_cases, na.rm = TRUE)) ## # A tibble: 255 × 2 ## location new_cases_mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 Afghanistan 167. ## 2 Africa 9710. ## 3 Albania 247. ## 4 Algeria 201. ## 5 American Samoa 6.18 ## 6 Andorra 35.6 ## 7 Angola 78.1 ## 8 Anguilla 2.89 ## 9 Antigua and Barbuda 6.76 ## 10 Argentina 7441. ## # … with 245 more rows # 指定した列のグループ別の最大値を計算 data_owid %&gt;% dplyr::group_by(location) %&gt;% dplyr::summarise(new_cases_max = max(new_cases, na.rm = TRUE)) ## # A tibble: 255 × 2 ## location new_cases_max ## &lt;chr&gt; &lt;dbl&gt; ## 1 Afghanistan 3243 ## 2 Africa 72463 ## 3 Albania 2832 ## 4 Algeria 2521 ## 5 American Samoa 955 ## 6 Andorra 1676 ## 7 Angola 5611 ## 8 Anguilla 196 ## 9 Antigua and Barbuda 468 ## 10 Argentina 135401 ## # … with 245 more rows 3.10.2 クロス集計 dplyr::group_by()関数で複数条件を指定してグループ化すると、クロス集計を行うことができます。 data_owid %&gt;% dplyr::group_by(location, lubridate::year(date)) %&gt;% dplyr::summarise(new_cases_mean = mean(new_cases, na.rm = TRUE)) ## `summarise()` has grouped output by &#39;location&#39;. You can override using the ## `.groups` argument. ## # A tibble: 1,014 × 3 ## # Groups: location [255] ## location `lubridate::year(date)` new_cases_mean ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 2020 144. ## 2 Afghanistan 2021 290. ## 3 Afghanistan 2022 136. ## 4 Afghanistan 2023 70.4 ## 5 Africa 2020 7549. ## 6 Africa 2021 19461. ## 7 Africa 2022 8630. ## 8 Africa 2023 420. ## 9 Albania 2020 157. ## 10 Albania 2021 416. ## # … with 1,004 more rows 3.10.3 複数列の一括処理 dplyr::summarise()関数内でacross()関数を用い、対象の列と処理方法を指定して一括処理します。 # new_casesとnew_deathsの列について、グループ別の合計値を計算 data_owid %&gt;% dplyr::group_by(location) %&gt;% dplyr::summarise(across(c(new_cases, new_deaths), sum, na.rm = TRUE)) ## # A tibble: 255 × 3 ## location new_cases new_deaths ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 225568 7945 ## 2 Africa 13108761 259020 ## 3 Albania 334090 3604 ## 4 Algeria 271852 6881 ## 5 American Samoa 8341 34 ## 6 Andorra 48015 159 ## 7 Angola 105384 1934 ## 8 Anguilla 3904 12 ## 9 Antigua and Barbuda 9114 146 ## 10 Argentina 10044957 130472 ## # … with 245 more rows # 列名がcasesで終わる列すべてについて、グループ別の合計値を計算 # 列名per_millionで終わる列すべてについて、グループ別の平均値を計算 data_owid %&gt;% dplyr::group_by(location) %&gt;% dplyr::summarise(across(ends_with(&quot;cases&quot;), sum, na.rm = TRUE), across(ends_with(&quot;per_million&quot;), mean, na.rm = TRUE) ) ## # A tibble: 255 × 7 ## location total_cases new_cases total_cas…¹ new_c…² total…³ new_d…⁴ ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 171182917 225568 3212. 4.06 132. 0.143 ## 2 Africa 10373043225 13108761 5567. 6.81 121. 0.134 ## 3 Albania 247467566 334090 67914. 87.1 867. 0.939 ## 4 Algeria 233561985 271852 4013. 4.48 109. 0.114 ## 5 American Samoa 4193856 8341 130413. 139. 722. 0.569 ## 6 Andorra 33533848 48015 325579. 445. 1542. 1.48 ## 7 Angola 79806178 105384 1775. 2.19 36.9 0.0402 ## 8 Anguilla 2312576 3904 115052. 182. 602. 0.560 ## 9 Antigua and Barbuda 6011304 9114 50161. 72.0 916. 1.15 ## 10 Argentina 7558580637 10044957 128948. 163. 2034. 2.12 ## # … with 245 more rows, and abbreviated variable names ## # ¹​total_cases_per_million, ²​new_cases_per_million, ## # ³​total_deaths_per_million, ⁴​new_deaths_per_million # 数値型の列すべてについて、グループ別の平均値を計算 data_owid %&gt;% dplyr::group_by(location) %&gt;% dplyr::summarise(across(is.double, mean, na.rm = TRUE)) ## # A tibble: 255 × 13 ## location date total…¹ new_c…² new_c…³ total…⁴ new_d…⁵ total…⁶ new_c…⁷ ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanist… 2021-11-07 1.32e5 167. 168. 5.44e3 5.89e+0 3212. 4.06 ## 2 Africa 2021-11-07 7.94e6 9710. 9746. 1.73e5 1.92e+2 5567. 6.81 ## 3 Albania 2021-11-07 1.93e5 247. 248. 2.46e3 2.67e+0 67914. 87.1 ## 4 Algeria 2021-11-07 1.80e5 201. 202. 4.89e3 5.10e+0 4013. 4.48 ## 5 American … 2021-11-07 5.78e3 6.18 6.20 3.20e1 2.52e-2 130413. 139. ## 6 Andorra 2021-11-07 2.60e4 35.6 35.7 1.23e2 1.18e-1 325579. 445. ## 7 Angola 2021-11-07 6.32e4 78.1 78.4 1.31e3 1.43e+0 1775. 2.19 ## 8 Anguilla 2021-11-07 1.83e3 2.89 2.90 9.56e0 8.89e-3 115052. 182. ## 9 Antigua a… 2021-11-07 4.70e3 6.76 6.78 8.59e1 1.08e-1 50161. 72.0 ## 10 Argentina 2021-11-08 5.87e6 7441. 7468. 9.26e4 9.66e+1 128948. 163. ## # … with 245 more rows, 4 more variables: total_deaths_per_million &lt;dbl&gt;, ## # new_deaths_per_million &lt;dbl&gt;, people_fully_vaccinated &lt;dbl&gt;, ## # stringency_index &lt;dbl&gt;, and abbreviated variable names ¹​total_cases, ## # ²​new_cases, ³​new_cases_smoothed, ⁴​total_deaths, ⁵​new_deaths, ## # ⁶​total_cases_per_million, ⁷​new_cases_per_million 3.11 カウント 集計の特殊例として、変数の要素をカウントする場合はdplyr::count()関数を使用します。 3.11.1 1変数のカウント dplyr::count()関数内に変数を指定すると、変数の各要素のレコード（行）数が出力されます。 なお、カウント対象の変数は文字列型などの離散型変数を指定するのが一般的です。数値型などの連続型変数の場合は、カウントではなくヒストグラムを利用すべきでしょう。 data_owid %&gt;% dplyr::count(location) ## # A tibble: 255 × 2 ## location n ## &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1350 ## 2 Africa 1350 ## 3 Albania 1350 ## 4 Algeria 1350 ## 5 American Samoa 1350 ## 6 Andorra 1350 ## 7 Angola 1350 ## 8 Anguilla 1350 ## 9 Antigua and Barbuda 1350 ## 10 Argentina 1356 ## # … with 245 more rows 3.11.2 多変数のカウント dplyr::count()関数内に2つ以上の変数を指定すると、要素の掛け合わせについてカウントが行われます。 data_owid %&gt;% dplyr::count(continent, date) ## # A tibble: 9,480 × 3 ## continent date n ## &lt;chr&gt; &lt;date&gt; &lt;int&gt; ## 1 Africa 2020-01-03 57 ## 2 Africa 2020-01-04 57 ## 3 Africa 2020-01-05 57 ## 4 Africa 2020-01-06 57 ## 5 Africa 2020-01-07 57 ## 6 Africa 2020-01-08 57 ## 7 Africa 2020-01-09 57 ## 8 Africa 2020-01-10 57 ## 9 Africa 2020-01-11 57 ## 10 Africa 2020-01-12 57 ## # … with 9,470 more rows 3.11.3 条件付きカウント 条件付きカウントを行う場合は、dplyr::count()関数内でif_else()関数を使用して条件を設定します。 data_owid %&gt;% dplyr::count(location, if_else(new_cases &gt; dplyr::lag(new_cases, n = 7), true = &quot;increase&quot;, false = &quot;decrease&quot;)) ## # A tibble: 591 × 3 ## location `if_else(...)` n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan decrease 723 ## 2 Afghanistan increase 620 ## 3 Afghanistan &lt;NA&gt; 7 ## 4 Africa decrease 731 ## 5 Africa increase 619 ## 6 Albania decrease 821 ## 7 Albania increase 529 ## 8 Algeria decrease 785 ## 9 Algeria increase 565 ## 10 American Samoa decrease 1273 ## # … with 581 more rows または、あらかじめdplyr::mutate()関数内でif_else()関数を使用して条件に合致する行を1にする列を作成し、dplyr::summarise()関数でその列の合計を集計することで、条件付きカウントを行うことができます。 data_owid %&gt;% dplyr::mutate(increase = if_else(new_cases &gt; dplyr::lag(new_cases, n = 7), true = 1, false = 0)) %&gt;% dplyr::group_by(location) %&gt;% dplyr::summarise(across(increase, sum, na.rm = TRUE)) ## # A tibble: 255 × 2 ## location increase ## &lt;chr&gt; &lt;dbl&gt; ## 1 Afghanistan 620 ## 2 Africa 619 ## 3 Albania 529 ## 4 Algeria 565 ## 5 American Samoa 77 ## 6 Andorra 311 ## 7 Angola 440 ## 8 Anguilla 104 ## 9 Antigua and Barbuda 263 ## 10 Argentina 428 ## # … with 245 more rows 3.11.4 要素の種類をカウント 離散型変数について要素の種類の数をカウントするには、対象の離散型変数をtidyr::pivot_longer()関数で縦型データに変換し、dplyr::distinct()関数で重複を排除してカウントします。 ここでは、dplyr::select()関数の中でwhere(is.character)を使用し、文字列型の変数をカウント対象にしています。 data_owid %&gt;% dplyr::select(where(is.character)) %&gt;% tidyr::pivot_longer(cols = everything()) %&gt;% dplyr::distinct() %&gt;% dplyr::count(name) ## # A tibble: 2 × 2 ## name n ## &lt;chr&gt; &lt;int&gt; ## 1 continent 7 ## 2 location 255 3.12 縦型・横型の変換 tidyrパッケージのpivot_longer()関数とpivot_wider()関数を使用して、縦型データ（tidy data）と横型データの変換を行います。なお、縦型データは、ggplot2パッケージによるグラフ作成で多用します。 まず、使用するサンプルデータを作成します。 data_owid_cases &lt;- data_owid %&gt;% dplyr::select(location, date, new_cases) %&gt;% dplyr::filter(date &gt;= &quot;2021-01-01&quot;) %&gt;% dplyr::arrange(date) data_owid_cases ## # A tibble: 249,982 × 3 ## location date new_cases ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; ## 1 Afghanistan 2021-01-01 183 ## 2 Africa 2021-01-01 29790 ## 3 Albania 2021-01-01 581 ## 4 Algeria 2021-01-01 299 ## 5 American Samoa 2021-01-01 0 ## 6 Andorra 2021-01-01 66 ## 7 Angola 2021-01-01 120 ## 8 Anguilla 2021-01-01 0 ## 9 Antigua and Barbuda 2021-01-01 1 ## 10 Argentina 2021-01-01 3422 ## # … with 249,972 more rows 3.12.1 縦型データを横型データに変換 data_owid_cases_wide &lt;- data_owid_cases %&gt;% tidyr::pivot_wider(id_cols = &quot;date&quot;, names_from = &quot;location&quot;, values_from = &quot;new_cases&quot;) data_owid_cases_wide ## # A tibble: 991 × 256 ## date Afghanistan Africa Albania Algeria Americ…¹ Andorra Angola Angui…² ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2021-01-01 183 29790 581 299 0 66 120 0 ## 2 2021-01-02 73 25919 589 287 0 68 15 1 ## 3 2021-01-03 123 23808 407 262 0 49 40 1 ## 4 2021-01-04 200 22630 268 249 0 26 34 0 ## 5 2021-01-05 102 21957 447 237 0 57 42 0 ## 6 2021-01-06 94 27434 185 228 0 59 72 0 ## 7 2021-01-07 102 37010 660 247 0 40 108 0 ## 8 2021-01-08 125 35228 725 262 0 0 110 0 ## 9 2021-01-09 68 37148 697 275 0 141 92 0 ## 10 2021-01-10 89 36565 673 256 0 97 90 0 ## # … with 981 more rows, 247 more variables: `Antigua and Barbuda` &lt;dbl&gt;, ## # Argentina &lt;dbl&gt;, Armenia &lt;dbl&gt;, Aruba &lt;dbl&gt;, Asia &lt;dbl&gt;, Australia &lt;dbl&gt;, ## # Austria &lt;dbl&gt;, Azerbaijan &lt;dbl&gt;, Bahamas &lt;dbl&gt;, Bahrain &lt;dbl&gt;, ## # Bangladesh &lt;dbl&gt;, Barbados &lt;dbl&gt;, Belarus &lt;dbl&gt;, Belgium &lt;dbl&gt;, ## # Belize &lt;dbl&gt;, Benin &lt;dbl&gt;, Bermuda &lt;dbl&gt;, Bhutan &lt;dbl&gt;, Bolivia &lt;dbl&gt;, ## # `Bonaire Sint Eustatius and Saba` &lt;dbl&gt;, `Bosnia and Herzegovina` &lt;dbl&gt;, ## # Botswana &lt;dbl&gt;, Brazil &lt;dbl&gt;, `British Virgin Islands` &lt;dbl&gt;, … 3.12.2 横型データを縦型データに変換 data_owid_cases_long &lt;- data_owid_cases_wide %&gt;% tidyr::pivot_longer(cols = -&quot;date&quot;, names_to = &quot;location&quot;, values_to = &quot;new_cases&quot;) data_owid_cases_long ## # A tibble: 252,705 × 3 ## date location new_cases ## &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2021-01-01 Afghanistan 183 ## 2 2021-01-01 Africa 29790 ## 3 2021-01-01 Albania 581 ## 4 2021-01-01 Algeria 299 ## 5 2021-01-01 American Samoa 0 ## 6 2021-01-01 Andorra 66 ## 7 2021-01-01 Angola 120 ## 8 2021-01-01 Anguilla 0 ## 9 2021-01-01 Antigua and Barbuda 1 ## 10 2021-01-01 Argentina 3422 ## # … with 252,695 more rows 3.13 データの結合 複数のデータセットのオブジェクトを結合して一つのデータセットにするには、dplyrパッケージのjoin()関数ファミリーを使用します。join()関数は、結合方法によって4種類に分かれています。 まず、使用するサンプルデータを確認します。 band_members ## # A tibble: 3 × 2 ## name band ## &lt;chr&gt; &lt;chr&gt; ## 1 Mick Stones ## 2 John Beatles ## 3 Paul Beatles band_instruments ## # A tibble: 3 × 2 ## name plays ## &lt;chr&gt; &lt;chr&gt; ## 1 John guitar ## 2 Paul bass ## 3 Keith guitar 3.13.1 内部結合 dplyr::inner_join()関数は、両方のデータに共通して存在する行のみ結合し、その他の行は削除します。 dplyr::inner_join(band_members, band_instruments, by = &quot;name&quot;) ## # A tibble: 2 × 3 ## name band plays ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 John Beatles guitar ## 2 Paul Beatles bass 3.13.2 左外部結合 dplyr::left_join()関数は、左側のデータに存在する行のみ結合し、その他の行は削除します。 dplyr::left_join(band_members, band_instruments, by = &quot;name&quot;) ## # A tibble: 3 × 3 ## name band plays ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Mick Stones &lt;NA&gt; ## 2 John Beatles guitar ## 3 Paul Beatles bass 3.13.3 右外部結合 dplyr::right_join()関数は、右側のデータに存在する行のみ結合し、その他の行は削除します。 dplyr::right_join(band_members, band_instruments, by = &quot;name&quot;) ## # A tibble: 3 × 3 ## name band plays ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 John Beatles guitar ## 2 Paul Beatles bass ## 3 Keith &lt;NA&gt; guitar 3.13.4 完全外部結合 dplyr::full_join()関数は、両方のデータのすべての行を結合し、行を削除しません。 dplyr::full_join(band_members, band_instruments, by = &quot;name&quot;) ## # A tibble: 4 × 3 ## name band plays ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Mick Stones &lt;NA&gt; ## 2 John Beatles guitar ## 3 Paul Beatles bass ## 4 Keith &lt;NA&gt; guitar 3.14 重複処理 3.14.1 重複行の削除 重複している行を削除するには、dplyr::distinct()関数を使用します。 # すべての列を対象にして、重複している行を削除 mpg %&gt;% dplyr::distinct() ## # A tibble: 225 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto… f 18 29 p comp… ## 2 audi a4 1.8 1999 4 manu… f 21 29 p comp… ## 3 audi a4 2 2008 4 manu… f 20 31 p comp… ## 4 audi a4 2 2008 4 auto… f 21 30 p comp… ## 5 audi a4 2.8 1999 6 auto… f 16 26 p comp… ## 6 audi a4 2.8 1999 6 manu… f 18 26 p comp… ## 7 audi a4 3.1 2008 6 auto… f 18 27 p comp… ## 8 audi a4 quattro 1.8 1999 4 manu… 4 18 26 p comp… ## 9 audi a4 quattro 1.8 1999 4 auto… 4 16 25 p comp… ## 10 audi a4 quattro 2 2008 4 manu… 4 20 28 p comp… ## # … with 215 more rows # 指定した列を対象にして、重複している行を削除 mpg %&gt;% dplyr::distinct(manufacturer) ## # A tibble: 15 × 1 ## manufacturer ## &lt;chr&gt; ## 1 audi ## 2 chevrolet ## 3 dodge ## 4 ford ## 5 honda ## 6 hyundai ## 7 jeep ## 8 land rover ## 9 lincoln ## 10 mercury ## 11 nissan ## 12 pontiac ## 13 subaru ## 14 toyota ## 15 volkswagen mpg %&gt;% dplyr::distinct(manufacturer, model) ## # A tibble: 38 × 2 ## manufacturer model ## &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 ## 2 audi a4 quattro ## 3 audi a6 quattro ## 4 chevrolet c1500 suburban 2wd ## 5 chevrolet corvette ## 6 chevrolet k1500 tahoe 4wd ## 7 chevrolet malibu ## 8 dodge caravan 2wd ## 9 dodge dakota pickup 4wd ## 10 dodge durango 4wd ## # … with 28 more rows # 指定した列を対象にして、重複している行を削除し、その他の列も残す mpg %&gt;% dplyr::distinct(manufacturer, model, .keep_all = TRUE) ## # A tibble: 38 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto… f 18 29 p comp… ## 2 audi a4 quattro 1.8 1999 4 manu… 4 18 26 p comp… ## 3 audi a6 quattro 2.8 1999 6 auto… 4 15 24 p mids… ## 4 chevrolet c1500 sub… 5.3 2008 8 auto… r 14 20 r suv ## 5 chevrolet corvette 5.7 1999 8 manu… r 16 26 p 2sea… ## 6 chevrolet k1500 tah… 5.3 2008 8 auto… 4 14 19 r suv ## 7 chevrolet malibu 2.4 1999 4 auto… f 19 27 r mids… ## 8 dodge caravan 2… 2.4 1999 4 auto… f 18 24 r mini… ## 9 dodge dakota pi… 3.7 2008 6 manu… 4 15 19 r pick… ## 10 dodge durango 4… 3.9 1999 6 auto… 4 13 17 r suv ## # … with 28 more rows 3.14.2 重複行の抽出 重複している行を抽出するには、dplyr::group_by()関数、dplyr::filter()関数、dplyr::n()関数を組み合わせて使用します。dplyr::n()関数は、dplyr::group_by()関数で指定したグループのサイズを返す関数です。 # 指定した列を対象にして、重複している行を抽出 mpg %&gt;% dplyr::group_by(manufacturer, model, displ, year, cyl, trans, cty, hwy) %&gt;% dplyr::filter(dplyr::n() &gt; 1) ## # A tibble: 18 × 11 ## # Groups: manufacturer, model, displ, year, cyl, trans, cty, hwy [9] ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 chevrolet c1500 sub… 5.3 2008 8 auto… r 14 20 r suv ## 2 chevrolet c1500 sub… 5.3 2008 8 auto… r 14 20 r suv ## 3 dodge caravan 2… 3.3 1999 6 auto… f 16 22 r mini… ## 4 dodge caravan 2… 3.3 1999 6 auto… f 16 22 r mini… ## 5 dodge caravan 2… 3.3 2008 6 auto… f 17 24 r mini… ## 6 dodge caravan 2… 3.3 2008 6 auto… f 17 24 r mini… ## 7 dodge dakota pi… 4.7 2008 8 auto… 4 14 19 r pick… ## 8 dodge dakota pi… 4.7 2008 8 auto… 4 14 19 r pick… ## 9 dodge durango 4… 4.7 2008 8 auto… 4 13 17 r suv ## 10 dodge durango 4… 4.7 2008 8 auto… 4 13 17 r suv ## 11 dodge ram 1500 … 4.7 2008 8 manu… 4 12 16 r pick… ## 12 dodge ram 1500 … 4.7 2008 8 auto… 4 13 17 r pick… ## 13 dodge ram 1500 … 4.7 2008 8 auto… 4 13 17 r pick… ## 14 dodge ram 1500 … 4.7 2008 8 manu… 4 12 16 r pick… ## 15 ford explorer … 4 1999 6 auto… 4 14 17 r suv ## 16 ford explorer … 4 1999 6 auto… 4 14 17 r suv ## 17 honda civic 1.6 1999 4 auto… f 24 32 r subc… ## 18 honda civic 1.6 1999 4 auto… f 24 32 r subc… 3.15 欠損値処理 欠損値（NA）がある行を削除したり、NAのレコードを他の値で置き換えたりするには、tidyrパッケージのdrop_na()関数、replace_na()関数、fill()関数を使用します。 まず、使用するサンプルデータを作成します。 data_owid_vaccinated &lt;- data_owid %&gt;% dplyr::select(location, date, people_fully_vaccinated) %&gt;% dplyr::filter(location %in% c(&quot;Japan&quot;, &quot;United States&quot;, &quot;United Kingdom&quot;), date &gt;= &quot;2021-01-01&quot; ) %&gt;% dplyr::arrange(date) %&gt;% tidyr::pivot_wider(id_cols = &quot;date&quot;, names_from = &quot;location&quot;, values_from = &quot;people_fully_vaccinated&quot;) tail(data_owid_vaccinated) ## # A tibble: 6 × 4 ## date Japan `United Kingdom` `United States` ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2023-09-08 NA NA NA ## 2 2023-09-09 NA NA NA ## 3 2023-09-10 NA NA NA ## 4 2023-09-11 NA NA NA ## 5 2023-09-12 NA NA NA ## 6 2023-09-13 NA NA NA 3.15.1 NAの削除 特定の列を対象にして、NAが含まれている行を削除するには、tidyr::drop_na()関数で列名を指定します。 data_owid_vaccinated %&gt;% tidyr::drop_na(Japan) %&gt;% tail() ## # A tibble: 6 × 4 ## date Japan `United Kingdom` `United States` ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2023-05-02 103379807 NA 230612536 ## 2 2023-05-03 103379815 NA 230618038 ## 3 2023-05-04 103379837 NA 230623850 ## 4 2023-05-05 103379896 NA 230629861 ## 5 2023-05-06 103380268 NA 230632064 ## 6 2023-05-07 103380343 NA 230633139 すべての列を対象にして、NAが含まれている行を削除するには、tidyr::drop_na()関数の中でeverything()関数を使用します。 data_owid_vaccinated %&gt;% tidyr::drop_na(everything()) %&gt;% tail() ## # A tibble: 6 × 4 ## date Japan `United Kingdom` `United States` ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2022-09-06 102759896 50749294 225381078 ## 2 2022-09-07 102762932 50750722 225422965 ## 3 2022-09-08 102765787 50752513 225466224 ## 4 2022-09-09 102773861 50754555 225521944 ## 5 2022-09-10 102787059 50760564 225555658 ## 6 2022-09-11 102789363 50762968 225574327 3.15.2 NAの置換 特定の列のNAを別の値に置き換えるには、tidyr::replace_na()関数で列名と置換する値を指定します。 data_owid_vaccinated %&gt;% dplyr::mutate(Japan = tidyr::replace_na(data = Japan, replace = 0)) %&gt;% tail() ## # A tibble: 6 × 4 ## date Japan `United Kingdom` `United States` ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2023-09-08 0 NA NA ## 2 2023-09-09 0 NA NA ## 3 2023-09-10 0 NA NA ## 4 2023-09-11 0 NA NA ## 5 2023-09-12 0 NA NA ## 6 2023-09-13 0 NA NA NAを他の値に置き換える列が複数ある場合は、dplyr::mutate_at()関数で列名を複数指定して一括処理します。 data_owid_vaccinated %&gt;% dplyr::mutate_at(vars(-date), tidyr::replace_na, replace = 0) %&gt;% tail() ## # A tibble: 6 × 4 ## date Japan `United Kingdom` `United States` ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2023-09-08 0 0 0 ## 2 2023-09-09 0 0 0 ## 3 2023-09-10 0 0 0 ## 4 2023-09-11 0 0 0 ## 5 2023-09-12 0 0 0 ## 6 2023-09-13 0 0 0 3.15.3 NAのフィル 特定の列のNAを同じ列の前後の値でフィルするには、tidyr::fill()関数で列名を指定します。.direction引数が\"down\"なら上にある値を使用して下向きにフィル、\"up\"なら下にある値を使用して上向きにフィルします。 data_owid_vaccinated %&gt;% tidyr::fill(Japan, .direction = &quot;down&quot;) %&gt;% tail() ## # A tibble: 6 × 4 ## date Japan `United Kingdom` `United States` ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2023-09-08 103380343 NA NA ## 2 2023-09-09 103380343 NA NA ## 3 2023-09-10 103380343 NA NA ## 4 2023-09-11 103380343 NA NA ## 5 2023-09-12 103380343 NA NA ## 6 2023-09-13 103380343 NA NA すべての列のNAの値をフィルする場合は、tidyr::fill()関数の中でeverything()関数を使用します。 data_owid_vaccinated %&gt;% tidyr::fill(c(-date, everything()), .direction = &quot;down&quot;) %&gt;% tail() ## # A tibble: 6 × 4 ## date Japan `United Kingdom` `United States` ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2023-09-08 103380343 50762968 230637348 ## 2 2023-09-09 103380343 50762968 230637348 ## 3 2023-09-10 103380343 50762968 230637348 ## 4 2023-09-11 103380343 50762968 230637348 ## 5 2023-09-12 103380343 50762968 230637348 ## 6 2023-09-13 103380343 50762968 230637348 3.16 補完処理 データセットが特定の属性の組み合わせのレコード（行）を欠いている場合や、時系列データで特定の時点のレコード（行）が含まれていない場合は、tidyr::complete()関数で補完することができます。 まず、使用するサンプルデータを作成します。 # サンプルデータ1 data_complete_1 &lt;- tibble( group = c(1:2, 1, 2), item_id = c(1:2, 2, 3), item_name = c(&quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;), value1 = c(1, NA, 3, 4), value2 = 4:7 ) data_complete_1 ## # A tibble: 4 × 5 ## group item_id item_name value1 value2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 1 a 1 4 ## 2 2 2 a NA 5 ## 3 1 2 b 3 6 ## 4 2 3 b 4 7 # サンプルデータ2 data_complete_2 &lt;- tibble( date = as.Date(c(&quot;2022-01-01&quot;, &quot;2022-01-03&quot;, &quot;2022-01-04&quot;)), value = c(11, 13, 14) ) data_complete_2 ## # A tibble: 3 × 2 ## date value ## &lt;date&gt; &lt;dbl&gt; ## 1 2022-01-01 11 ## 2 2022-01-03 13 ## 3 2022-01-04 14 3.16.1 組み合わせ候補の補完 tidyr::complete()関数の中で列名を指定し、当該変数のすべての組み合わせ候補を補完します。補完した行の値はNAになります。 data_complete_1 %&gt;% tidyr::complete(group, item_id, item_name) ## # A tibble: 12 × 5 ## group item_id item_name value1 value2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 1 a 1 4 ## 2 1 1 b NA NA ## 3 1 2 a NA NA ## 4 1 2 b 3 6 ## 5 1 3 a NA NA ## 6 1 3 b NA NA ## 7 2 1 a NA NA ## 8 2 1 b NA NA ## 9 2 2 a NA 5 ## 10 2 2 b NA NA ## 11 2 3 a NA NA ## 12 2 3 b 4 7 tidyr::nesting()関数に複数の列名を指定すると、それらの列について実現値のユニークな組み合わせをあらかじめ求め、それと別の列とのすべての組み合わせ候補を補完します。 data_complete_1 %&gt;% tidyr::complete(group, tidyr::nesting(item_id, item_name)) ## # A tibble: 8 × 5 ## group item_id item_name value1 value2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 1 a 1 4 ## 2 1 2 a NA NA ## 3 1 2 b 3 6 ## 4 1 3 b NA NA ## 5 2 1 a NA NA ## 6 2 2 a NA 5 ## 7 2 2 b NA NA ## 8 2 3 b 4 7 3.16.2 時系列データの補完 時系列データの欠損期間を補完するには、full_seq()関数を使用します。 # 日次データの欠損日を補完 data_complete_2 %&gt;% tidyr::complete(date = full_seq(date, period = 1)) ## # A tibble: 4 × 2 ## date value ## &lt;date&gt; &lt;dbl&gt; ## 1 2022-01-01 11 ## 2 2022-01-02 NA ## 3 2022-01-03 13 ## 4 2022-01-04 14 "],["ggplot2によるグラフ作成-1.html", "4 ggplot2によるグラフ作成 4.1 第4章の準備 4.2 ggplot2とは 4.3 ggplot2の設定 4.4 GUI形式の直感的なグラフ作成 4.5 1次元の度数分布（離散型） 4.6 1次元の度数分布（連続型） 4.7 1次元の密度グラフ 4.8 2次元の度数分布（離散型） 4.9 2次元の度数分布（連続型） 4.10 2次元の密度グラフ 4.11 QQプロット 4.12 散布図・バブルチャート 4.13 棒グラフ 4.14 円グラフ 4.15 折れ線グラフ 4.16 ステップグラフ 4.17 面グラフ 4.18 箱ひげ図・ヴァイオリングラフ 4.19 ドットプロット 4.20 複合グラフ（ファセット） 4.21 地図ファセット 4.22 補助線 4.23 設定：色 4.24 設定：軸 4.25 設定：パネル目盛線 4.26 設定：凡例 4.27 設定：タイトル・キャプション 4.28 設定：フォント・マージン 4.29 グラフの保存 4.30 実例", " 4 ggplot2によるグラフ作成 第4章「ggplot2によるグラフ作成」では、tidyverseのggplot2パッケージを主に使用したグラフ作成方法について解説します。 4.1 第4章の準備 4.1.1 パッケージのインポート library(esquisse) library(geofacet) library(ggplotgui) library(ggpubr) library(ggsci) library(ggrepel) library(lemon) library(magrittr) library(openxlsx) library(RColorBrewer) library(readxl) library(tidyverse) 4.1.2 外部データセットの取得 この章では、外部データセットとして以下のデータを使用します。第1章のコードを使用してあらかじめウェブからデータセットを取得してください。 OWIDのCOVID-19データセット： data_owid 日本の県内総生産： data_gdp_pref 日本の四半期別GDP： data_gdp_jp 4.2 ggplot2とは ggplot2パッケージでは、最初にggplot()関数でグラフを作成するデータと変数を指定します。その後に、geom_histogram()などのグラフの種類を指定する関数や、scale_x_dateなどの軸設定の関数、theme()などの各種設定の関数を+演算子で追加していきます。 ggplot()関数では、まずdata引数にtibble形式の縦型データを指定します。次にmapping引数にaes()関数を指定し、aes()関数の中でX軸やY軸の変数、必要に応じてcolorやsizeなど変数の値に応じて変化させる色・サイズなどの要素を指定します。 なお、グラフの種類により、X軸、Y軸に指定できる変数の型（連続型、離散型）が決まっていますので、注意してください。 ggplot2パッケージの詳細については公式ウェブサイトを参照してください。各関数の使い方については、公式ウェブサイトのチートシートが分かりやすくまとまっています。 4.3 ggplot2の設定 第1章で紹介したggplot2の設定です。筆者の実行環境はMacのため、Windowsの設定はコメントアウトしています。 # Windowsのグラフ設定（筆者の実行環境では不要のためコメントアウト） # windowsFonts(&quot;MEIRYO&quot; = windowsFont(&quot;Meiryo UI&quot;)) # windowsFonts(&quot;YUGO&quot; = windowsFont(&quot;Yu Gothic UI&quot;)) # theme_set(theme_light(base_family = &quot;YUGO&quot;)) # Macのグラフ設定 theme_set(theme_light(base_family = &quot;HiraKakuProN-W3&quot;)) 4.4 GUI形式の直感的なグラフ作成 この章ではggplot2パッケージによるグラフ作成方法を紹介しますが、ggplot2はコードを記述してグラフを作成するため、試行錯誤を伴う探索的データ分析（EDA）にはあまり向いていません。 そこで、ggplot2によるグラフ作成を行う前に、GUI形式でコードを記述せずにグラフを作成できるパッケージを用い、どのようなグラフを作成するべきか検討します。 4.4.1 esquisseパッケージ esquisseパッケージは、ドラッグ＆ドロップでX軸・Y軸の変数や色・サイズを変化させる属性の変数を指定し、グラフを作成することができる便利なパッケージです。ただし、データセットの規模が大きいと起動に時間がかかるため、変数の数が多い場合は、あらかじめ注目する変数を絞るといった前処理が必要です。 Rで下記のコードを実行すると別のウィンドウでesquisseの画面が立ち上がり、GUI形式でインタラクティブにグラフを作成することができます。ウェブサイト上では実行できないため、各自で試してみてください。 # irisデータ esquisse::esquisser(data = iris) ## 世界の新型コロナデータの一部を抽出 data_owid %&gt;% dplyr::select(location, date, new_cases_smoothed) %&gt;% dplyr::filter(location %in% c(&quot;Japan&quot;, &quot;United States&quot;, &quot;United Kingdom&quot;, &quot;Germany&quot;, &quot;France&quot;, &quot;Italy&quot;)) %&gt;% esquisse::esquisser() 4.4.2 ggplotguiパッケージ その他、GUI形式でグラフを作成することができるパッケージとして、ggplotguiがあります。同様にウェブサイト上では実行できないため、各自で試してみてください。 # irisデータ ggplotgui::ggplot_shiny(data = iris) ## 世界の新型コロナデータの一部を抽出 data_owid %&gt;% dplyr::select(location, date, new_cases_smoothed) %&gt;% dplyr::filter(location %in% c(&quot;Japan&quot;, &quot;United States&quot;, &quot;United Kingdom&quot;, &quot;Germany&quot;, &quot;France&quot;, &quot;Italy&quot;)) %&gt;% ggplotgui::ggplot_shiny() 4.5 1次元の度数分布（離散型） X軸：離散型変数 Y軸：なし 離散型変数の1次元の度数分布を作成するには、度数棒グラフを用います。度数棒グラフを作成するにはgeom_bar()関数を使用します。 4.5.1 度数棒グラフの基本形 ggplot(data = mpg, mapping = aes(x = manufacturer) ) + geom_bar(alpha = 1.0, # 塗りつぶしの透明度 color = &quot;darkblue&quot;, # 線の色 fill = &quot;lightblue&quot;, # 塗りつぶしの色 size = 0.5, # 線の太さ width = 0.9 # 棒の幅 (0-1) ) 4.6 1次元の度数分布（連続型） X軸：連続型変数 Y軸：なし 連続型変数の1次元の度数分布を作成するには、ヒストグラムを用います。ヒストグラムを作成するにはgeom_histogram()関数やgeom_freqpoly()関数を使用します。 4.6.1 ヒストグラムの基本形 ggplot(data = mpg, mapping = aes(x = hwy) ) + geom_histogram(binwidth = 5, # 階級幅 color = &quot;gray&quot;, # 線の色 fill = &quot;darkblue&quot;, # 塗りつぶしの色 size = 0.5 # 線の太さ ) 4.6.2 横並びヒストグラム グループ別のヒストグラムを横並び形式で作成するには、position引数に\"dodge\"を指定します。 ggplot(data = mpg, mapping = aes(x = hwy, fill = class) ) + geom_histogram(binwidth = 5, # 階級幅 position = &quot;dodge&quot;, # 横並びポジション color = &quot;black&quot;, # 線の色 size = 0.5 # 線の太さ ) グループ別の度数分布はgeom_freqpoly()関数でも可視化することができます。 ggplot(data = mpg, mapping = aes(x = hwy, color = class) ) + geom_freqpoly(binwidth = 5, # 階級幅 size = 0.5 # 線の太さ ) 4.6.3 積み上げヒストグラム グループ別のヒストグラムを積み上げ形式で作成するには、position引数に\"stack\"を指定します。 ggplot(data = mpg, mapping = aes(x = hwy, fill = class) ) + geom_histogram(binwidth = 5, # 階級幅 position = &quot;stack&quot;, # 積み上げポジション color = &quot;black&quot;, # 線の色 size = 0.5 # 線の太さ ) 4.7 1次元の密度グラフ X軸：連続型変数 Y軸：なし 密度グラフはgeom_density()関数で作成します。 4.7.1 密度グラフの基本形 ggplot(data = mpg, mapping = aes(x = hwy) ) + geom_density(kernel = &quot;gaussian&quot;, # カーネル関数の種類 color = &quot;darkblue&quot;, # 線の色 linetype = &quot;solid&quot;, # 線の種類（solid / dashed / dotted / dotdash / twodash / longdash） size = 1.0 # 線の太さ ) 4.7.2 横並び密度グラフ グループ別の密度グラフを横並び形式で作成するには、position引数に\"dodge\"を指定します。 ggplot(data = mpg, mapping = aes(x = hwy, color = class) ) + geom_density(kernel = &quot;gaussian&quot;, # カーネル関数の種類 position = &quot;dodge&quot;, # 横並びポジション linetype = &quot;solid&quot;, # 線の種類（solid / dashed / dotted / dotdash / twodash / longdash） size = 1.0 # 線の太さ ) 横並びの密度グラフはgeom_freqpoly()関数でも作成できます。geom_freqpoly()関数でmapping = aes(y = ..density..)を指定すると、各グループの度数を標準化して密度グラフに変換します。 ggplot(data = mpg, mapping = aes(x = hwy, y = ..density.., color = class) ) + geom_freqpoly(binwidth = 5, # 階級幅 size = 0.5 # 線の太さ ) 4.7.3 積み上げ密度グラフ グループ別の密度グラフを積み上げ形式で作成するには、position引数に\"stack\"を指定します。 ggplot(data = mpg, mapping = aes(x = hwy, fill = class) ) + geom_density(kernel = &quot;gaussian&quot;, # カーネル関数の種類 position = &quot;stack&quot;, # 積み上げポジション color = &quot;black&quot;, # 線の色 linetype = &quot;solid&quot;, # 線の種類（solid / dashed / dotted / dotdash / twodash / longdash） size = 0.5 # 線の太さ ) 4.8 2次元の度数分布（離散型） X軸：離散型変数 Y軸：離散型変数 離散型変数の2次元の度数分布を作成するには、度数バブルチャートや度数ヒートマップを用います。度数バブルチャートを作成するにはgeom_count()関数を使用します。また、離散型変数の度数ヒートマップを作成するにはgeom_tile()関数を使用します。 4.8.1 度数バブルチャートの基本形 ggplot(data = mpg, mapping = aes(x = manufacturer, y = class) ) + geom_count(alpha = 0.5, # 塗りつぶしの透明度 color = &quot;darkblue&quot;, # 線の色 fill = &quot;lightblue&quot;, # 塗りつぶしの色 shape = 21, # マーカーの種類 stroke = 0.5 # 線の太さ ) + scale_size_area(max_size = 15 # 変量とマーカーの面積を比例させる ) + theme(axis.text.x = element_text(angle = 45, hjust = 1.0)) 4.8.2 度数ヒートマップの基本形 ggplot(data = mpg %&gt;% dplyr::count(manufacturer, class), # データの度数を格納した変数を作成 mapping = aes(x = manufacturer, y = class, fill = n) ) + geom_tile(height = 1.0, # タイルの高さ width = 1.0, # タイルの幅 alpha = 1.0, # 塗りつぶしの色の透明度 color = &quot;grey&quot;, # 線の色 linetype = &quot;solid&quot;, # 線の種類（solid / dashed / dotted / dotdash / twodash / longdash） size = 0.5 # 線の太さ ) + scale_fill_viridis_c() + theme(axis.text.x = element_text(angle = 45, hjust = 1.0)) 4.9 2次元の度数分布（連続型） X軸：連続型変数 Y軸：連続型変数 連続型変数の2次元の度数分布を作成するには、度数ヒートマップを用います。連続型変数の度数ヒートマップを作成するにはgeom_bin2d()関数を使用します。 4.9.1 度数ヒートマップの基本形 ggplot(data = diamonds, mapping = aes(x = carat, y = price) ) + geom_bin2d(bins = c(100, 75), # X軸・Y軸の階級数（デフォルトは30） alpha = 1.0, # 塗りつぶしの色の透明度 color = NA, # 線の色 linetype = &quot;solid&quot;, # 線の種類 size = 0.2 # 線の太さ ) + scale_fill_viridis_c() 4.10 2次元の密度グラフ X軸：連続型変数 Y軸：連続型変数 2次元の密度グラフを作成するには、geom_density2d()関数ファミリーを使用します。 まず、使用するサンプルデータを作成します。データサイズが大きい場合、2次元密度グラフを作成するために時間を要するため、dplyr::sample_n()関数でサンプリングします。 diamonds_small &lt;- diamonds %&gt;% dplyr::sample_n(size = 5000) 4.10.1 2次元密度グラフの基本形 ggplot(data = diamonds_small, mapping = aes(x = carat, y = price) ) + geom_density2d() 4.10.2 2次元密度グラフの塗りつぶし版 ggplot(data = diamonds_small, mapping = aes(x = carat, y = price) ) + geom_density2d_filled() + scale_fill_brewer(palette = &quot;Reds&quot;, direction = 1 ) 4.11 QQプロット X軸・Y軸：連続型変数 QQプロットはgeom_qq()関数で作成します。他のグラフと異なり、mapping = aes(sample = 変数)で変数を指定します。 QQ（Quantile-Quantile）プロットは、データが正規分布にどの程度近いかを検証するグラフです。Y軸にデータを順番に並べた値、X軸にデータが正規分布に従うと仮定した場合にとりうる期待値を標準化した値をとり、散布図を描いたものです。 QQプロットが直線上にある場合、データは正規分布に従うと解釈できます。一方、QQプロットが直線から乖離している場合は、正規分布に従っておらず、分布にゆがみがあると解釈できます。 例えば、QQプロットが右下に凸な形状の場合は、データは右に長い裾（テール）をもちます。一方、QQプロットが左上に凸な形状の場合は、データの分布は左に長い裾（テール）をもちます。 また、QQプロットがS字状（前半は右下に凸、後半は左上に凸）の場合は、データの両裾が重い、いわゆるファット・テールの分布になります。一方、逆S字状（前半は左上に凸、後半は右下に凸）の場合は、ピーク部分が突出した分布になります。 # 右に長い裾をもつデータのQQプロット ggplot(data = iris, mapping = aes(sample = Sepal.Length) ) + geom_qq_line() + geom_qq() # ファット・テールをもつデータのQQプロット ggplot(data = economics, mapping = aes(sample = unemploy) ) + geom_qq_line() + geom_qq() 4.12 散布図・バブルチャート X軸：連続型変数 Y軸：連続型変数 散布図とバブルチャートはgeom_point()関数で作成します。バブルチャートの上にgeom_smooth()関数で近似曲線を重ねることができます。 まず、プロット用のデータを作成します。 data_mpg_point &lt;- mpg %&gt;% dplyr::group_by(manufacturer) %&gt;% dplyr::summarise(across(c(displ, cty, hwy), mean, na.rm = TRUE)) data_mpg_point ## # A tibble: 15 × 4 ## manufacturer displ cty hwy ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 audi 2.54 17.6 26.4 ## 2 chevrolet 5.06 15 21.9 ## 3 dodge 4.38 13.1 17.9 ## 4 ford 4.54 14 19.4 ## 5 honda 1.71 24.4 32.6 ## 6 hyundai 2.43 18.6 26.9 ## 7 jeep 4.58 13.5 17.6 ## 8 land rover 4.3 11.5 16.5 ## 9 lincoln 5.4 11.3 17 ## 10 mercury 4.4 13.2 18 ## 11 nissan 3.27 18.1 24.6 ## 12 pontiac 3.96 17 26.4 ## 13 subaru 2.46 19.3 25.6 ## 14 toyota 2.95 18.5 24.9 ## 15 volkswagen 2.26 20.9 29.2 4.12.1 散布図の基本形 ggplot(data = mpg, mapping = aes(x = cty, y = hwy) ) + geom_point(alpha = 1.0, # 塗りつぶしの透明度 color = &quot;darkblue&quot;, # 線の色 fill = &quot;lightblue&quot;, # 塗りつぶしの色 shape = 21, # マーカーの種類 size = 2.0, # マーカーの大きさ stroke = 0.5 # 線の太さ ) 4.12.2 バブルチャートの基本形 バブルチャートを作成するには、ggplot()関数のmapping = aes()関数でsize引数にバブルのサイズを変化させたい変数を指定します。 ggplot(data = data_mpg_point, mapping = aes(x = cty, y = hwy, size = displ) # aes() 関数内のsize引数に連続型変数を指定 ) + geom_point(alpha = 1.0, # 塗りつぶしの透明度 color = &quot;darkblue&quot;, # 線の色 fill = &quot;lightblue&quot;, # 塗りつぶしの色 shape = 21, # マーカーの種類 stroke = 0.5 # 線の太さ ) 4.12.3 散布図にコネクタ線を追加 散布図のマーカー間をつなぐコネクタ線を追加するには、geom_path()関数を追加します。折れ線グラフを作成するgeom_line()関数とは異なるので注意してください。 geom_path()関数とgeom_line()関数はどちらもマーカー間をつなぐコネクタ線を追加する関数ですが、geom_path()関数はデータセット上の順番で線を引く一方、geom_line()関数はデータセット上の順番に関わらずX軸上の順番で線を引くという違いがあります。 economics %&gt;% dplyr::filter(date &gt;= &quot;2000-01-01&quot;) %&gt;% ggplot(mapping = aes(x = unemploy, y = uempmed)) + geom_path(alpha = 1.0, # 塗りつぶしの透明度 color = &quot;darkblue&quot;, # 線の色 size = 0.5 # 線の大きさ ) + geom_point(alpha = 1.0, # 塗りつぶしの透明度 color = &quot;darkblue&quot;, # 線の色 fill = &quot;lightblue&quot;, # 塗りつぶしの色 shape = 21, # マーカーの種類 size = 2.0, # マーカーの大きさ stroke = 0.5 # 線の太さ ) 4.12.4 散布図に近似曲線を追加 散布図に近似曲線を追加するには、geom_smooth()関数を使用します。 ggplot(data = mpg, mapping = aes(x = cty, y = hwy) ) + geom_point(alpha = 1.0, # 塗りつぶしの透明度 color = &quot;darkblue&quot;, # 線の色 fill = &quot;lightblue&quot;, # 塗りつぶしの色 shape = 21, # マーカーの種類 size = 2.0, # マーカーの大きさ stroke = 0.5 # 線の太さ ) + geom_smooth(formula = y ~ x, # 近似曲線の推計式 method = &quot;loess&quot;, # 近似手法 (lm, glm, gam, loess) alpha = 0.5, # 誤差範囲の透明度 color = &quot;black&quot;, # 近似曲線の色 fill = &quot;gray&quot;, # 誤差範囲の色 linetype = &quot;dashed&quot;, # 近似曲線の種類（solid / dashed / dotted / dotdash / twodash / longdash） size = 1.0 # 近似曲線の太さ ) 4.12.5 散布図にラベルを追加 散布図にラベルを追加するには、ggrepelパッケージのgeom_text_repel()関数を使用します。geom_text_repel()関数は、散布図のマーカーとラベルが重ならないようにラベルの位置を自動で調整します。 ggplot(data = data_mpg_point, mapping = aes(x = cty, y = hwy) ) + geom_point(alpha = 1.0, # 塗りつぶしの透明度 color = &quot;darkblue&quot;, # 線の色 fill = &quot;lightblue&quot;, # 塗りつぶしの色 shape = 21, # マーカーの種類 size = 2.0, # マーカーの大きさ stroke = 0.5 # 線の太さ ) + geom_text_repel(mapping = aes(label = manufacturer), seed = NA, # テキストの配置を決定するランダムシード direction = &quot;both&quot;, # テキストの整列方向 (both / x / y) hjust = 0.0, # 横方向の整列位置 (0-1) vjust = 0.5, # 縦方向の整列位置 (0-1) nudge_x = NULL, # マーカーからの横方向のスペース (NULL, 0-) nudge_y = NULL, # マーカーからの縦方向のスペース (NULL, 0-) box.padding = 0.5, # テキスト周囲のスペース (0-) point.padding = 0.1, # マーカー周囲のスペース (0-) segment.alpha = 1.0, # 引き出し線の透明度 segment.color = &quot;grey&quot;, # 引き出し線の色 segment.size = 0.5, # 引き出し線の太さ min.segment.length = 0, # 引き出し線の最低長 color = &quot;black&quot;, # テキストの色 fontface = &quot;plain&quot;, # テキストの書体 (plain / bold / italic / bold.italic) size = 4.0 # テキストのサイズ ) 4.12.6 バブルチャートにラベルを追加 ggplot(data = data_mpg_point, mapping = aes(x = cty, y = hwy, size = displ) # aes() 関数内のsize引数に連続型変数を指定 ) + geom_point(alpha = 1.0, # 塗りつぶしの透明度 color = &quot;darkblue&quot;, # 線の色 fill = &quot;lightblue&quot;, # 塗りつぶしの色 shape = 21, # マーカーの種類 stroke = 0.5 # 線の太さ ) + geom_text_repel(mapping = aes(label = manufacturer), seed = NA, # テキストの配置を決定するランダムシード direction = &quot;both&quot;, # テキストの整列方向 (both / x / y) hjust = 0.0, # 横方向の整列位置 (0-1) vjust = 0.5, # 縦方向の整列位置 (0-1) nudge_x = NULL, # マーカーからの横方向のスペース (NULL, 0-) nudge_y = NULL, # マーカーからの縦方向のスペース (NULL, 0-) box.padding = 1.0, # テキスト周囲のスペース (0-) point.padding = 0.1, # マーカー周囲のスペース (0-) segment.alpha = 1.0, # 引き出し線の透明度 segment.color = &quot;grey&quot;, # 引き出し線の色 segment.size = 0.5, # 引き出し線の太さ min.segment.length = 0, # 引き出し線の最低長 color = &quot;black&quot;, # テキストの色 fontface = &quot;plain&quot;, # テキストの書体 (plain / bold / italic / bold.italic) size = 4.0 # テキストのサイズ ) + scale_size_area(max_size = 15 # 変量とマーカーの面積を比例させる ) 4.13 棒グラフ X軸：離散型変数 Y軸：連続型変数 棒グラフはgeom_col()関数で作成します。 まず、プロット用のデータを作成します。 data_mpg_col &lt;- mpg %&gt;% dplyr::group_by(manufacturer) %&gt;% dplyr::summarise(across(c(cty, hwy), mean, na.rm = TRUE)) data_mpg_col ## # A tibble: 15 × 3 ## manufacturer cty hwy ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 audi 17.6 26.4 ## 2 chevrolet 15 21.9 ## 3 dodge 13.1 17.9 ## 4 ford 14 19.4 ## 5 honda 24.4 32.6 ## 6 hyundai 18.6 26.9 ## 7 jeep 13.5 17.6 ## 8 land rover 11.5 16.5 ## 9 lincoln 11.3 17 ## 10 mercury 13.2 18 ## 11 nissan 18.1 24.6 ## 12 pontiac 17 26.4 ## 13 subaru 19.3 25.6 ## 14 toyota 18.5 24.9 ## 15 volkswagen 20.9 29.2 data_owid_col &lt;- data_owid %&gt;% dplyr::select(location, date, new_cases) %&gt;% dplyr::filter(location %in% c(&quot;Japan&quot;, &quot;United Kingdom&quot;, &quot;Germany&quot;, &quot;France&quot;, &quot;Italy&quot;)) %&gt;% dplyr::mutate(year = str_c(lubridate::year(date), &quot;年&quot;), .after = &quot;date&quot;) %&gt;% dplyr::group_by(location, year) %&gt;% dplyr::summarise(across(new_cases, sum, na.rm = TRUE)) data_owid_col ## # A tibble: 20 × 3 ## # Groups: location [5] ## location year new_cases ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 France 2020年 2338258 ## 2 France 2021年 6371668 ## 3 France 2022年 29279621 ## 4 France 2023年 1007943 ## 5 Germany 2020年 1660178 ## 6 Germany 2021年 5353865 ## 7 Germany 2022年 30227893 ## 8 Germany 2023年 1195820 ## 9 Italy 2020年 2083689 ## 10 Italy 2021年 3897739 ## 11 Italy 2022年 19187010 ## 12 Italy 2023年 808574 ## 13 Japan 2020年 230304 ## 14 Japan 2021年 1503484 ## 15 Japan 2022年 27371282 ## 16 Japan 2023年 4698502 ## 17 United Kingdom 2020年 2563561 ## 18 United Kingdom 2021年 10878143 ## 19 United Kingdom 2022年 10752834 ## 20 United Kingdom 2023年 509575 4.13.1 縦棒グラフの基本形 ggplot(data = data_mpg_col, mapping = aes(x = manufacturer, y = cty) ) + geom_col(alpha = 1.0, # 塗りつぶしの透明度 color = &quot;darkblue&quot;, # 線の色 fill = &quot;lightblue&quot;, # 塗りつぶしの色 size = 0.5, # 線の太さ width = 0.9 # 棒の幅 (0-1) ) + theme(axis.text.x = element_text(angle = 45, hjust = 1.0)) 4.13.2 横棒グラフ 横棒グラフを作成するには、縦棒グラフを作成した後にcoord_flip()関数で横棒グラフに転換します。そのため、X軸とY軸の変数指定は縦棒グラフの場合と同じです。 ggplot(data = data_mpg_col, mapping = aes(x = manufacturer, y = cty) ) + geom_col(alpha = 1.0, # 塗りつぶしの透明度 color = &quot;darkblue&quot;, # 線の色 fill = &quot;lightblue&quot;, # 塗りつぶしの色 size = 0.5, # 線の太さ width = 0.9 # 棒の幅 (0-1) ) + coord_flip() 4.13.3 縦棒グラフの順序並べ替え 縦棒グラフのX軸の順番を並べ替えるには、X軸の変数を指定する際にfct_reorder()関数を使用します。 ggplot(data = data_mpg_col, mapping = aes(x = fct_reorder(manufacturer, -cty), y = cty) ) + geom_col(alpha = 1.0, # 塗りつぶしの透明度 color = &quot;darkblue&quot;, # 線の色 fill = &quot;lightblue&quot;, # 塗りつぶしの色 size = 0.5, # 線の太さ width = 0.9 # 棒の幅 (0-1) ) + theme(axis.text.x = element_text(angle = 45, hjust = 1.0)) 4.13.4 縦棒グラフにラベルを追加 縦棒グラフにデータラベルを追加するには、geom_text()関数を使用します。 ggplot(data = data_mpg_col, mapping = aes(x = fct_reorder(manufacturer, -cty), y = cty) ) + geom_col(alpha = 1.0, # 塗りつぶしの透明度 color = &quot;darkblue&quot;, # 線の色 fill = &quot;lightblue&quot;, # 塗りつぶしの色 size = 0.5, # 線の太さ width = 0.9 # 棒の幅 (0-1) ) + geom_text(mapping = aes(label = cty %&gt;% sprintf(fmt = &quot;%0.1f&quot;)), # sprintf() 関数で数値の表示形式を指定 vjust = -1, # 縦方向の整列位置 (0-1) color = &quot;black&quot;, # テキストの色 fontface = &quot;plain&quot;, # テキストの書体 (plain / bold / italic / bold.italic) size = 4.0 # テキストのサイズ ) + theme(axis.text.x = element_text(angle = 45, hjust = 1.0)) 4.13.5 集合縦棒グラフ 複数の縦棒グラフを横並び形式で作成するには、position引数にposition_dodge()関数を指定します。 ggplot(data = data_owid_col, mapping = aes(x = location, y = new_cases, fill = year, group = rev(year)) ) + geom_col(position = position_dodge(width = 0.5), # 横並びポジション alpha = 1.0, # 塗りつぶしの透明度 size = 0.5, # 線の太さ width = 0.9 # 棒の幅 (0-1) ) 4.13.6 積み上げ縦棒グラフ 複数の縦棒グラフを積み上げ形式で作成するには、position引数にposition_stack()関数を指定します。 ggplot(data = data_owid_col, mapping = aes(x = location, y = new_cases, fill = year, group = rev(year)) ) + geom_col(position = position_stack(), # 積み上げポジション alpha = 1.0, # 塗りつぶしの透明度 size = 0.5, # 線の太さ width = 0.9 # 棒の幅 (0-1) ) + guides(fill = guide_legend(reverse = TRUE)) 4.13.7 積み上げ縦棒グラフにラベルを追加 ggplot(data = data_owid_col, mapping = aes(x = location, y = new_cases, fill = year, group = rev(year)) ) + geom_col(position = position_stack(), # 積み上げポジション alpha = 1.0, # 塗りつぶしの透明度 size = 0.5, # 線の太さ width = 0.9 # 棒の幅 (0-1) ) + geom_text(mapping = aes(label = new_cases %&gt;% sprintf(fmt = &quot;%0.1f&quot;)), # sprintf() 関数で数値の表示形式を指定 position = position_stack(vjust = 0.5), # position_stack() 関数で積み上げ棒グラフ上のラベル位置を指定 color = &quot;white&quot;, # テキストの色 fontface = &quot;plain&quot;, # テキストの書体 (plain / bold / italic / bold.italic) size = 4.0 # テキストのサイズ ) + guides(fill = guide_legend(reverse = TRUE)) 4.13.8 100％積み上げ縦棒グラフ 複数の縦棒グラフを合計が100％になる積み上げ形式で作成するには、position引数にposition_fill()関数を指定します。 ggplot(data = data_owid_col, mapping = aes(x = location, y = new_cases, fill = year, group = rev(year)) ) + geom_col(position = position_fill(), # 割合積み上げポジション alpha = 1.0, # 塗りつぶしの透明度 size = 0.5, # 線の太さ width = 0.9 # 棒の幅 (0-1) ) + guides(fill = guide_legend(reverse = TRUE)) 4.13.9 100％積み上げ縦棒グラフにラベルを追加 data_owid_col %&gt;% dplyr::group_by(location) %&gt;% dplyr::mutate(percent = new_cases / sum(new_cases)) %&gt;% ggplot(mapping = aes(x = location, y = new_cases, fill = year, group = rev(year))) + geom_col(position = position_fill(), # 割合ポジション alpha = 1.0, # 塗りつぶしの透明度 size = 0.5, # 線の太さ width = 0.9 # 棒の幅 (0-1) ) + geom_text(mapping = aes(label = percent %&gt;% sprintf(fmt = &quot;%0.2f&quot;)), # sprintf() 関数で数値の表示形式を指定 position = position_fill(vjust = 0.5), # position_fill() 関数で割合棒グラフ上のラベル位置を指定 color = &quot;white&quot;, # テキストの色 fontface = &quot;plain&quot;, # テキストの書体 (plain / bold / italic / bold.italic) size = 4.0 # テキストのサイズ ) + guides(fill = guide_legend(reverse = TRUE)) 4.14 円グラフ X軸：単一の値 Y軸：連続型変数 円グラフを作成するには、geom_col()関数で積み上げ棒グラフを作成した後に、coord_polar()関数で円グラフに転換します。 まず、プロット用のデータを作成します。 data_mpg_circle &lt;- mpg %&gt;% dplyr::group_by(class) %&gt;% dplyr::summarise(across(c(hwy), mean, na.rm = TRUE)) data_mpg_circle ## # A tibble: 7 × 2 ## class hwy ## &lt;chr&gt; &lt;dbl&gt; ## 1 2seater 24.8 ## 2 compact 28.3 ## 3 midsize 27.3 ## 4 minivan 22.4 ## 5 pickup 16.9 ## 6 subcompact 28.1 ## 7 suv 18.1 4.14.1 円グラフの基本形 ggplot(data = data_mpg_circle, mapping = aes(x = 1, y = hwy, fill = class, group = rev(class)) ) + geom_col(position = position_stack(), # 積み上げポジション alpha = 1.0, # 塗りつぶしの透明度 color = &quot;grey&quot;, # 線の色 size = 0.5 # 線の太さ ) + geom_text(mapping = aes(label = hwy %&gt;% sprintf(fmt = &quot;%0.1f&quot;)), # sprintf() 関数で数値の表示形式を指定 position = position_stack(vjust = 0.5), # position_stack() 関数で積み上げ棒グラフ上のラベル位置を指定 color = &quot;white&quot;, # テキストの色 fontface = &quot;plain&quot;, # テキストの書体 (plain / bold / italic / bold.italic) size = 4.0 # テキストのサイズ ) + coord_polar(theta = &quot;y&quot;, # 円グラフを作成する軸 start = 0, # 円グラフの開始位置（ラジアン） direction = 1 # 円グラフの方向（1：時計回り、-1：反時計回り） ) + theme(panel.grid = element_blank(), # パネルの軸や目盛り線を表示しない axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank() ) + guides(fill = guide_legend(reverse = FALSE)) 4.15 折れ線グラフ X軸：日付型変数 Y軸：連続型変数 折れ線グラフは時系列データを可視化する最も基本的なグラフで、geom_line()関数で作成します。 まず、プロット用のデータを作成します。 data_owid_line &lt;- data_owid %&gt;% dplyr::select(location, date, new_cases_smoothed) %&gt;% dplyr::filter(location %in% c(&quot;Japan&quot;, &quot;United Kingdom&quot;, &quot;Germany&quot;, &quot;France&quot;, &quot;Italy&quot;), date &gt;= &quot;2022-01-01&quot;, date &lt;= &quot;2022-12-31&quot; ) %&gt;% tidyr::drop_na(everything()) data_owid_line ## # A tibble: 1,825 × 3 ## location date new_cases_smoothed ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; ## 1 France 2022-01-01 74249. ## 2 France 2022-01-02 162961. ## 3 France 2022-01-03 162961. ## 4 France 2022-01-04 162961. ## 5 France 2022-01-05 162961. ## 6 France 2022-01-06 162961. ## 7 France 2022-01-07 162961. ## 8 France 2022-01-08 162961. ## 9 France 2022-01-09 246602. ## 10 France 2022-01-10 246602. ## # … with 1,815 more rows 4.15.1 折れ線グラフの基本形 ggplot(data = economics, mapping = aes(x = date, y = unemploy) ) + geom_line(alpha = 1.0, # 線の透明度 color = &quot;darkblue&quot;, # 線の色 linetype = &quot;solid&quot;, # 線の種類（solid / dashed / dotted / dotdash / twodash / longdash） size = 1.0 # 線の太さ ) 4.15.2 折れ線グラフにマーカーを追加 折れ線グラフを作成するgeom_line()関数と、散布図を作成するgeom_point()関数を併用します。 ggplot(data = economics, mapping = aes(x = date, y = unemploy) ) + geom_line(alpha = 1.0, # 線の透明度 color = &quot;darkblue&quot;, # 線の色 linetype = &quot;solid&quot;, # 線の種類（solid / dashed / dotted / dotdash / twodash / longdash） size = 1.0 # 線の太さ ) + geom_point(alpha = 1.0, # 塗りつぶしの透明度 color = &quot;darkblue&quot;, # 線の色 fill = &quot;lightblue&quot;, # 塗りつぶしの色 shape = 21, # マーカーの種類 size = 2.0, # マーカーの大きさ stroke = 0.5 # 線の太さ ) 4.15.3 折れ線グラフに系列ラベルを追加 折れ線グラフの数が多く凡例が分かりにくい場合などに、折れ線グラフの右側に系列名のラベルを追加します。ラベルを追加するには、ggrepelパッケージのgeom_text_repel関数を使用します。 ggplot(data = data_owid_line, mapping = aes(x = date, y = new_cases_smoothed, color = location) ) + geom_line(alpha = 1.0, # 線の透明度 linetype = &quot;solid&quot;, # 線の種類（solid / dashed / dotted / dotdash / twodash / longdash） size = 1.0 # 線の太さ ) + geom_text_repel(data = data_owid_line %&gt;% dplyr::group_by(location) %&gt;% dplyr::filter(date == max(date)), mapping = aes(x = date, y = new_cases_smoothed, label = location), seed = NA, # テキストの配置を決定するランダムシード direction = &quot;y&quot;, # テキストの整列方向 (both / x / y) hjust = 0.0, # 横方向の整列位置 (0-1) vjust = 0.5, # 縦方向の整列位置 (0-1) nudge_x = 5, # マーカーからの横方向のスペース (NULL, 0-) nudge_y = NULL, # マーカーからの縦方向のスペース (NULL, 0-) box.padding = 0.1, # テキスト周囲のスペース (0-) point.padding = 0.1, # マーカー周囲のスペース (0-) segment.alpha = 1.0, # 引き出し線の透明度 segment.color = &quot;grey&quot;, # 引き出し線の色 segment.size = 0.5, # 引き出し線の太さ min.segment.length = Inf, # 引き出し線の最低長 fontface = &quot;plain&quot;, # テキストの書体 (plain / bold / italic / bold.italic) size = 3.0 # テキストのサイズ ) + scale_x_date(date_breaks = &quot;1 month&quot;, # 日付目盛の周期 date_labels = &quot;%y/%m&quot;, # 日付フォーマット（%Y：4桁年、%y：2桁年、%m：2桁月、%d：日） limits = c(as.Date(&quot;2022-01-01&quot;), NA), # 始期・終期（指定しない場合はNA） expand = expansion(mult = c(0.00, 0.2), add = c(0, 0)) # 始期・終期からの余白（multは余白率、addは余白幅） ) + theme(legend.position = &quot;none&quot;) 4.16 ステップグラフ X軸：日付型変数 Y軸：連続型変数 ステップグラフは折れ線グラフの特殊形です。政策金利のように、段階的に値が変化するデータに適しています。ステップグラフはgeom_step()関数で作成します。 まず、サンプルデータを作成します。ここでは、Our World in Dataのdata_owidデータセットに含まれるStringency Indexのステップグラフを作成します。Stringency Indexは、各国当局が実施する新型コロナウイルス感染対策の厳格度を示す指数であり、段階的に値が変化します。 data_owid_step &lt;- data_owid %&gt;% dplyr::select(location, date, stringency_index) %&gt;% dplyr::filter(location %in% c(&quot;Japan&quot;, &quot;United Kingdom&quot;, &quot;Germany&quot;, &quot;France&quot;, &quot;Italy&quot;)) data_owid_step ## # A tibble: 6,750 × 3 ## location date stringency_index ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; ## 1 France 2020-01-03 0 ## 2 France 2020-01-04 0 ## 3 France 2020-01-05 0 ## 4 France 2020-01-06 0 ## 5 France 2020-01-07 0 ## 6 France 2020-01-08 0 ## 7 France 2020-01-09 0 ## 8 France 2020-01-10 0 ## 9 France 2020-01-11 0 ## 10 France 2020-01-12 0 ## # … with 6,740 more rows 4.16.1 ステップグラフの基本形 ggplot(data = data_owid_step, mapping = aes(x = date, y = stringency_index, color = location) ) + geom_step(alpha = 1.0, # 線の透明度 linetype = &quot;solid&quot;, # 線の種類（solid / dashed / dotted / dotdash / twodash / longdash） size = 1.0 # 線の太さ ) 4.17 面グラフ X軸：日付型変数 Y軸：連続型変数 面グラフは折れ線グラフの特殊形です。面グラフには2種類あり、折れ線グラフとX軸で挟まれた範囲を塗りつぶすグラフはgeom_area()関数、2つの折れ線グラフで挟まれた範囲を塗りつぶすグラフはgeom_ribbon()関数で作成します。後者は特にリボングラフと呼ばれ、信頼区間の可視化に用いることができます。 まず、プロット用のデータを作成します。 data_owid_line &lt;- data_owid %&gt;% dplyr::select(location, date, new_cases_smoothed) %&gt;% dplyr::filter(location %in% c(&quot;Japan&quot;, &quot;United Kingdom&quot;, &quot;Germany&quot;, &quot;France&quot;, &quot;Italy&quot;), date &gt;= &quot;2022-01-01&quot; ) %&gt;% tidyr::drop_na(everything()) data_owid_line ## # A tibble: 3,094 × 3 ## location date new_cases_smoothed ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; ## 1 France 2022-01-01 74249. ## 2 France 2022-01-02 162961. ## 3 France 2022-01-03 162961. ## 4 France 2022-01-04 162961. ## 5 France 2022-01-05 162961. ## 6 France 2022-01-06 162961. ## 7 France 2022-01-07 162961. ## 8 France 2022-01-08 162961. ## 9 France 2022-01-09 246602. ## 10 France 2022-01-10 246602. ## # … with 3,084 more rows 4.17.1 面グラフの基本形 ggplot(data = economics, mapping = aes(x = date, y = unemploy) ) + geom_area(alpha = 1.0, # 線の透明度 color = &quot;darkblue&quot;, # 線の色 fill = &quot;lightblue&quot;, # 塗りつぶしの色 linetype = &quot;solid&quot;, # 線の種類（solid / dashed / dotted / dotdash / twodash / longdash） size = 1.0 # 線の太さ ) 4.17.2 積み上げ面グラフ 複数の面グラフを積み上げ形式で作成するには、position引数にposition_stack()関数を指定します。 data_owid_line %&gt;% dplyr::mutate(location = factor(location, levels = c(&quot;Japan&quot;, &quot;United Kingdom&quot;, &quot;Germany&quot;, &quot;France&quot;, &quot;Italy&quot;) %&gt;% rev() ) ) %&gt;% ggplot(mapping = aes(x = date, y = new_cases_smoothed, fill = location)) + geom_area(position = position_stack(), alpha = 1.0, # 線の透明度 linetype = &quot;solid&quot;, # 線の種類（solid / dashed / dotted / dotdash / twodash / longdash） size = 1.0 # 線の太さ ) 4.17.3 リボングラフの基本形 リボングラフでは、aes()関数の中で塗りつぶし範囲の下端をymin引数、上端をymax引数に指定します。 data_owid_line %&gt;% dplyr::group_by(date) %&gt;% dplyr::summarise(new_cases_max = max(new_cases_smoothed, na.rm = TRUE), new_cases_min = min(new_cases_smoothed, na.rm = TRUE) ) %&gt;% ggplot(mapping = aes(x = date, ymin = new_cases_min, ymax = new_cases_max)) + geom_ribbon(alpha = 1.0, # 線の透明度 color = &quot;darkblue&quot;, # 線の色 fill = &quot;lightblue&quot;, # 塗りつぶしの色 linetype = &quot;solid&quot;, # 線の種類（solid / dashed / dotted / dotdash / twodash / longdash） size = 0.5 # 線の太さ ) 4.17.4 リボングラフに折れ線グラフを追加 リボングラフに折れ線グラフを追加する際は、まずgeom_ribbon()関数でリボングラフを作成し、そのあとに+演算子でgeom_line()関数をつなげて折れ線グラフを追加します。 data_owid_line %&gt;% dplyr::group_by(date) %&gt;% dplyr::summarise(new_cases_max = max(new_cases_smoothed, na.rm = TRUE), new_cases_median = median(new_cases_smoothed, na.rm = TRUE), new_cases_min = min(new_cases_smoothed, na.rm = TRUE) ) %&gt;% ggplot(mapping = aes(x = date, ymin = new_cases_min, ymax = new_cases_max, y = new_cases_median)) + geom_ribbon(alpha = 1.0, # 線の透明度 color = &quot;darkblue&quot;, # 線の色 fill = &quot;lightblue&quot;, # 塗りつぶしの色 linetype = &quot;solid&quot;, # 線の種類（solid / dashed / dotted / dotdash / twodash / longdash） size = 0.5 # 線の太さ ) + geom_line(alpha = 1.0, # 線の透明度 color = &quot;red&quot;, # 線の色 linetype = &quot;dashed&quot;, # 線の種類（solid / dashed / dotted / dotdash / twodash / longdash） size = 1.0 # 線の太さ ) 4.18 箱ひげ図・ヴァイオリングラフ X軸：離散型変数 Y軸：連続型変数 データの分布範囲を可視化するには箱ひげ図やヴァイオリングラフを用います。箱ひげ図はgeom_boxplot()関数、ヴァイオリングラフはgeom_violin()関数で作成します。 また、geom_jitter()関数で作成するジッターグラフを併用し、箱ひげ図やヴァイオリングラフに個別データの位置をプロットすると、データの分布がより具体的に分かりやすくなります。 4.18.1 箱ひげ図の基本形 箱ひげ図は一般的に、ひげの下端が最小値、箱の下端が第1四分位数、箱内部の横線が中央値、箱の上端が第3四分位数、ひげの上端が最大値を表します。 しかし、ggplot2パッケージのgeom_boxplot()で作成すると、ひげの下端・上端は箱の下端・上端から1.5×IQRの範囲内にある最小・最大のサンプルの位置を表すことに注意が必要です。その範囲外にあるサンプルは外れ値として扱われます。なお、IQRは四分位範囲（第3四分位数-第1四分位数）を意味します。 ggplot(data = mpg, mapping = aes(x = class, y = hwy) ) + geom_boxplot(outlier.alpha = 1.0, # 外れ値マーカーの塗りつぶしの透明度 outlier.color = &quot;darkblue&quot;, # 外れ値マーカーの線の色 outlier.fill = &quot;lightblue&quot;, # 外れ値マーカーの塗りつぶしの色 outlier.shape = 21, # 外れ値マーカーの種類（外れ値を表示しない場合はNA） outlier.size = 2.0, # 外れ値マーカーの大きさ outlier.stroke = 0.5, # 外れ値マーカーの線の太さ alpha = 0.5, # 箱の塗りつぶしの透明度 color = &quot;darkblue&quot;, # 箱ひげの線の色 fill = &quot;lightblue&quot;, # 箱の塗りつぶしの色 size = 0.5 # 箱ひげの線の太さ ) 4.18.2 箱ひげ図に個別サンプルを追加 箱ひげ図にジッターグラフで個別サンプルを追加する場合は、データの重複表示を避けるため、geom_boxplot()関数のoutlier.shape引数にNAを指定して、箱ひげ図による外れ値のマーカーを非表示にします。 ggplot(data = mpg, mapping = aes(x = class, y = hwy) ) + geom_boxplot(outlier.shape = NA, # 外れ値マーカーの種類（外れ値を表示しない場合はNA） alpha = 0.5, # 箱の塗りつぶしの透明度 color = &quot;darkblue&quot;, # 箱ひげの線の色 fill = &quot;lightblue&quot;, # 箱の塗りつぶしの色 size = 0.5 # 箱ひげの線の太さ ) + geom_jitter(height = 0.3, # マーカーの縦方向の分布範囲 width = 0.3, # マーカーの横方向の分布範囲 alpha = 1.0, # マーカーの塗りつぶしの透明度 color = &quot;darkblue&quot;, # マーカーの線の色 fill = &quot;darkblue&quot;, # マーカーの塗りつぶしの色 shape = 21, # マーカーの種類 size = 1.0, # マーカーの大きさ stroke = 0.5 # マーカーの線の太さ ) 4.18.3 ヴァイオリングラフの基本形 ggplot(data = mpg, mapping = aes(x = class, y = hwy) ) + geom_violin(scale = &quot;area&quot;, # ヴァイオリンの大きさ（area：すべての面積が同じ、count：サンプル個数に比例、width：すべての幅が同じ） alpha = 0.5, # ヴァイオリンの塗りつぶしの透明度 color = &quot;darkblue&quot;, # ヴァイオリンの線の色 fill = &quot;lightblue&quot;, # ヴァイオリンの塗りつぶしの色 linetype = &quot;solid&quot;, # ヴァイオリンの線の種類 size = 0.5 # ヴァイオリンの線の太さ ) 4.18.4 ヴァイオリングラフに個別サンプルを追加 ggplot(data = mpg, mapping = aes(x = class, y = hwy) ) + geom_violin(scale = &quot;area&quot;, # ヴァイオリンの大きさ（area：すべての面積が同じ、count：サンプル個数に比例、width：すべての幅が同じ） alpha = 0.5, # ヴァイオリンの塗りつぶしの透明度 color = &quot;darkblue&quot;, # ヴァイオリンの線の色 fill = &quot;lightblue&quot;, # ヴァイオリンの塗りつぶしの色 linetype = &quot;solid&quot;, # ヴァイオリンの線の種類 size = 0.5 # ヴァイオリンの線の太さ ) + geom_jitter(height = 0.4, # マーカーの縦方向の分布範囲 width = 0.4, # マーカーの横方向の分布範囲 alpha = 1.0, # マーカーの塗りつぶしの透明度 color = &quot;darkblue&quot;, # マーカーの線の色 fill = &quot;darkblue&quot;, # マーカーの塗りつぶしの色 shape = 21, # マーカーの種類 size = 1.0, # マーカーの大きさ stroke = 0.5 # マーカーの線の太さ ) 4.19 ドットプロット X軸：離散型変数 Y軸：離散型変数 ドットプロットはgeom_dotplot()関数で作成します。実務では、FOMCで示される参加者の政策金利予想を可視化する際に使用されます。 4.19.1 ドットプロットの基本形 ggplot(data = mpg, mapping = aes(x = class, y = hwy) ) + geom_dotplot(binaxis = &quot;y&quot;, binwidth = 0.5, stackdir = &quot;center&quot;, alpha = 1.0, # マーカーの塗りつぶしの透明度 color = &quot;darkblue&quot;, # マーカーの線の色 fill = &quot;lightblue&quot;, # マーカーの塗りつぶしの色 stroke = 0.5 # マーカーの線の太さ ) 4.20 複合グラフ（ファセット） 複数のグラフをまとめたファセットを作成するには、facet関数ファミリーを使用します。どのようなファセットにするかで使用する関数が異なります。 4.20.1 行方向の複合グラフ 行方向に複合グラフを並べる場合は、facet_rep_grid()関数のrows引数にvars(離散型変数)の形でグラフを分けて作成したい変数名を指定します。 ggplot(data = mpg, mapping = aes(x = cty, y = hwy) ) + geom_point() + facet_rep_grid(rows = vars(class), # 変数class別にグラフを作成 scales = &quot;fixed&quot;, # 目盛設定（fixed：全グラフ共通、free：全グラフ独立、free_x：X軸のみ独立、free_y：Y軸のみ独立） repeat.tick.labels = FALSE # 目盛表示（TRUE：すべてのグラフに表示、FALSE：端のグラフのみ表示 ) 4.20.2 列方向の複合グラフ 列方向に複合グラフを並べる場合は、facet_rep_grid()関数のcols引数にvars(離散型変数)の形でグラフを分けて作成したい変数名を指定します。 ggplot(data = mpg, mapping = aes(x = cty, y = hwy) ) + geom_point() + facet_rep_grid(cols = vars(class), # 変数class別にグラフを作成 scales = &quot;fixed&quot;, # 目盛設定（fixed：全グラフ共通、free：全グラフ独立、free_x：X軸のみ独立、free_y：Y軸のみ独立） repeat.tick.labels = FALSE # 目盛表示（TRUE：すべてのグラフに表示、FALSE：端のグラフのみ表示 ) 4.20.3 行・列方向の複合グラフ 行・列方向に複合グラフを並べる場合は、facet_rep_grid()関数のrows引数とcols引数にそれぞれvars(離散型変数)の形でグラフを分けて作成したい変数名を指定します。 ggplot(data = mpg, mapping = aes(x = cty, y = hwy) ) + geom_point() + facet_rep_grid(rows = vars(year), # 変数year別に行方向のグラフを作成 cols = vars(class), # 変数class別に列方向のグラフを作成 scales = &quot;fixed&quot;, # 目盛設定（fixed：全グラフ共通、free：全グラフ独立、free_x：X軸のみ独立、free_y：Y軸のみ独立） repeat.tick.labels = TRUE # 目盛表示（TRUE：すべてのグラフに表示、FALSE：端のグラフのみ表示 ) 4.20.4 行数・列数を指定した複合グラフ 行数と列数を指定してグラフを順番に並べるには、facet_rep_wrap()関数のfacets引数に「~ 離散型変数」の形でグラフを分けて作成したい変数名を指定し、nrow引数とncol引数に行数・列数を指定します。 ggplot(data = mpg, mapping = aes(x = cty, y = hwy) ) + geom_point() + facet_rep_wrap(facets = ~ manufacturer, # 変数manufacturer別にグラフを作成 nrow = 3, # 行方向のグラフ数 ncol = 5, # 列方向のグラフ数 scales = &quot;fixed&quot;, # 目盛設定（fixed：全グラフ共通、free：全グラフ独立、free_x：X軸のみ独立、free_y：Y軸のみ独立） repeat.tick.labels = TRUE # 目盛表示（TRUE：すべてのグラフに表示、FALSE：端のグラフのみ表示 ) 4.20.5 複合グラフの設定 ファセットの各種設定を行うには、theme()関数内に必要事項を記入します。 # 行数・列数を指定した複合グラフの例 ggplot(data = mpg, mapping = aes(x = cty, y = hwy) ) + geom_point() + facet_rep_wrap(facets = ~ manufacturer # 変数manufacturer別にグラフを作成 ) + theme(strip.background = element_rect(color = NA, # ファセットタイトル領域の枠の色 fill = &quot;White&quot; # ファセットタイトル領域の塗りつぶしの色（NAで無色） ), strip.text = element_text(color = &quot;Black&quot;, # ファセットタイトルの色 face = &quot;bold&quot;, # ファセットタイトルの書体 size = 8, # ファセットタイトルのフォントサイズ hjust = 0.5, # ファセットタイトルの横方向の整列位置 vjust = 0.5 # ファセットタイトルの縦方向の整列位置 ), panel.spacing.x = unit(x = 2, units = &quot;mm&quot;), # ファセットのグラフ間の横方向のスペース panel.spacing.y = unit(x = 2, units = &quot;mm&quot;) # ファセットのグラフ間の縦方向のスペース ) 4.21 地図ファセット 国別・州別・都道府県別のデータを、地図を模した複合グラフに表示するには、geofacetパッケージのfacet_geo()関数を使用します。 ここでは、日本の県内総生産データを使用します。 data_gdp_pref ## # A tibble: 611 × 5 ## pref_code pref_name year gdp_nominal gdp_nominal_pchg ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 北海道 2006 19316568 NA ## 2 1 北海道 2007 19137599 -0.927 ## 3 1 北海道 2008 18457858 -3.55 ## 4 1 北海道 2009 18219113 -1.29 ## 5 1 北海道 2010 18122675 -0.529 ## 6 1 北海道 2011 18071493 -0.282 ## 7 1 北海道 2012 17923502 -0.819 ## 8 1 北海道 2013 18242119 1.78 ## 9 1 北海道 2014 18579766 1.85 ## 10 1 北海道 2015 19128504 2.95 ## # … with 601 more rows geofacetパッケージでは、都道府県名と地図上の位置を結びつけるjp_prefs_grid2データが用意されています。ただし都道府県名が英語表記でしか格納されていないため、日本語表記を名寄せして追加します。 # geofacetのjp_prefs_grid2に都道府県名の日本語表記を追加 data_prefs &lt;- data_gdp_pref %&gt;% dplyr::select(pref_code, pref_name) %&gt;% dplyr::distinct() %&gt;% dplyr::rename(code = pref_code) jp_prefs_grid2 %&lt;&gt;% dplyr::left_join(data_prefs, by = &quot;code&quot;) そのうえで、ファセット部分にfacet_geo()関数を用いると、都道府県別のデータを日本地図を模した形でグラフ化することができます。 ggplot(data = data_gdp_pref, mapping = aes(x = year, y = gdp_nominal_pchg) ) + geom_line() + geom_point() + facet_geo(facets = ~ pref_name, # 変数pref_name別にグラフを作成 grid = &quot;jp_prefs_grid2&quot;, # 使用する地図グリッド scales = &quot;fixed&quot; # 目盛設定（fixed：全グラフ共通、free：全グラフ独立、free_x：X軸のみ独立、free_y：Y軸のみ独立） ) + theme(strip.background = element_rect(color = NA, # ファセットタイトル領域の枠の色 fill = NA # ファセットタイトル領域の塗りつぶしの色（NAで無色） ), strip.text = element_text(color = &quot;Black&quot;, # ファセットタイトルの色 face = &quot;bold&quot;, # ファセットタイトルの書体 size = 8, # ファセットタイトルのフォントサイズ hjust = 0.5, # ファセットタイトルの横方向の整列位置 vjust = 0.5 # ファセットタイトルの縦方向の整列位置 ), panel.spacing.x = unit(x = 1, units = &quot;mm&quot;), # ファセットのグラフ間の横方向のスペース panel.spacing.y = unit(x = 0.5, units = &quot;mm&quot;) # ファセットのグラフ間の縦方向のスペース ) 4.22 補助線 水平線、垂直線、傾き・切片のある直線、といった補助線を追加するには、それぞれgeom_hline()関数、geom_vline()関数、geom_abline()関数を使用します。 補助線を追加する際には、グラフをプロットする関数（ここでは散布図のgeom_point()関数）との順序に注意してください。コード上で先に記述した方がグラフ上では背面に、後に記述した方が前面に配置されます。 ggplot(data = mpg, mapping = aes(x = cty, y = hwy) ) + # 水平線（背面） geom_hline(yintercept = 20, # 水平線の縦軸との交点 color = &quot;Tomato&quot;, # 水平線の色 size = 0.5 # 水平線の太さ ) + # 垂直線（背面） geom_vline(xintercept = 20, # 垂直線の横軸との交点 color = &quot;Forestgreen&quot;, # 垂直線の色 size = 0.5 # 垂直線の太さ ) + # 傾き・切片のある直線（背面） geom_abline(intercept = 5, # 直線の切片 slope = 1, # 直線の傾き color = &quot;gold&quot;, # 直線の色 size = 2.0 # 直線の太さ ) + # 散布図 geom_point(alpha = 1.0, # 塗りつぶしの透明度 color = &quot;darkblue&quot;, # 線の色 fill = &quot;lightblue&quot;, # 塗りつぶしの色 shape = 21, # マーカーの種類 size = 2.0, # マーカーの大きさ stroke = 0.5 # 線の太さ ) + # 水平線（前面） geom_hline(yintercept = 25, # 水平線の縦軸との交点 color = &quot;Tomato&quot;, # 水平線の色 size = 0.5 # 水平線の太さ ) + # 垂直線（前面） geom_vline(xintercept = 25, # 垂直線の横軸との交点 color = &quot;Forestgreen&quot;, # 垂直線の色 size = 0.5 # 垂直線の太さ ) + # 傾き・切片のある直線（前面） geom_abline(intercept = 10, # 直線の切片 slope = 1, # 直線の傾き color = &quot;gold&quot;, # 直線の色 size = 2.0 # 直線の太さ ) 4.23 設定：色 変数の値や属性に応じて色を自動で変化させるには、geom関数ファミリーでグラフを作成したあとに、+演算子でscale_color_*()関数ファミリー（線の色）やscale_fill_*()関数ファミリー（塗りつぶしの色）をつなぎます。色の変化に対応する変数が離散型か連続型かによって、どのscale_color_*()・scale_fill_*()関数を用いるかが変わります。 まず、プロット用のデータを作成します。 data_mpg_color &lt;- mpg %&gt;% dplyr::group_by(class) %&gt;% dplyr::summarise(across(c(cty, hwy), mean, na.rm = TRUE)) data_mpg_color ## # A tibble: 7 × 3 ## class cty hwy ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2seater 15.4 24.8 ## 2 compact 20.1 28.3 ## 3 midsize 18.8 27.3 ## 4 minivan 15.8 22.4 ## 5 pickup 13 16.9 ## 6 subcompact 20.4 28.1 ## 7 suv 13.5 18.1 4.23.1 色の手動指定（離散型変数） 自分で色の割り当てを決める方法です。Rで使用できる色の種類については、こちらを参照してください。 ggplot(data = mpg, mapping = aes(x = fl, color = fl, fill = fl) ) + geom_bar() + # 色 scale_color_manual(values = c(&quot;Red&quot;, &quot;Salmon&quot;, &quot;Gold&quot;, &quot;Forestgreen&quot;, &quot;Darkblue&quot;) # colorで指定した変数の離散値と同数の色を指定 ) + scale_fill_manual(values = c(&quot;Red&quot;, &quot;Salmon&quot;, &quot;Gold&quot;, &quot;Forestgreen&quot;, &quot;Darkblue&quot;) # fillで指定した変数の離散値と同数の色を指定 ) 4.23.2 Brewerカラーパレット（離散型変数） RColorBrewerパッケージでは、様々な種類の美麗なプリセットカラーパレットが利用できます。 # カラーパレットの確認 RColorBrewer::display.brewer.all() ggplot(data = mpg, mapping = aes(x = fl, color = fl, fill = fl) ) + geom_bar() + # 色 scale_color_brewer(palette = &quot;Paired&quot;, # カラーパレット名 direction = 1 # 色の順序（-1で逆順） ) + scale_fill_brewer(palette = &quot;Paired&quot;, # カラーパレット名 direction = 1 # 色の順序（-1で逆順） ) 4.23.3 Viridisカラーパレット（離散型変数） 色覚障害に対応したカラーパレットです。パレットの種類などの詳細はこちらを参照してください。 ggplot(data = mpg, mapping = aes(x = fl, color = fl, fill = fl) ) + geom_bar() + # 色 scale_color_viridis_d(option = &quot;viridis&quot;, # パレットの種類（magma / inferno / plasma / viridis / cividis） direction = 1 # 色の順序（-1で逆順） ) + scale_fill_viridis_d(option = &quot;viridis&quot;, # パレットの種類（magma / inferno / plasma / viridis / cividis） direction = 1 # 色の順序（-1で逆順） ) 4.23.4 ggsciカラーパレット（離散型変数） 様々な科学ジャーナルで使用されるカラーパレットです。パレットの種類などの詳細はこちらをご覧ください。 ggplot(data = mpg, mapping = aes(x = fl, color = fl, fill = fl) ) + geom_bar() + # 色 scale_color_npg() + scale_fill_npg() 4.23.5 distillerカラーパレット（連続型変数） こちらは、RColorBrewerパッケージの連続型変数用の関数です。様々な種類の美麗なプリセットカラーパレットが利用できます。 # カラーパレットの確認 RColorBrewer::display.brewer.all() ggplot(data = mpg, mapping = aes(x = hwy, fill = ..x..) ) + geom_dotplot() + # 色 scale_fill_distiller(palette = &quot;Blues&quot;, # カラーパレット名 direction = 1 # 色の順序（-1で逆順） ) 4.23.6 Viridisカラーパレット（連続型変数） 色覚障害に対応したカラーパレットです。パレットの種類などの詳細はこちらを参照してください。 ggplot(data = mpg, mapping = aes(x = hwy, fill = ..x..) ) + geom_dotplot() + # 色 scale_fill_viridis_c(option = &quot;viridis&quot;, # パレットの種類（magma / inferno / plasma / viridis / cividis） direction = 1 # 色の順序（-1で逆順） ) 4.23.7 gradientカラーパレット（連続型変数） 自分で色を指定してグラデーションを作成するカラーパレットです。2色グラデーション、3色グラデーション、n色グラデーションの3種類の関数があります。n色グラデーションでは、事前にベースとなるカラーパレットをRColorBrewer::display.brewer.all()関数などで確認して使用します。 # 2色グラデーション ggplot(data = mpg, mapping = aes(x = hwy, fill = ..x..) ) + geom_dotplot() + # 色 scale_fill_gradient(low = &quot;Red&quot;, # 最小値の色 high = &quot;Forestgreen&quot; # 最大値の色 ) # 3色グラデーション ggplot(data = mpg, mapping = aes(x = hwy, fill = ..x..) ) + geom_dotplot() + # 色 scale_fill_gradient2(low = &quot;Red&quot;, # 最小値の色 mid = &quot;Gold&quot;, # 中間値の色 high = &quot;Forestgreen&quot;, # 最大値の色 midpoint = 30 # 中間値 ) # パレットを指定したn色グラデーション ggplot(data = mpg, mapping = aes(x = hwy, fill = ..x..) ) + geom_dotplot() + # 色 scale_fill_gradientn(colors = brewer.pal(name = &quot;RdYlGn&quot;, n = 9) ) 4.24 設定：軸 X軸、Y軸の設定を行うには、scale_x_*()・scale_y_*()関数ファミリーを使用します。X軸、Y軸に指定する変数の型によって、使用する関数が異なります。 まず、プロット用のデータを作成します。 data_owid_scale &lt;- data_owid %&gt;% dplyr::group_by(location) %&gt;% dplyr::summarise(across(c(new_cases, new_deaths), mean, na.rm = TRUE)) summary(data_owid_scale) ## location new_cases new_deaths ## Length:255 Min. : 0.0 Min. : 0.000 ## Class :character 1st Qu.: 19.8 1st Qu.: 0.132 ## Mode :character Median : 168.9 Median : 1.677 ## Mean : 9862.7 Mean : 87.935 ## 3rd Qu.: 1247.7 3rd Qu.: 14.362 ## Max. :570847.5 Max. :5158.296 ## NA&#39;s :9 NA&#39;s :9 4.24.1 X軸：連続型、Y軸：連続型 ggplot(data = data_owid_scale, mapping = aes(x = new_cases, y = new_deaths) ) + geom_point() + geom_smooth() + # 軸 scale_x_continuous(breaks = breaks_x &lt;- seq(0, 1e+06, 100000), # 目盛 labels = breaks_x, # 目盛ラベル limits = c(0, 7e+05), # 下限・上限値（指定しない場合はNA） expand = expansion(mult = c(0.05, 0.05), add = c(0, 0)) # 下限・上限値からの余白（multは余白率、addは余白幅） ) + scale_y_continuous(breaks = breaks_y &lt;- seq(0, 10000, 1000), # 目盛 labels = breaks_y, # 目盛ラベル limits = c(0, 8000), # 下限・上限値（指定しない場合はNA） expand = expansion(mult = c(0.05, 0.05), add = c(0, 0)) # 下限・上限値からの余白（multは余白率、addは余白幅） ) + labs(x = &quot;新規感染者数（1日当たり平均、人）&quot;, # X軸のタイトル y = &quot;新規死亡者数（1日当たり平均、人）&quot; # Y軸のタイトル ) + theme(axis.title = element_text(size = 8), # 軸タイトルの文字サイズ axis.text.x = element_text(angle = 0, # X軸ラベルの角度 hjust = 0.5, # X軸ラベルの横整列位置（0-1） vjust = 0.5), # X軸ラベルの縦整列位置（0-1） axis.text.y = element_text(angle = 0, # Y軸ラベルの角度 hjust = 0.5, # Y軸ラベルの横整列位置（0-1） vjust = 0.5) # Y軸ラベルの縦整列位置（0-1） ) ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; 4.24.2 対数目盛 ggplot(data = data_owid_scale, mapping = aes(x = new_cases, y = new_deaths) ) + geom_point() + geom_smooth() + # 軸 scale_x_log10(breaks = breaks_x &lt;- 10 ** seq(0, 6, 1), # 目盛 labels = c(breaks_x[1:4], &quot;1万&quot;, &quot;10万&quot;, &quot;100万&quot;), # 目盛ラベル limits = c(1, 7e+05), # 下限・上限値（指定しない場合はNA） expand = expansion(mult = c(0.05, 0.05), add = c(0, 0)) # 下限・上限値からの余白（multは余白率、addは余白幅） ) + scale_y_log10(breaks = breaks_y &lt;- 10 ** seq(0, 4, 1), # 目盛 labels = c(breaks_y[1:4], &quot;1万&quot;), # 目盛ラベル limits = c(1, 8000), # 下限・上限値（指定しない場合はNA） expand = expansion(mult = c(0.05, 0.05), add = c(0, 0)) # 下限・上限値からの余白（multは余白率、addは余白幅） ) + labs(x = &quot;新規感染者数（1日当たり平均、人）&quot;, # X軸のタイトル y = &quot;新規死亡者数（1日当たり平均、人）&quot; # Y軸のタイトル ) + theme(axis.title = element_text(size = 8), # 軸タイトルの文字サイズ axis.text.x = element_text(angle = 0, # X軸ラベルの角度 hjust = 0.5, # X軸ラベルの横整列位置（0-1） vjust = 0.5 # X軸ラベルの縦整列位置（0-1） ), axis.text.y = element_text(angle = 0, # Y軸ラベルの角度 hjust = 0.5, # Y軸ラベルの横整列位置（0-1） vjust = 0.5 # Y軸ラベルの縦整列位置（0-1） ) ) ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; 4.24.3 X軸：日付型、Y軸：連続型 # date_breaks引数を使用する場合 ggplot(data = economics, mapping = aes(x = date, y = unemploy) ) + geom_line() + # 軸 scale_x_date(date_breaks = &quot;3 month&quot;, # 日付目盛の周期 date_labels = &quot;%y/%m&quot;, # %Y：4桁年、%y：2桁年、%m：2桁月、%d：日 limits = c(as.Date(&quot;2000-01-01&quot;), as.Date(&quot;2002-12-31&quot;)), # 始期・終期（指定しない場合はNA） expand = expansion(mult = c(0.00, 0.00), add = c(0, 0)) # 始期・終期からの余白（multは余白率、addは余白幅） ) + scale_y_continuous(breaks = breaks_y &lt;- seq(0, 50000, 1000), # 目盛 labels = breaks_y, # 目盛ラベル limits = c(5000, 10000), # 下限・上限値（指定しない場合はNA） expand = expansion(mult = c(0.05, 0.05), add = c(0, 0)) # 下限・上限値からの余白（multは余白率、addは余白幅） ) + labs(x = &quot;日付（年/月）&quot;, # X軸のタイトル y = &quot;失業者数（千人）&quot; # Y軸のタイトル ) + theme(axis.title = element_text(size = 8), # 軸タイトルの文字サイズ axis.text.x = element_text(angle = 0, # X軸ラベルの角度 hjust = 0.5, # X軸ラベルの横整列位置（0-1） vjust = 0.5 # X軸ラベルの縦整列位置（0-1） ), axis.text.y = element_text(angle = 0, # Y軸ラベルの角度 hjust = 0.5, # Y軸ラベルの横整列位置（0-1） vjust = 0.5 # Y軸ラベルの縦整列位置（0-1） ) ) # date_breaks引数を使用しない場合（結果は同じ） ggplot(data = economics, mapping = aes(x = date, y = unemploy) ) + geom_line() + scale_x_date(breaks = breaks_x &lt;- seq(as.Date(&quot;1967-01-01&quot;), max(economics$date), by = &quot;3 months&quot;), # 日付目盛ベクトル date_labels = str_c(str_sub(lubridate::year(breaks_x), 3, 4), &quot;/&quot;, lubridate::month(breaks_x)), # 日付目盛ベクトルをラベル用に加工 limits = c(as.Date(&quot;2000-01-01&quot;), as.Date(&quot;2002-12-31&quot;)), # 始期・終期（指定しない場合はNA） expand = expansion(mult = c(0.00, 0.00), add = c(0, 0)) # 始期・終期からの余白（multは余白率、addは余白幅） ) + scale_y_continuous(breaks = breaks_y &lt;- seq(0, 50000, 1000), # 目盛 labels = breaks_y, # 目盛ラベル limits = c(5000, 10000), # 下限・上限値（指定しない場合はNA） expand = expansion(mult = c(0.05, 0.05), add = c(0, 0)) # 下限・上限値からの余白（multは余白率、addは余白幅） ) + labs(x = &quot;日付（年/月）&quot;, # X軸のタイトル y = &quot;失業者数（千人）&quot; # Y軸のタイトル ) + theme(axis.title = element_text(size = 8), # 軸タイトルの文字サイズ axis.text.x = element_text(angle = 0, # X軸ラベルの角度 hjust = 0.5, # X軸ラベルの横整列位置（0-1） vjust = 0.5 # X軸ラベルの縦整列位置（0-1） ), axis.text.y = element_text(angle = 0, # Y軸ラベルの角度 hjust = 0.5, # Y軸ラベルの横整列位置（0-1） vjust = 0.5 # Y軸ラベルの縦整列位置（0-1） ) ) 4.25 設定：パネル目盛線 ggplot(data = mpg, mapping = aes(x = cty, y = hwy, fill = class) ) + geom_point(shape = 21, size = 2.0 ) + # パネル目盛線 theme(panel.grid.major = element_line(color = &quot;grey&quot;, # 主目盛線の色 linetype = &quot;dashed&quot;, # 主目盛線の種類（solid / dashed / dotted / dotdash / twodash / longdash） size = 0.2 # 主目盛線の太さ ), panel.grid.minor = element_blank() # 補助目盛り線は表示しない ) 4.26 設定：凡例 ggplot(data = mpg, mapping = aes(x = cty, y = hwy, fill = class) ) + geom_point(shape = 21, size = 2.0 ) + # 凡例 theme(legend.title = element_text(size = 8, # 凡例タイトルの文字サイズ face = &quot;bold&quot;, # 凡例タイトルの書体 hjust = 0.0 # 凡例タイトルの横整列位置 ), legend.text = element_text(size = 8), # 凡例ラベルの文字サイズ legend.box.background = element_rect(color = &quot;grey&quot;, # 凡例の枠線の色 size = 0.5 # 凡例の枠線の太さ ), legend.margin = margin(t = 1, # 凡例の上マージン b = 1, # 凡例の下マージン r = 1, # 凡例の右マージン l = 1, # 凡例の左マージン unit = &quot;mm&quot; # 凡例マージンの単位 ), legend.justification = c(1.0, 0.0), # 凡例の横整列位置・縦整列位置 legend.position = c(0.95, 0.05) # 凡例の横位置・縦位置 ) + labs(fill = &quot;車体クラス&quot;, # 凡例に使用するscaleのタイトル ) + guides(fill = guide_legend(keywidth = unit(5, units = &quot;mm&quot;), # 凡例キーの幅 keyheight = unit(5, units = &quot;mm&quot;), # 凡例キーの高さ direction = &quot;vertical&quot;, # 凡例の整列方向（horizontal / vertical） nrow = 3, # 凡例の行数 ncol = 3, # 凡例の列数 reverse = FALSE # 凡例順序の逆転 ) ) 4.27 設定：タイトル・キャプション ggplot(data = mpg, mapping = aes(x = cty, y = hwy, fill = class) ) + geom_point(shape = 21, size = 2.0 ) + # タイトル・キャプション labs(title = &quot;車体クラス別の一般道燃費と高速道燃費&quot;, # 図表タイトル caption = &quot;（出所）EPA.gov&quot;, # キャプション ) + theme(plot.title = element_text(size = 10, # 図表タイトルの文字サイズ face = &quot;bold&quot;, # 図表タイトルの書体 hjust = 0.5 # 図表タイトルの横整列位置 ), plot.caption = element_text(size = 8, # キャプションの文字サイズ face = &quot;plain&quot;, # キャプションの書体 hjust = 0.0 # キャプションの横整列位置 ) ) 4.28 設定：フォント・マージン ggplot(data = mpg, mapping = aes(x = cty, y = hwy, fill = class) ) + # フォント・マージン geom_point(shape = 21, size = 2.0 ) + theme(text = element_text(size = 8 # 図表全体の無事サイズ ), plot.margin = margin(t = 1, # 図表の上マージン b = 1, # 図表の下マージン r = 1, # 図表の右マージン l = 1, # 図表の左マージン unit = &quot;mm&quot; # 図表マージンの単位 ) ) 4.29 グラフの保存 ggplot2パッケージで作成した図表を画像形式で保存するには、ggsave()関数を使用します。 ggsave(filename = &quot;directory/filename.png&quot;, # 図表のファイル名 width = 12.00, # 図表の横サイズ height = 8.50, # 図表の縦サイズ units = &quot;cm&quot;, # 図表のサイズ単位 dpi = 400 # 図表の解像度 ) 4.30 実例 4.30.1 実例1：折れ線グラフ 日本と欧州主要国の新型コロナ新規感染者数の時系列折れ線グラフを作成します。 まず、外部データセットdata_owidを用い、プロット用のデータを作成します。外部データセットの取得方法については、第1章「Rの設定」を参照してください。 # 列を選択 data_plot &lt;- data_owid %&gt;% dplyr::select(location, date, new_cases_smoothed) # 日本と欧州主要国をフィルタ data_plot %&lt;&gt;% dplyr::filter(location %in% c(&quot;Japan&quot;, &quot;United Kingdom&quot;, &quot;Germany&quot;, &quot;France&quot;, &quot;Italy&quot;), date &gt;= &quot;2022-01-01&quot; ) # NAを削除 data_plot %&lt;&gt;% tidyr::drop_na(everything()) ggplot()関数でプロットを作成します。グラフを画像として保存するには、別途ggsave()関数を実行してください。 ggplot(data = data_owid_line, mapping = aes(x = date, y = new_cases_smoothed / 1000, color = location) ) + # グラフ geom_line(alpha = 1.0, # 線の透明度 linetype = &quot;solid&quot;, # 線の種類（solid / dashed / dotted / dotdash / twodash / longdash） size = 0.5 # 線の太さ ) + # 色 scale_color_npg() + # 軸 scale_x_date(date_breaks = &quot;1 month&quot;, # 日付目盛の周期 date_labels = &quot;%y/%m&quot;, # 日付フォーマット（%Y：4桁年、%y：2桁年、%m：2桁月、%d：日） limits = c(as.Date(&quot;2022-01-01&quot;), NA), # 始期・終期（指定しない場合はNA） expand = expansion(mult = c(0.01, 0.01), add = c(0, 0)) # 始期・終期からの余白（multは余白率、addは余白幅） ) + scale_y_continuous(breaks = breaks_y &lt;- seq(0, 1000, 50), # 目盛 labels = breaks_y, # 目盛ラベル limits = c(0, NA), # 下限・上限値（指定しない場合はNA） expand = expansion(mult = c(0.00, 0.05), add = c(0, 0)) # 下限・上限値からの余白（multは余白率、addは余白幅） ) + labs(x = &quot;日付（年/月）&quot;, # X軸のタイトル y = &quot;新規感染者数（千人/日）&quot; # Y軸のタイトル ) + theme(axis.title = element_text(size = 8), # 軸タイトルの文字サイズ axis.text.x = element_text(angle = 0, # X軸ラベルの角度 hjust = 0.5, # X軸ラベルの横整列位置（0-1） vjust = 0.5 # X軸ラベルの縦整列位置（0-1） ), axis.text.y = element_text(angle = 0, # Y軸ラベルの角度 hjust = 0.5, # Y軸ラベルの横整列位置（0-1） vjust = 0.5 # Y軸ラベルの縦整列位置（0-1） ) ) + # パネル目盛線 theme(panel.grid.major = element_line(color = &quot;grey&quot;, # 主目盛線の色 linetype = &quot;dashed&quot;, # 主目盛線の種類（solid / dashed / dotted / dotdash / twodash / longdash） size = 0.2 # 主目盛線の太さ ), panel.grid.minor = element_blank() # 補助目盛り線は表示しない ) + # 凡例 theme(legend.title = element_text(size = 8, # 凡例タイトルの文字サイズ face = &quot;bold&quot;, # 凡例タイトルの書体 hjust = 0.0 # 凡例タイトルの横整列位置 ), legend.text = element_text(size = 8 # 凡例ラベルの文字サイズ ), legend.box.background = element_rect(color = &quot;grey&quot;, # 凡例の枠線の色 size = 0.5 # 凡例の枠線の太さ ), legend.margin = margin(t = 1, # 凡例の上マージン b = 1, # 凡例の下マージン r = 1, # 凡例の右マージン l = 1, # 凡例の左マージン unit = &quot;mm&quot; # 凡例マージンの単位 ), legend.justification = c(1.0, 1.0), # 凡例の横整列位置・縦整列位置 legend.position = c(0.97, 0.95) # 凡例の横位置・縦位置 ) + guides(color = guide_legend(keywidth = unit(5, units = &quot;mm&quot;), # 凡例キーの幅 keyheight = unit(5, units = &quot;mm&quot;), # 凡例キーの高さ direction = &quot;vertical&quot;, # 凡例の整列方向（horizontal / vertical） nrow = 5, # 凡例の行数 ncol = 1, # 凡例の列数 reverse = FALSE) # 凡例順序の逆転 ) + # タイトル・キャプション labs(title = &quot;日本と欧州主要国の新型コロナ新規感染者数&quot;, # 図表タイトル caption = &quot;（注）後方7日間移動平均。\\n（出所）Our World in Data、@naohat23&quot;, # キャプション color = &quot;国名&quot; # 凡例に使用するscaleのタイトル ) + theme(plot.title = element_text(size = 10, # 図表タイトルの文字サイズ face = &quot;bold&quot;, # 図表タイトルの書体 hjust = 0.5 # 図表タイトルの横整列位置 ), plot.caption = element_text(size = 8, # キャプションの文字サイズ face = &quot;plain&quot;, # キャプションの書体 hjust = 0.0 # キャプションの横整列位置 ) ) + # フォント・マージン theme(text = element_text(size = 8 # 図表全体の無事サイズ ), plot.margin = margin(t = 1, # 図表の上マージン b = 1, # 図表の下マージン r = 1, # 図表の右マージン l = 1, # 図表の左マージン unit = &quot;mm&quot; # 図表マージンの単位 ) ) 4.30.2 実例2：積み上げ棒グラフ 日本の実質GDP変化率をマーカー付き折れ線グラフに、内訳の需要項目別寄与度を積み上げ棒グラフにした、時系列グラフを作成します。 まず、外部データセットdata_gdp_jpを用い、プロット用のデータを作成します。外部データセットの取得方法については、第1章「Rの設定」を参照してください。 # 列を集約 data_plot &lt;- data_gdp_jp %&gt;% dplyr::mutate(`家計` = `民間最終消費支出` + `民間住宅`, `政府` = `政府最終消費支出` + `公的固定資本形成` + `公的在庫変動` ) %&gt;% dplyr::select(-`民間最終消費支出`, -`民間住宅`,-`政府最終消費支出`, -`公的固定資本形成`, -`公的在庫変動`) # 列名を変更 data_plot %&lt;&gt;% dplyr::rename(`GDP` = `国内総生産`, `設備投資` = `民間企業設備`, `在庫` = `民間在庫変動` ) # 実質GDPの前期比変化率に対する各需要項目の寄与度を計算 data_plot %&lt;&gt;% dplyr::mutate(across(`GDP`:`政府`, ~ {100 * (. - dplyr::lag(., 1)) / dplyr::lag(`GDP`, 1)})) # グラフ表示範囲を選択 data_plot %&lt;&gt;% dplyr::filter(`日付` &gt;= &quot;2019-01-01&quot;) # GDP全体と寄与度にデータを分割 data_plot_gdp &lt;- data_plot %&gt;% dplyr::select(`日付`, `GDP`) data_plot_cont &lt;- data_plot %&gt;% dplyr::select(-`GDP`, -`開差`) # 寄与度データを縦型に変換し、項目の順序を設定 data_plot_cont %&lt;&gt;% tidyr::pivot_longer(cols = -`日付`, names_to = &quot;需要項目&quot;, values_to = &quot;寄与度&quot;) %&gt;% dplyr::mutate(`需要項目` = factor(`需要項目`, level = c(&quot;家計&quot;, &quot;設備投資&quot;, &quot;在庫&quot;, &quot;政府&quot;, &quot;純輸出&quot;) ) ) %&gt;% dplyr::arrange(`日付`, `需要項目`) ggplot()関数でプロットを作成します。グラフを画像として保存するには、別途ggsave()関数を実行してください。 ggplot() + # グラフ geom_col(data = data_plot_cont, mapping = aes(x = `日付`, y = `寄与度`, fill = `需要項目`, group = rev(`需要項目`)), stat = &quot;identity&quot;, position = position_stack(), # 積み上げポジション alpha = 1.0, # 塗りつぶしの透明度 size = 0.5, # 線の太さ width = NULL # 棒の幅 (0-1) ) + guides(fill = guide_legend(reverse = TRUE)) + geom_line(data = data_plot_gdp, mapping = aes(x = `日付`, y = `GDP`), alpha = 1.0, # 線の透明度 color = &quot;black&quot;, # 線の色 linetype = &quot;solid&quot;, # 線の種類（solid / dashed / dotted / dotdash / twodash / longdash） size = 1.0 # 線の太さ ) + geom_point(data = data_plot_gdp, mapping = aes(x = `日付`, y = `GDP`),alpha = 1.0, # 塗りつぶしの透明度 color = &quot;black&quot;, # 線の色 fill = &quot;white&quot;, # 塗りつぶしの色 shape = 21, # マーカーの種類 size = 2.0, # マーカーの大きさ stroke = 0.5 # 線の太さ ) + # 補助線 geom_hline(yintercept = 0, # 水平線の縦軸との交点 color = &quot;grey&quot;, # 水平線の色 size = 0.5 # 水平線の太さ ) + # 色 scale_fill_brewer(palette = &quot;Paired&quot;, # カラーパレット名 direction = 1 # 色の順序（-1で逆順） ) + # 軸 scale_x_date(breaks = data_plot_gdp$日付, # 日付目盛の周期 date_labels = &quot;%y/%m&quot;, # 日付フォーマット（%Y：4桁年、%y：2桁年、%m：2桁月、%d：日） expand = expansion(mult = c(0.01, 0.01), add = c(0, 0)) # 始期・終期からの余白（multは余白率、addは余白幅） ) + scale_y_continuous(breaks = breaks_y &lt;- seq(-20, 20, 1), # 目盛 labels = breaks_y %&gt;% stringr::str_replace_all(&quot;-&quot;, &quot;▲&quot;), # 目盛ラベル limits = c(NA, NA), # 下限・上限値（指定しない場合はNA） expand = expansion(mult = c(0.05, 0.05), add = c(0, 0)) # 下限・上限値からの余白（multは余白率、addは余白幅） ) + labs(x = &quot;日付（年/月）&quot;, # X軸のタイトル y = &quot;実質GDP変化率（前期比、％）&quot; # Y軸のタイトル ) + theme(axis.title = element_text(size = 8), # 軸タイトルの文字サイズ axis.text.x = element_text(angle = 0, # X軸ラベルの角度 hjust = 0.5, # X軸ラベルの横整列位置（0-1） vjust = 0.5 # X軸ラベルの縦整列位置（0-1） ), axis.text.y = element_text(angle = 0, # Y軸ラベルの角度 hjust = 1.0, # Y軸ラベルの横整列位置（0-1） vjust = 0.5 # Y軸ラベルの縦整列位置（0-1） ) ) + # パネル目盛線 theme(panel.grid.major = element_line(color = &quot;grey&quot;, # 主目盛線の色 linetype = &quot;dashed&quot;, # 主目盛線の種類（solid / dashed / dotted / dotdash / twodash / longdash） size = 0.2 # 主目盛線の太さ ), panel.grid.minor = element_blank() # 補助目盛り線は表示しない ) + # 凡例 theme(legend.title = element_text(size = 8, # 凡例タイトルの文字サイズ face = &quot;bold&quot;, # 凡例タイトルの書体 hjust = 0.0 # 凡例タイトルの横整列位置 ), legend.text = element_text(size = 8), # 凡例ラベルの文字サイズ legend.box.background = element_rect(color = &quot;grey&quot;, # 凡例の枠線の色 size = 0.5 # 凡例の枠線の太さ ), legend.margin = margin(t = 1, # 凡例の上マージン b = 1, # 凡例の下マージン r = 1, # 凡例の右マージン l = 1, # 凡例の左マージン unit = &quot;mm&quot; # 凡例マージンの単位 ), legend.justification = c(0.5, 0.5), # 凡例の横整列位置・縦整列位置 legend.position = &quot;right&quot; # 凡例の横位置・縦位置 ) + guides(fill = guide_legend(keywidth = unit(5, units = &quot;mm&quot;), # 凡例キーの幅 keyheight = unit(5, units = &quot;mm&quot;), # 凡例キーの高さ direction = &quot;vertical&quot;, # 凡例の整列方向（horizontal / vertical） nrow = 5, # 凡例の行数 ncol = 1, # 凡例の列数 reverse = FALSE # 凡例順序の逆転 ) ) + # タイトル・キャプション labs(title = &quot;日本の実質GDP変化率の項目別寄与度&quot;, # 図表タイトル caption = &quot;（注）家計は個人消費と住宅の合計。\\n（出所）内閣府「四半期別GDP速報」。&quot;, # キャプション fill = element_blank() # 凡例に使用するscaleのタイトル ) + theme(plot.title = element_text(size = 10, # 図表タイトルの文字サイズ face = &quot;bold&quot;, # 図表タイトルの書体 hjust = 0.5 # 図表タイトルの横整列位置 ), plot.caption = element_text(size = 8, # キャプションの文字サイズ face = &quot;plain&quot;, # キャプションの書体 hjust = 0.0 # キャプションの横整列位置 ) ) + # フォント・マージン theme(text = element_text(size = 8 # 図表全体の無事サイズ ), plot.margin = margin(t = 1, # 図表の上マージン b = 1, # 図表の下マージン r = 1, # 図表の右マージン l = 1, # 図表の左マージン unit = &quot;mm&quot; # 図表マージンの単位 ) ) 4.30.3 実例3：箱ひげ図 世界各国の新型コロナ死亡率の分布を年別に集計した箱ひげ図を作成します。 まず、外部データセットdata_owidを用い、プロット用のデータを作成します。外部データセットの取得方法については、第1章「Rの設定」を参照してください。 # 新規感染者数と新規死亡者数の列を選択 data_plot &lt;- data_owid %&gt;% dplyr::select(location, date, new_cases, new_deaths) # 直近データを除外 data_plot %&lt;&gt;% dplyr::filter(date &lt;= max(date) - 28) # 年の列を追加し年別の平均を集計 data_plot %&lt;&gt;% dplyr::mutate(year = lubridate::year(date)) %&gt;% dplyr::group_by(location, year) %&gt;% dplyr::summarise(across(c(new_cases, new_deaths), mean, na.rm = TRUE)) # 年別の死亡率を計算 data_plot %&lt;&gt;% dplyr::mutate(mort_rate = 100 * new_deaths / new_cases, year = str_c(year, &quot;年&quot;) ) ggplot()関数でプロットを作成します。グラフを画像として保存するには、別途ggsave()関数を実行してください。 ggplot(data = data_plot, mapping = aes(x = year, y = mort_rate, color = year, fill = year) ) + # グラフ geom_boxplot(outlier.shape = NA, alpha = 0.25, size = 0.5) + geom_jitter(alpha = 0.5, size = 1.0, shape = 21) + # 補助線 geom_hline(yintercept = 0, # 水平線の縦軸との交点 color = &quot;gray&quot;, # 水平線の色 size = 0.5 # 水平線の太さ ) + # 軸 scale_y_continuous(breaks = breaks_y &lt;- seq(0, 100, 0.5), # 目盛 labels = breaks_y %&gt;% sprintf(fmt = &quot;%0.1f&quot;), # 目盛ラベル limits = c(0, 5), # 下限・上限値（指定しない場合はNA） expand = expansion(mult = c(0.05, 0.05), add = c(0, 0)) # 下限・上限値からの余白（multは余白率、addは余白幅） ) + labs(x = element_blank(), # X軸のタイトル y = &quot;新型コロナ感染者の致死率（％）&quot; # Y軸のタイトル ) + theme(axis.title = element_text(size = 8) # 軸タイトルの文字サイズ ) + # パネル目盛線 theme(panel.grid.major = element_line(color = &quot;grey&quot;, # 主目盛線の色 linetype = &quot;dashed&quot;, # 主目盛線の種類（solid / dashed / dotted / dotdash / twodash / longdash） size = 0.2 # 主目盛線の太さ ), panel.grid.minor = element_blank() # 補助目盛り線は表示しない ) + # 凡例 theme(legend.position = &quot;none&quot; # 凡例の横位置・縦位置 ) + # タイトル・キャプション labs(title = &quot;世界各国の新型コロナ感染者の致死率（年別）&quot;, # 図表タイトル caption = &quot;（注）箱の中の横線は中央値を示す。2022年は直近4週間のサンプルを除外。\\n（出所）Our World in Data、@naohat23&quot; # キャプション ) + theme(plot.title = element_text(size = 10, # 図表タイトルの文字サイズ face = &quot;bold&quot;, # 図表タイトルの書体 hjust = 0.5 # 図表タイトルの横整列位置 ), plot.caption = element_text(size = 8, # キャプションの文字サイズ face = &quot;plain&quot;, # キャプションの書体 hjust = 0.0 # キャプションの横整列位置 ) ) + # フォント・マージン theme(text = element_text(size = 8 # 図表全体の無事サイズ ), plot.margin = margin(t = 1, # 図表の上マージン b = 1, # 図表の下マージン r = 1, # 図表の右マージン l = 1, # 図表の左マージン unit = &quot;mm&quot; # 図表マージンの単位 ) ) "],["探索的データ分析.html", "5 探索的データ分析 5.1 第5章の準備 5.2 データの中身（記述統計量） 5.3 データの分布 5.4 データの関係性 5.5 相関関係・ペアプロット 5.6 探索的データ分析の一括実行", " 5 探索的データ分析 第5章「探索的データ分析（Exploratory Data Analysis）」では、データの内容を理解するための方法を解説します。これは、本格的な統計モデルを構築する方針を立てるための重要なプロセスです。 Wickham &amp; Grolemund（2017）は、探索的データ分析について次のように説明しています。 分析対象のデータについて問いを立てる。 可視化、変換、モデル化により、問いに対する解を探る。 得られた解をもとに、新たな問いを立てる。 このサイクルを繰り返す。 このように、探索的データ分析には本来決まったやり方が存在するわけではありません。とはいえ、「問いの立て方」や「解の探り方」には一般的によく利用される手法があるため、この章ではそうした手法について解説します。 5.1 第5章の準備 5.1.1 パッケージのインポート library(corrplot) library(corrr) library(DataExplorer) library(GGally) library(magrittr) library(SmartEDA) library(tidyverse) library(psych) 5.1.2 ggplot2の設定 第1章で紹介したggplot2の設定です。筆者の実行環境はMacのため、Windowsの設定はコメントアウトしています。 # Windowsのグラフ設定（筆者の実行環境では不要のためコメントアウト） # windowsFonts(&quot;MEIRYO&quot; = windowsFont(&quot;Meiryo UI&quot;)) # windowsFonts(&quot;YUGO&quot; = windowsFont(&quot;Yu Gothic UI&quot;)) # theme_set(theme_light(base_family = &quot;YUGO&quot;)) # Macのグラフ設定 theme_set(theme_light(base_family = &quot;HiraKakuProN-W3&quot;)) 5.2 データの中身（記述統計量） まず、データが何を含んでいるか確認します。 summary()関数は、各変数の記述統計量を出力する関数です。数値型の変数は、平均、中央値、最大値、最小値、四分位数が出力されます。離散型変数のうち、因子型（factor型）の変数は要素毎のサンプル数が出力されます。 # データをコンソールに出力 diamonds ## # A tibble: 53,940 × 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 ## 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 ## 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 ## 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 ## 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 ## # … with 53,930 more rows # データの変数名を出力 colnames(diamonds) ## [1] &quot;carat&quot; &quot;cut&quot; &quot;color&quot; &quot;clarity&quot; &quot;depth&quot; &quot;table&quot; &quot;price&quot; ## [8] &quot;x&quot; &quot;y&quot; &quot;z&quot; # データの記述統計量を出力 summary(diamonds) ## carat cut color clarity depth ## Min. :0.2000 Fair : 1610 D: 6775 SI1 :13065 Min. :43.00 ## 1st Qu.:0.4000 Good : 4906 E: 9797 VS2 :12258 1st Qu.:61.00 ## Median :0.7000 Very Good:12082 F: 9542 SI2 : 9194 Median :61.80 ## Mean :0.7979 Premium :13791 G:11292 VS1 : 8171 Mean :61.75 ## 3rd Qu.:1.0400 Ideal :21551 H: 8304 VVS2 : 5066 3rd Qu.:62.50 ## Max. :5.0100 I: 5422 VVS1 : 3655 Max. :79.00 ## J: 2808 (Other): 2531 ## table price x y ## Min. :43.00 Min. : 326 Min. : 0.000 Min. : 0.000 ## 1st Qu.:56.00 1st Qu.: 950 1st Qu.: 4.710 1st Qu.: 4.720 ## Median :57.00 Median : 2401 Median : 5.700 Median : 5.710 ## Mean :57.46 Mean : 3933 Mean : 5.731 Mean : 5.735 ## 3rd Qu.:59.00 3rd Qu.: 5324 3rd Qu.: 6.540 3rd Qu.: 6.540 ## Max. :95.00 Max. :18823 Max. :10.740 Max. :58.900 ## ## z ## Min. : 0.000 ## 1st Qu.: 2.910 ## Median : 3.530 ## Mean : 3.539 ## 3rd Qu.: 4.040 ## Max. :31.800 ## 因子型でない離散型変数（文字列型など）について要素毎のサンプル数を出力する場合は、あらかじめ因子型に変換し、summary()関数を適用します。 mpg %&gt;% dplyr::mutate(across(.cols = where(is.character), .fns = as.factor)) %&gt;% summary() ## manufacturer model displ year ## dodge :37 caravan 2wd : 11 Min. :1.600 Min. :1999 ## toyota :34 ram 1500 pickup 4wd: 10 1st Qu.:2.400 1st Qu.:1999 ## volkswagen:27 civic : 9 Median :3.300 Median :2004 ## ford :25 dakota pickup 4wd : 9 Mean :3.472 Mean :2004 ## chevrolet :19 jetta : 9 3rd Qu.:4.600 3rd Qu.:2008 ## audi :18 mustang : 9 Max. :7.000 Max. :2008 ## (Other) :74 (Other) :177 ## cyl trans drv cty hwy ## Min. :4.000 auto(l4) :83 4:103 Min. : 9.00 Min. :12.00 ## 1st Qu.:4.000 manual(m5):58 f:106 1st Qu.:14.00 1st Qu.:18.00 ## Median :6.000 auto(l5) :39 r: 25 Median :17.00 Median :24.00 ## Mean :5.889 manual(m6):19 Mean :16.86 Mean :23.44 ## 3rd Qu.:8.000 auto(s6) :16 3rd Qu.:19.00 3rd Qu.:27.00 ## Max. :8.000 auto(l6) : 6 Max. :35.00 Max. :44.00 ## (Other) :13 ## fl class ## c: 1 2seater : 5 ## d: 5 compact :47 ## e: 8 midsize :41 ## p: 52 minivan :11 ## r:168 pickup :33 ## subcompact:35 ## suv :62 psych::describe()関数は、各変数の記述統計量や分布に関する情報を出力する関数です。平均、中央値、標準偏差に加え、刈り込み平均（trimmed）、中央絶対偏差（mad）、レンジ（最大値と最小値の差）、歪度（skew）、尖度（kurtosis）、標準誤差（se）が出力されます。 # データの記述統計量や分布に関する情報を出力 psych::describe(diamonds) ## vars n mean sd median trimmed mad min max ## carat 1 53940 0.80 0.47 0.70 0.73 0.47 0.2 5.01 ## cut* 2 53940 3.90 1.12 4.00 4.04 1.48 1.0 5.00 ## color* 3 53940 3.59 1.70 4.00 3.55 1.48 1.0 7.00 ## clarity* 4 53940 4.05 1.65 4.00 3.91 1.48 1.0 8.00 ## depth 5 53940 61.75 1.43 61.80 61.78 1.04 43.0 79.00 ## table 6 53940 57.46 2.23 57.00 57.32 1.48 43.0 95.00 ## price 7 53940 3932.80 3989.44 2401.00 3158.99 2475.94 326.0 18823.00 ## x 8 53940 5.73 1.12 5.70 5.66 1.38 0.0 10.74 ## y 9 53940 5.73 1.14 5.71 5.66 1.36 0.0 58.90 ## z 10 53940 3.54 0.71 3.53 3.49 0.85 0.0 31.80 ## range skew kurtosis se ## carat 4.81 1.12 1.26 0.00 ## cut* 4.00 -0.72 -0.40 0.00 ## color* 6.00 0.19 -0.87 0.01 ## clarity* 7.00 0.55 -0.39 0.01 ## depth 36.00 -0.08 5.74 0.01 ## table 52.00 0.80 2.80 0.01 ## price 18497.00 1.62 2.18 17.18 ## x 10.74 0.38 -0.62 0.00 ## y 58.90 2.43 91.20 0.00 ## z 31.80 1.52 47.08 0.00 5.3 データの分布 次に、データに含まれる変数がどのように分布しているかを、第4章で解説したggplot2パッケージの関数を使用して可視化します。 5.3.1 離散型変数の度数分布 離散型変数の度数分布を出力するにはdplyr::count()関数を使用します。 diamonds %&gt;% dplyr::count(cut) ## # A tibble: 5 × 2 ## cut n ## &lt;ord&gt; &lt;int&gt; ## 1 Fair 1610 ## 2 Good 4906 ## 3 Very Good 12082 ## 4 Premium 13791 ## 5 Ideal 21551 離散型変数の度数分布を可視化するには、度数棒グラフを使用します。 diamonds %&gt;% ggplot(mapping = aes(x = cut)) + geom_bar() 5.3.2 連続型変数の度数分布 連続型変数の度数分布を出力するには、dplyr::count()関数とggplot2::cut_width()関数を使用します。 diamonds %&gt;% dplyr::count(ggplot2::cut_width(carat, 0.5)) ## # A tibble: 11 × 2 ## `ggplot2::cut_width(carat, 0.5)` n ## &lt;fct&gt; &lt;int&gt; ## 1 [-0.25,0.25] 785 ## 2 (0.25,0.75] 29498 ## 3 (0.75,1.25] 15977 ## 4 (1.25,1.75] 5313 ## 5 (1.75,2.25] 2002 ## 6 (2.25,2.75] 322 ## 7 (2.75,3.25] 32 ## 8 (3.25,3.75] 5 ## 9 (3.75,4.25] 4 ## 10 (4.25,4.75] 1 ## 11 (4.75,5.25] 1 連続型変数の度数分布を可視化するには、ヒストグラムを使用します。 diamonds %&gt;% ggplot(mapping = aes(x = carat)) + geom_histogram(binwidth = 0.5) 連続型変数の度数分布をグループ別に可視化するには、geom_freqpoly()関数を使用します。 diamonds %&gt;% ggplot(mapping = aes(x = carat, color = cut)) + geom_freqpoly(binwidth = 0.1) 連続型変数の度数を標準化して密度をグループ別に可視化するには、mapping = aes(y = ..density..)を指定して、geom_fredpoly()関数を使用します。 diamonds %&gt;% ggplot(mapping = aes(x = price, y = ..density.., color = cut)) + geom_freqpoly(binwidth = 500) 5.3.3 外れ値 外れ値や異常値をグラフで確認するには、ヒストグラムを使用し、coord_cartesian()関数のylim引数にY軸の下限・上限値を指定して、Y軸方向に図表を拡大します。 なお、scale_y_continuous()関数のlimits引数に下限・上限を指定する方法でもY軸の表示範囲を変えることができますが、下限・上限の範囲外にあるデータが表示されなくなるため、単に拡大するだけであればcoord_cartesian()関数を用いるほうが良いでしょう。 diamonds %&gt;% ggplot(mapping = aes(x = y)) + geom_histogram() + coord_cartesian(ylim = c(0, 50)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. グループ別に外れ値を可視化する場合は、箱ひげ図を使用します。 diamonds %&gt;% ggplot(mapping = aes(x = cut, y = carat)) + geom_boxplot() 5.4 データの関係性 データに含まれる変数が互いにどのような関係にあるかを、ggplot2パッケージの関数を使用して可視化します。 5.4.1 離散型変数の関係性 離散型変数の観測値の組み合わせの分布を可視化するには、geom_count()関数の度数バブルチャートや、geom_tile()関数のヒートマップを使用します。 diamonds %&gt;% ggplot(mapping = aes(x = cut, y = color)) + geom_count() diamonds %&gt;% dplyr::count(cut, color) %&gt;% ggplot(mapping = aes(x = cut, y = color, fill = n)) + geom_tile() 5.4.2 連続型変数の関係性 連続型変数の観測値の組み合わせの分布を可視化するには、geom_point()関数の散布図や、geom_bin2d()関数、geom_hex()関数のヒートマップを使用します。 データサイズが大きい場合は、geom_point()関数の実行に時間がかかるため、geom_bin2d()関数やgeom_hex()関数を用いるのが効果的です。 diamonds %&gt;% ggplot(mapping = aes(x = carat, y = price)) + geom_point(alpha = 0.05) diamonds %&gt;% ggplot(mapping = aes(x = carat, y = price)) + geom_bin2d(bins = 100) # X軸・Y軸の階級数（デフォルトは30） diamonds %&gt;% ggplot(mapping = aes(x = carat, y = price)) + geom_hex(bins = 50) # X軸・Y軸の階級数（デフォルトは30） 5.5 相関関係・ペアプロット 5.5.1 相関係数行列 cor()関数で相関係数行列を出力します。ここではサンプルデータセットとしてmtcarsデータセットを使用しています。 cor(mtcars) ## mpg cyl disp hp drat wt ## mpg 1.0000000 -0.8521620 -0.8475514 -0.7761684 0.68117191 -0.8676594 ## cyl -0.8521620 1.0000000 0.9020329 0.8324475 -0.69993811 0.7824958 ## disp -0.8475514 0.9020329 1.0000000 0.7909486 -0.71021393 0.8879799 ## hp -0.7761684 0.8324475 0.7909486 1.0000000 -0.44875912 0.6587479 ## drat 0.6811719 -0.6999381 -0.7102139 -0.4487591 1.00000000 -0.7124406 ## wt -0.8676594 0.7824958 0.8879799 0.6587479 -0.71244065 1.0000000 ## qsec 0.4186840 -0.5912421 -0.4336979 -0.7082234 0.09120476 -0.1747159 ## vs 0.6640389 -0.8108118 -0.7104159 -0.7230967 0.44027846 -0.5549157 ## am 0.5998324 -0.5226070 -0.5912270 -0.2432043 0.71271113 -0.6924953 ## gear 0.4802848 -0.4926866 -0.5555692 -0.1257043 0.69961013 -0.5832870 ## carb -0.5509251 0.5269883 0.3949769 0.7498125 -0.09078980 0.4276059 ## qsec vs am gear carb ## mpg 0.41868403 0.6640389 0.59983243 0.4802848 -0.55092507 ## cyl -0.59124207 -0.8108118 -0.52260705 -0.4926866 0.52698829 ## disp -0.43369788 -0.7104159 -0.59122704 -0.5555692 0.39497686 ## hp -0.70822339 -0.7230967 -0.24320426 -0.1257043 0.74981247 ## drat 0.09120476 0.4402785 0.71271113 0.6996101 -0.09078980 ## wt -0.17471588 -0.5549157 -0.69249526 -0.5832870 0.42760594 ## qsec 1.00000000 0.7445354 -0.22986086 -0.2126822 -0.65624923 ## vs 0.74453544 1.0000000 0.16834512 0.2060233 -0.56960714 ## am -0.22986086 0.1683451 1.00000000 0.7940588 0.05753435 ## gear -0.21268223 0.2060233 0.79405876 1.0000000 0.27407284 ## carb -0.65624923 -0.5696071 0.05753435 0.2740728 1.00000000 5.5.2 corrplotパッケージ R標準のcor()関数の出力結果では見にくいため、corrplotパッケージを使用して相関係数行列を可視化します。 # cor()関数で相関係数行列を計算し、その結果をcorrplot::corrplot()関数に渡す mtcars %&gt;% cor() %&gt;% corrplot::corrplot(method = &quot;square&quot;, # 可視化方法（&quot;circle&quot;, &quot;square&quot;, &quot;ellipse&quot;, &quot;number&quot;, &quot;shade&quot;, &quot;color&quot;, &quot;pie&quot;） type = &quot;full&quot;, # 表示形式（&quot;full&quot;, &quot;upper&quot;, &quot;lower&quot;） addCoef.col = &quot;black&quot;, # 相関係数の値の色 diag = FALSE, # 対角要素を表示するか number.cex = 0.8, # 相関係数の値のフォントサイズ number.digits = 2 # 相関係数の値の小数点以下桁数 ) corrplot::cor.mtest()関数を使用すると、「相関係数が0である」との帰無仮説に対するp値を計算し、その結果を可視化することができます。 # 相関係数の検定を行いp値を計算 mtcars_p &lt;- corrplot::cor.mtest(mtcars) # p値の計算結果を指定して検定結果を可視化 mtcars %&gt;% cor() %&gt;% corrplot::corrplot(method = &quot;square&quot;, # 可視化方法（&quot;circle&quot;, &quot;square&quot;, &quot;ellipse&quot;, &quot;number&quot;, &quot;shade&quot;, &quot;color&quot;, &quot;pie&quot;） type = &quot;full&quot;, # 表示形式（&quot;full&quot;, &quot;upper&quot;, &quot;lower&quot;） p.mat = mtcars_p$p, # p値の計算結果 sig.level = 0.05, # 有意水準 addCoef.col = &quot;black&quot;, # 相関係数の値の色 diag = FALSE, # 対角要素を表示するか number.cex = 0.8, # 相関係数の値のフォントサイズ number.digits = 2 # 相関係数の値の小数点以下桁数 ) 5.5.3 corrrパッケージ 次に、corrrパッケージを使用して相関係数を可視化します。 corrr::correlate()関数で相関係数行列を格納したcor_dfオブジェクトを作成します。 mtcars_cor &lt;- corrr::correlate(mtcars, method = &quot;pearson&quot;) # 相関係数の算出法（&quot;pearson&quot;, &quot;kendall&quot;, &quot;spearman&quot;） ## Correlation computed with ## • Method: &#39;pearson&#39; ## • Missing treated using: &#39;pairwise.complete.obs&#39; mtcars_cor ## # A tibble: 11 × 12 ## term mpg cyl disp hp drat wt qsec vs am ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mpg NA -0.852 -0.848 -0.776 0.681 -0.868 0.419 0.664 0.600 ## 2 cyl -0.852 NA 0.902 0.832 -0.700 0.782 -0.591 -0.811 -0.523 ## 3 disp -0.848 0.902 NA 0.791 -0.710 0.888 -0.434 -0.710 -0.591 ## 4 hp -0.776 0.832 0.791 NA -0.449 0.659 -0.708 -0.723 -0.243 ## 5 drat 0.681 -0.700 -0.710 -0.449 NA -0.712 0.0912 0.440 0.713 ## 6 wt -0.868 0.782 0.888 0.659 -0.712 NA -0.175 -0.555 -0.692 ## 7 qsec 0.419 -0.591 -0.434 -0.708 0.0912 -0.175 NA 0.745 -0.230 ## 8 vs 0.664 -0.811 -0.710 -0.723 0.440 -0.555 0.745 NA 0.168 ## 9 am 0.600 -0.523 -0.591 -0.243 0.713 -0.692 -0.230 0.168 NA ## 10 gear 0.480 -0.493 -0.556 -0.126 0.700 -0.583 -0.213 0.206 0.794 ## 11 carb -0.551 0.527 0.395 0.750 -0.0908 0.428 -0.656 -0.570 0.0575 ## # … with 2 more variables: gear &lt;dbl&gt;, carb &lt;dbl&gt; corrr::rearrange()関数を使用すると、高い相関を持つ変数を近くに配置するよう、変数の順序を自動で変更します。 mtcars_cor %&gt;% corrr::rearrange() ## Registered S3 methods overwritten by &#39;registry&#39;: ## method from ## print.registry_field proxy ## print.registry_entry proxy ## # A tibble: 11 × 12 ## term mpg vs drat am gear qsec carb hp wt ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mpg NA 0.664 0.681 0.600 0.480 0.419 -0.551 -0.776 -0.868 ## 2 vs 0.664 NA 0.440 0.168 0.206 0.745 -0.570 -0.723 -0.555 ## 3 drat 0.681 0.440 NA 0.713 0.700 0.0912 -0.0908 -0.449 -0.712 ## 4 am 0.600 0.168 0.713 NA 0.794 -0.230 0.0575 -0.243 -0.692 ## 5 gear 0.480 0.206 0.700 0.794 NA -0.213 0.274 -0.126 -0.583 ## 6 qsec 0.419 0.745 0.0912 -0.230 -0.213 NA -0.656 -0.708 -0.175 ## 7 carb -0.551 -0.570 -0.0908 0.0575 0.274 -0.656 NA 0.750 0.428 ## 8 hp -0.776 -0.723 -0.449 -0.243 -0.126 -0.708 0.750 NA 0.659 ## 9 wt -0.868 -0.555 -0.712 -0.692 -0.583 -0.175 0.428 0.659 NA ## 10 disp -0.848 -0.710 -0.710 -0.591 -0.556 -0.434 0.395 0.791 0.888 ## 11 cyl -0.852 -0.811 -0.700 -0.523 -0.493 -0.591 0.527 0.832 0.782 ## # … with 2 more variables: disp &lt;dbl&gt;, cyl &lt;dbl&gt; corrr::rplot()関数を使用して、相関係数行列を散布図で可視化します。corrr::shave()関数は、相関係数行列の左下半分のみを抽出する関数です。 mtcars_cor %&gt;% corrr::rearrange() %&gt;% corrr::shave() %&gt;% corrr::rplot(print_cor = TRUE) # 散布図上に相関係数の値を表示する corrr::network_plot()関数を使用すると、変数の相関関係をネットワーク図で可視化することができます。ネットワーク図では相関が強い変数が近くに配置されます。 mtcars_cor %&gt;% corrr::network_plot(min_cor = 0.6) # 表示する最小の相関係数の値 5.5.4 GGallyパッケージ GGallyパッケージのGGally::ggpairs()関数を使用すると、変数のペアプロット（pairwise plot）を簡単に出力することができます。なお、GGallyパッケージにはペアプロット以外にも様々な機能があります。詳細は公式ウェブサイトを参照してください。 mtcars %&gt;% dplyr::select(mpg, cyl, disp, hp, gear) %&gt;% GGally::ggpairs(progress = FALSE) # コンソールにプログラスバーを表示するか また、mapping = aes()内の引数にグループ化する変数を指定することで、グループ別のペアプロットを出力できます。ここではサンプルデータセットとしてirisデータセットを用いています。 iris %&gt;% GGally::ggpairs(mapping = aes(color = Species, alpha = 0.5), progress = FALSE # コンソールにプログラスバーを表示するか ) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 5.6 探索的データ分析の一括実行 これまで一つずつ実施してきた探索的データ分析を一括して実行するパッケージを紹介します。 5.6.1 DataExplorerパッケージ DataExplorerパッケージのcreate_report()関数を使用すると、データセットに含まれる変数、データ構造、欠損値、ヒストグラム、QQプロット、相関係数行列、主成分分析（PCA）などを自動で作成して、HTML形式のレポートをブラウザに出力します。ウェブサイト上では実行できないため、各自で試してみてください。 # irisデータセットに対し被説明変数を指定せずに探索的データ分析を実施 DataExplorer::create_report(data = iris) # diamondsデータセットに対し被説明変数をpriceに指定して探索的データ分析を実施 DataExplorer::create_report(data = diamonds, y = &quot;price&quot; ) 5.6.2 SmartEDAパッケージ SmartEDAパッケージのExpReport()関数を使用すると、データセットに含まれる変数、ヒストグラム、QQプロット、相関係数行列などを自動で作成して、HTML形式のレポートファイルを作成します。ウェブサイト上では実行できないため、各自で試してみてください。 なお、データセットのサイズが大きい場合は処理に時間がかかるため、注意が必要です。 # irisデータセットに対し被説明変数を指定せずに探索的データ分析を実施 SmartEDA::ExpReport(data = iris, op_file = &quot;report.html&quot; ) # diamondsデータセットに対し被説明変数をpriceに指定して探索的データ分析を実施 SmartEDA::ExpReport(data = diamonds, Target = &quot;price&quot;, op_file = &quot;report.html&quot; ) また、SmartEDA::ExpCatStat()では、Target引数で指定した被説明変数に対する各変数の予測力を計算することができます。 # mtcarsデータセットに対し被説明変数をamに指定して各変数の予測力を計算 SmartEDA::ExpCatStat(data = mtcars, Target = &quot;am&quot;, plot = TRUE ) ## Variable Target Unique Chi-squared p-value df IV Value Cramers V ## 1 cyl am 3 8.741 0.005 NA 1.32 0.52 ## 2 vs am 2 0.907 0.479 NA 0.11 0.17 ## 3 gear am 3 20.945 0.000 NA 0.44 0.81 ## 4 carb am 6 6.237 0.257 NA 0.17 0.44 ## 5 mpg am 10 20.945 0.002 NA 0.14 0.81 ## 6 disp am 10 21.636 0.003 NA 0.35 0.82 ## 7 hp am 10 17.490 0.020 NA 0.47 0.74 ## 8 drat am 10 21.497 0.001 NA 0.12 0.82 ## 9 wt am 10 20.254 0.006 NA 0.46 0.80 ## 10 qsec am 10 11.824 0.248 NA 0.54 0.61 ## Degree of Association Predictive Power ## 1 Strong Highly Predictive ## 2 Weak Somewhat Predictive ## 3 Strong Highly Predictive ## 4 Strong Somewhat Predictive ## 5 Strong Somewhat Predictive ## 6 Strong Highly Predictive ## 7 Strong Highly Predictive ## 8 Strong Somewhat Predictive ## 9 Strong Highly Predictive ## 10 Strong Highly Predictive "],["線形回帰-1.html", "6 線形回帰 6.1 第6章の準備 6.2 用語の説明 6.3 最小2乗法（OLS） 6.4 モデルの種類と係数の解釈 6.5 重回帰モデルの役割と注意点 6.6 Rの単回帰モデル推定 6.7 Rの重回帰モデル推定", " 6 線形回帰 第6章「線形回帰」では、計量経済学の基本である線形回帰モデルについて解説します。この章の説明は、山本（2015）、西山 他（2019）、末石（2015）をもとにしています。詳細は各参考文献を参照してください。 実例は、西山 他（2019）のデータセットを使用し、北川（2020）のウェブサイトに掲載されているRコードを参考にしています。 6.1 第6章の準備 6.1.1 パッケージのインポート library(estimatr) library(GGally) library(lmtest) library(magrittr) library(sandwich) library(tidyquant) library(tidyverse) 6.1.2 外部データセットの取得 この章では、西山 他（2019）のデータセットを使用します。西山 他（2019）のサポートウェブサイトからデータファイルを取得し、各自の実行環境のワーキングディレクトリ直下にdata_nishiyamaフォルダを作成して、その中に格納してください。 6.2 用語の説明 まず、統計学・計量経済学で使用される基本的な用語を説明します。 6.2.1 標本と推定 母集団： 調べる対象全体（population） 標本： 母集団から取り出した対象の一部（sample） 推定： 標本を使って母集団の分布に関する未知の値を言い当てること（estimation） 未知パラメータ： 推定すべき未知の値。一般的に\\(\\theta\\)で表す（unknown parameter） 統計量： 未知の値に依存せず、標本があれば値が計算できるデータの関数（statistic） 推定量： 推定に用いる統計量（estimator） 推定値： 未知の値について、実際にデータから計算して得た値（estimate） 点推定： 未知パラメータの値を1点で言い当てること（point estimation） 区間推定： 統計誤差を考慮して未知パラメータの値を区間で言い当てること（interval estimation） 統計誤差： 母集団全体でなく標本を用いることで推定結果に生じる誤差（statistical error） 6.2.2 推定量の望ましい性質 不偏性： 推定量の期待値が推定対象の未知パラメータの真の値に等しいこと（unbiasedness） 一致制： サンプルサイズが大きくなると、推定量が未知パラメータの真の値となる確率が1に近づくこと（consistency） 漸近正規性： サンプルサイズが大きいとき、そのサンプルから構成される確率変数が近似的に正規分布に従うこと（asymptomatic normality） 効率性： 推定量の分散が小さく、未知パラメータの真の値に近い値をとりやすいこと（efficiency） 頑健性： 異常値があっても推定量が影響を受けにくいこと（robustness） 6.3 最小2乗法（OLS） 次に、線形回帰の推定方法である最小2乗法と、統計的仮説検定について解説します。 変数\\(X\\)を用いて変数\\(Y\\)を推定する、次のようなモデルを考えます。 \\[ Y_i = \\beta_0 + \\beta_1X_i + u_i \\quad (i = 1,\\, \\ldots,\\, n) \\] ここで、\\(Y_i\\)は被説明変数（dependent variable）、\\(X_i\\)は説明変数（independent variable）、\\(u_i\\)は誤差項（error term）と呼ばれます。誤差項は\\(X_i\\)以外で\\(Y_i\\)に影響を与える要因をひとまとめにしたものと解釈できます。 6.3.1 最小2乗法の仮定 次の3つの仮定が成立するとき、OLS推定量は推定の望ましい性質である「不偏性」、「一致性」、「漸近正規性」を満たします。 仮定1：データが無作為抽出 標本が同一の母集団から無作為抽出（ランダムサンプリング）されているという仮定であり、数学的には、\\(Y_i\\)と\\(X_i\\)が独立かつ同一の分布に従う（independently and identically distributed、i.i.d）と定義されます。 多くのクロスセクションデータでは仮定1が成立しますが、時系列データや、時系列の要素を含むパネルデータでは仮定1が成立しないと考えるのが自然です。家計の消費支出の慣性効果など、経済主体の行動は過去からの影響を受けやすく、過去と現在のデータに相関があることが多いためです。 仮定1が成立しない場合、誤差項が自己相関（autocorrelation）・系列相関（serial correlation）をもち、推定量のばらつきが大きくなる問題が発生します。そのような場合に、仮定1のもとで計算した標準誤差（推定量の標準偏差を推定した値）を用いると分散の大きさを過小評価してしまうため、自己相関の可能性を考慮したHAC標準誤差（ニューウィー＝ウェストの標準誤差）を用いる必要があります。 なお、HAC標準誤差のHACはHeteroskedasticity and Autocorrelation Consistentの略であり、自己相関（autocorrelation）の問題だけでなく、この後で紹介する不均一分散（heteroskedasticity）の問題が存在しても対応可能であることを意味しています。サンプルサイズが十分大きければ、誤差項に自己相関がない場合にHAC標準誤差を使用しても、大きな問題は生じません。したがって、誤って「自己相関がない」と判断するリスクを考慮すれば、常にHAC標準誤差を用いるのが無難です（西山 他（2019）P.493）。 仮定2：説明変数と誤差項が無相関 外生性と呼ばれる仮定であり、\\(Y_i\\)の変動要因のうち\\(X_i\\)以外の部分（すなわち誤差項\\(u_i\\)）が平均的には\\(X_i\\)と無関係であることを意味します。数学的には、誤差項の条件付き期待値が説明変数に依存しない（\\(E(u_i|X_i) = 0\\)）と定義されます。 仮定2が成立していれば、モデルが因果関係を表していると解釈できます（ただし、仮定3が成立しているかどうかを回帰分析の結果から判断することはできません）。 仮定2が成立しないケースは多く、例えば、説明変数に本来含まれるべき変数が欠落している欠落変数バイアス、被説明変数が説明変数の変動要因になっている内生性バイアス（逆の因果性）などがあります。 仮定2が成立しない場合、OLS推定量は不偏性と一致性を満たさず、未知パラメータの値をきちんと推定することができない深刻な問題が発生します。対処方法として、他の説明変数の追加（重回帰モデル）による欠落変数バイアスの解消、パネルデータを用いた固定効果モデルの推定、2段階最小2乗法や操作変数法による内生性バイアスの解消を検討する必要があります。 仮定3：異常値が少ない 標本に含まれる異常値（データの絶対値が平均から大幅に離れている値）が少ないという仮定であり、数学的には変数の4次のモーメントが有限（\\(0 &lt; E(X_i^4) &lt; \\infty, \\quad 0 &lt; E(Y_i^4) &lt; \\infty\\)）と定義されます。すなわち、データの分布の裾が厚くない（ファット・テールでない）ということです。 一部の金融データなどでは異常値を取ることが多く（分布の裾が厚く）、仮定3が満たされない可能性があります。 仮定3が成立しない場合、OLS推定量が漸近正規性を満たさなくなります。漸近正規性は、直接的に推定量の良し悪しに関わる性質ではありませんが、係数に関する仮説検定や区間推定を行う基礎になる重要な性質です。 追加の仮定：誤差項の均一分散 被説明変数\\(Y_i\\)のデータの散らばり具合が説明変数\\(X_i\\)の値によらず一定であるとき、誤差項\\(u_i\\)の分散が一定、すなわち均一分散（homoskedasticity）であるといいます（\\(Var(u|X) = E(u^2|X) = \\sigma^2\\)）。 仮定1～3に加えて「誤差項の均一分散」の仮定が成立している場合、OLS推定量は線形な不偏推定量の中で「効率性」が最も高くなります（ガウス＝マルコフの定理）。これは、推定量の分散が最も小さく、推定量が真の値に近い値を取りやすいことを意味します。この時の推定量を、最良線形不偏推定量（best linear unbiased estimator, BLUE）と呼びます。 しかし、経済データ（特にクロスセクションデータ）で「誤差項の均一分散」の仮定が成立することは少なく、誤差項\\(u_i\\)の分散が説明変数\\(X_i\\)に依存する不均一分散（heteroskedasticity）の方がむしろ一般的と考えられます。例えば、複数の家計の所得・消費データから消費関数を推定する際、所得水準が高いほど消費スタイルの違いによる消費額のばらつきが大きいことが想定されますが、その場合は均一分散の仮定が成立しません。 均一分散の仮定が成立しない（不均一分散が生じている）場合は、推定量の効率性が悪化し、ばらつきが大きくなってしまう問題があります。そのようなケースで均一分散を仮定して標準誤差（推定量の標準偏差を推定した値）を計算すると分散の大きさを過小評価してしまうため、不均一分散の可能性を考慮した不均一分散に頑健な標準誤差（ホワイトの標準誤差）を用いる必要があります。ただし、時系列データのように自己相関の可能性が考えられる場合は、上記のように不均一分散と自己相関に頑健なHAC標準誤差（ニューウィー＝ウェストの標準誤差）を用いる方が良いでしょう。 6.3.2 検定と信頼区間 帰無仮説と対立仮説 モデル\\(Y_i = \\beta_0 + \\beta_1 X_i + u_i\\)について、次のような帰無仮説\\(H_0\\)（null hypothesis）と対立仮説\\(H_1\\)（alternative hypothesis）を考えます。 \\[ H_0 : \\beta_1 = 0 \\\\ H_1 : \\beta_1 \\neq 0 \\] 推定にあたり、我々が本当に興味があるのは未知パラメータ\\(\\beta_1\\)の真の値であり、またそれを言い当てるための推定値と標準誤差であるわけですが、ここでは「少なくとも未知パラメータの真の値がゼロではない（有意である）」ことを統計的に確認するために、こうした検定を行います。 t値 検定にはt統計量（t値）を用います。t統計量は、未知パラメータの推定量と帰無仮説の設定値の差を、標準誤差（推定量の標準偏差を推定した値）で割ったものです。 \\[ t = \\frac{\\text{推定量} - \\text{帰無仮説の設定値（ここではゼロ）}}{\\text{標準誤差}} \\] t値が大きいほど、推定値に対して相対的に標準誤差が小さいことを意味します。目安として、t値が1.9や1.6など絶対値で2に近い値より大きければ、推定値の標準誤差は未知パラメータの符号条件を変えるほど大きくなく、未知パラメータの真の値がゼロである可能性は低い（推定値が統計的に有意である）と判断できます。 p値 このt値を確率に置き換えたものがp値です。p値は「帰無仮説が正しいとしたときの確率分布のもとで、t値が観測したデータ（標本）から計算した値より極端な値をとる確率」を示したもので、t値が大きくなるほどp値は小さくなります。 p値は「帰無仮説が正しいとしたときの確率分布のもとで、観測したデータによる統計量の値、もしくはそれより極端な統計量をとる確率」と定義されます（McAlinn（2022）P.50）。p値がある一定の水準より小さければ、観測したデータは帰無仮説下では起こりにくいと判断し、帰無仮説\\(H_0\\)を棄却して対立仮説\\(H_1\\)を採択します。その際の「一定の水準」を有意水準と言い、一般的に5％や1％といった水準が用いられます。 p値の解釈については、いくつか注意点があります。 p値が有意水準より大きく、帰無仮説を棄却できなかった場合、帰無仮説が正しい（未知パラメータの真の値がゼロ）と結論付けることはできません。この場合は「未知パラメータの推定値が統計的に有意であることが確認できず、判断を留保する」ことになります。 p値は、帰無仮説を反証（棄却）するエビデンスの強さではありません。帰無仮説は正しいか正しくないかの二択でしかないため、p値そのものに意味はなく、あくまで事前に決めた有意水準に達したか達していないかの二分法になります。有意水準5％で棄却すると設定したのであれば、p値が4.99％でも0.01％でも関係ないということです。 p値は効果の大きさや結果の重要性を示すものではありません。ある説明変数が被説明変数にとって重要であるかを判断するためには、未知パラメータの推定値が有意かどうかに加えて、未知パラメータの推定値の大きさや、説明変数の変動量を考慮する必要があります。 信頼区間 信頼区間（confidence interval、CI）は、推定結果の精度を幅で示すものです。具体的には、未知パラメータの推定値と標準誤差を用い、ある一定の確率の下で未知パラメータの真の値を含む区間を計算します。その際の「一定の確率」を信頼係数といい、要求される推定の精度に応じ、一般的に95％や99％といった水準が用いられます。 信頼区間の解釈には注意が必要です。例えば、標本から未知パラメータの95％信頼区間が10～20と計算されたとき、これは「パラメータの真の値が95％の確率で10～20の間にある」ことを意味するものではありません。パラメータの真の値は定数であり、動かないのに対し、信頼区間は標本から計算した実現値のうちの一つであり、標本の選び方によりランダムに変わるためです。95％信頼区間の意味は、「同一の母集団から独立な標本が100回得られて、信頼区間を100回計算することができれば、そのうち95回はパラメータの真の値を含む」ということです（末石（2015）P.11）。 6.4 モデルの種類と係数の解釈 ここでは線形回帰モデルの具体的な推定式として、実務で頻繁に用いる水準モデル、対数モデル、変化率モデル、多項式モデル、交互作用モデル、ダミー変数モデルを取り上げ、それぞれの内容と係数の解釈方法を説明します。 6.4.1 水準モデル \\[ Y = \\beta_0 + \\beta_1 X + u \\] 被説明変数\\(Y\\)、説明変数\\(X\\)ともにレベルの場合、係数\\(\\beta_1\\)は、説明変数\\(X\\)が1単位変化すると被説明変数\\(Y\\)が\\(\\beta_1\\)単位変化することを示します。 例えば、\\(Y\\)が実質賃金（円）、\\(X\\)が労働生産性（円）のとき、労働生産性が1円上昇すると実質賃金は\\(\\beta_1\\)円上昇します。 6.4.2 対数モデル 両辺が自然対数の場合 \\[ \\ln Y = \\beta_0 + \\beta_1 \\ln X + u \\] 被説明変数\\(\\ln Y\\)、説明変数\\(\\ln X\\)ともに自然対数の場合、係数\\(\\beta_1\\)は、説明変数\\(X\\)が1％変化したときに被説明変数\\(Y\\)が\\(\\beta_1\\)％変化することを示します。このとき、\\(\\beta_1\\)を弾性値（弾力性）と呼びます。 このように解釈できる理由は、次の通りです。変数\\(X\\)がある小さい量\\(\\Delta x\\)だけ変化したときの\\(\\ln X\\)の変化量を\\(\\Delta \\ln X\\)とすると、対数関数の性質を用いて、 \\[ \\Delta \\ln X = \\ln (X + \\Delta x) - \\ln X = \\ln (1 + \\frac{\\Delta x}{X}) \\approx \\frac{\\Delta x}{X} \\] となります。\\(\\Delta x / X\\)は\\(X\\)の変化率なので、この式から、\\(X\\)が1％変化することと\\(\\ln X\\)が0.01変化することはほぼ同じであることがわかります。すると、「\\(X\\)が1％変化する」＝「\\(\\ln X\\)が\\(0.01\\)変化する」＝「\\(\\ln Y\\)が\\(0.01 \\times \\beta_1\\)変化する」＝「\\(Y\\)が\\(\\beta_1\\)％変化する」となります（西山 他（2019）P.107）。 どちらか一方の辺が自然対数の場合 \\[ \\begin{aligned} \\ln Y &amp;= \\beta_0 + \\beta_1 X + u \\\\ Y &amp;= \\beta_0 + \\beta_1 \\ln X + u \\end{aligned} \\] 被説明変数\\(\\ln Y\\)が自然対数、説明変数\\(X\\)がレベルのとき、係数\\(\\beta_1\\)は、説明変数\\(X\\)が1単位変化したときに被説明変数\\(Y\\)が\\(\\beta_1\\)％変化することを示します。このとき、\\(\\beta_1\\)を偏弾性値と呼びます。 逆に、被説明変数\\(Y\\)がレベル、説明変数\\(\\ln X\\)が自然対数の場合、係数\\(\\beta_1\\)は、説明変数\\(X\\)が1％変化したときに被説明変数\\(Y\\)が\\(\\beta_1/100\\)単位変化することを示します。 6.4.3 変化率モデル \\[ \\begin{aligned} 100 \\times \\frac{\\Delta Y_t}{Y_{t-1}} &amp;= \\beta_0 + \\beta_1 \\times 100 \\times \\frac{\\Delta X_t}{X_{t-1}} + u \\\\ 100 \\times \\Delta \\ln Y_t &amp;= \\beta_0 + \\beta_1 \\times 100 \\times \\Delta \\ln X_t + u \\end{aligned} \\] 時系列データについて、ある変数\\(X_t\\)と、その1期ラグ付き変数\\(X_{t-1}\\)の差\\(X_t - X_{t-1}\\)を階差と呼び、\\(\\Delta X_t = X_t - X_{t-1}\\)と表します。階差\\(\\Delta X_t\\)を1期ラグ付き変数\\(X_{t-1}\\)で割り、100を掛けてパーセント表示にした系列\\(100 \\times \\Delta X_t / X_{t-1}\\)を変化率と呼びます。 また、上記の対数モデルで見た\\(\\Delta \\ln X \\approx \\Delta X / X\\)を用いると、時系列データの変数\\(X_t\\)について自然対数の階差をとった\\(\\Delta \\ln X_t = \\ln X_t - \\ln X_{t-1}\\)に100を掛けた系列は、変化率\\(100 \\times \\Delta X_t / X_{t-1}\\)に近似できます。すなわち、\\(100 \\times \\Delta \\ln X_t \\approx 100 \\times \\Delta X_t / X_{t-1}\\)です。この\\(100 \\times \\Delta \\ln X_t\\)を対数階差変化率と呼びます。 被説明変数と説明変数がともに変化率もしくは対数階差変化率のとき、係数\\(\\beta_1\\)は、説明変数（変化率）が1％ポイント変化したときに、被説明変数（変化率）が\\(\\beta_1\\)％ポイント変化することを示します。なお、このときの係数\\(\\beta_1\\)は、対数モデルの係数の解釈である弾力性とは全く異なるものである点に注意してください。 6.4.4 多項式モデル \\[ Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + u \\] 多項式の推定式では、説明変数\\(X\\)が入る項が複数あるため、\\(\\beta_1\\)、\\(\\beta_2\\)単独では\\(X\\)の変化による\\(Y\\)への影響を測ることができません。\\(X\\)の変化による\\(Y\\)への影響は、右辺を\\(X\\)で微分した結果である \\[ \\frac{\\partial(\\beta_0 + \\beta_1 X + \\beta_2 X^2)}{\\partial X} = \\beta_1 + 2 \\beta_2 X \\] で表されます。これを限界効果と呼びます。ただし、限界効果の大きさは\\(X\\)の値に依存するので、一定ではありません。そこで、限界効果の\\(X\\)を\\(X\\)の平均値\\(\\overline{X}\\)で置き換えた平均限界効果\\(\\beta_1 + 2 \\beta_2 \\overline{X}\\)も、\\(X\\)の変化による\\(Y\\)への影響を測る値をして使用されます。 なお、上記の例では、限界効果がゼロ、すなわち\\(\\beta_1 + 2 \\beta_2 X = 0\\)とおいて、\\(X\\)について解くと、\\(X = -\\beta_1 / 2 \\beta_2\\)が\\(Y\\)を最大もしくは最小にする\\(X\\)の値であることが計算できます。 6.4.5 交互作用モデル \\[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 + u \\] 交互作用モデルには、複数の変数の掛け算である交差項（ここでは\\(X_1 X_2\\)）が入っています。これは、説明変数\\(X_1\\)と\\(X_2\\)が互いに影響しながら相乗効果として被説明変数\\(Y\\)に影響するメカニズムを捉えています。 例えば、被説明変数\\(Y\\)が給与額、説明変数\\(X_1\\)と\\(X_2\\)がそれぞれ教育年数と勤続年数の交互作用モデルを推定すると、給与額は教育年数、勤続年数それぞれ単独の影響だけでなく、教育年数と勤続年数の相乗効果の影響も受けることを想定していることになります（山本（2015）P.40）。 上記の交互作用モデルにおける説明変数\\(X_1\\)の限界効果は\\(\\beta_1 + \\beta_3 X_2\\)、平均限界効果は\\(\\beta_1 + \\beta_3 \\overline{X_2}\\)となります。 6.4.6 ダミー変数 切片ダミーの場合 \\[ Y = \\beta_0 + \\beta_1 D + \\beta_2 X + u \\] 切片ダミーは、データの属性を区別するダミー変数\\(D\\)が単独の項として推定式に入っているモデルであり、被説明変数\\(Y\\)の平均的な水準がデータの属性により異なるメカニズムを捉えます。 例えば、被説明変数\\(Y\\)が給与額、説明変数\\(X\\)が勤続年数のモデルで、男性なら1、女性なら0をとる「男性ダミー変数」\\(D\\)を考えます。このとき、男性の給与額は\\(Y = (\\beta_0 + \\beta_1) + \\beta_2 X\\)、女性の給与額は\\(Y = \\beta_0 + \\beta_2 X\\)で表されるため、男性ダミー\\(D\\)の係数\\(\\beta_1\\)は給与額の男性プレミアム（またはディスカウント）を捉えたものと解釈できます。 なお、「男性ダミー変数」\\(D\\)の係数\\(\\beta_1\\)が捉えているのは、比較の基準である女性の給与額との差であり、男性の給与額の絶対的な水準を示すものではありません。また、\\(\\beta_1\\)は比較の基準である女性の給与額に関する情報は一切もっていません。 傾きダミーの場合 \\[ Y = \\beta_0 + \\beta_1 X + \\beta_2 D X+ u \\] 傾きダミーは、ダミー変数\\(D\\)が説明変数\\(X\\)との交差項として推定式に入っているモデルであり、説明変数\\(X\\)の影響が属性により異なるメカニズムを捉えます。 例えば、被説明変数\\(Y\\)が給与額、説明変数\\(X\\)が勤続年数のモデルで、男性なら1、女性なら0をとる「男性ダミー変数」\\(D\\)を考えます。このとき、男性の給与額は\\(Y = \\beta_0 + (\\beta_1 + \\beta_2) X\\)、女性の給与額は\\(Y = \\beta_0 + \\beta_1 X\\)で表されるため、男性ダミー\\(D\\)の係数\\(\\beta_2\\)は給与額に対する勤続年数の効果を増幅（または縮小）させる影響を捉えたものと解釈できます。 なお、「男性ダミー変数」\\(D\\)の係数\\(\\beta_2\\)が捉えているのは、比較の基準である女性の勤続年数効果との差であり、男性の勤続年数効果を示すものではありません。また、\\(\\beta_2\\)は比較の基準である女性の勤続年数効果に関する情報は一切もっていません。 6.5 重回帰モデルの役割と注意点 線形重回帰モデルは、次のように説明変数を2つ以上もつ線形回帰モデルのことです。 \\[ Y_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\dots + \\beta_kX_{ki} + u_i \\quad (i = 1,\\, \\ldots,\\, n) \\] 6.5.1 欠落変数バイアス 重回帰モデルを使用する主な目的は、最小2乗法の仮定3「説明変数と誤差項が無相関」が成立しない原因の一つである欠落変数バイアス（omitted variable bias）に対処するためです。 欠落変数バイアスは、被説明変数\\(Y_i\\)に影響を及ぼす変数を説明変数に含んでいないことによって起こります。こうした変数を交絡因子（confounding factor）と呼びます。 重回帰分析では、交絡因子の可能性がある変数を説明変数に追加し、欠落変数バイアスの問題を回避します。この作業をコントロールと呼び、追加される変数をコントロール変数と言います。コントロール変数は欠落変数バイアスを回避するためにモデルに追加されるため、コントロール変数が被説明変数に与える影響を推定することは主たる目的ではありません。 6.5.2 多重共線性 多重共線性には2つのケースがあります。 一つは完全な多重共線性（perfect multicollinearity）で、ある説明変数が他の説明変数の線形和によって表される場合が該当します。このとき、最小2乗推定量が一意に定まらないため、推定不能になります。 もう一つは不完全な多重共線性で、説明変数間に強い相関がある場合が該当します。一般的に多重共線性というときはこちらを差すことが多いでしょう。多重共線性が発生すると、推定量の分散が大きくなり、係数の正確な推定が困難になります。 多重共線性には有効な解決策はありません。重回帰モデルでは欠落変数バイアスを回避するためにコントロール変数を追加していることが多く、多重共線性に対処するためにコントロール変数を除外すれば、欠落変数バイアスを回避することができなくなるためです。 末石（2015）P.18では、「明らかに不要な説明変数が入っているときは別として、多重共線性をなくそうとあれこれと工夫をするよりも、多くの場合は放置しておくのが無難」、「多重共線性はないに越したことはないが、完全な多重共線性とは異なり、論理的な欠陥ではない」、「推定量の分散は大きくなるが、それは標準誤差に反映されるので、検定をしたり信頼区間を求める際には、特に問題にならない」としています。 6.5.3 変数選択の指針 ここでは、重回帰分析にとって重要な「どの変数を推定式に含めるべきか」について解説します。詳細は西山 他（2019）P.185を参照してください。 1. 欠落変数バイアスを避けられるか 興味がある説明変数が被説明変数に与える影響を正しく推定するためには、その説明変数と相関し、かつ被説明変数に影響を与える変数をコントロール変数として回帰モデルに含める必要があります。コントロール変数について注意すべき点は次の通りです。 コントロール変数は分析対象として興味がなくても良い。 興味のある説明変数と無関係なコントロール変数を入れる必要はない。 コントロール変数に関する欠落変数バイアスを考慮する必要はない。 コントロール変数の係数は一致推定できず、係数の解釈は困難である（符号条件などが直感に反する結果でも気にする必要はない）。 2. 興味のある説明変数の影響の解釈が変わりうるか 重回帰分析では、モデルにどの変数を入れるかによって、係数の解釈が変わりえます。特に、興味ある説明変数が被説明変数に影響を与える経路の中間にある変数（中間経路の変数）をモデルに含めると、係数が本来の研究対象とは異なる解釈を持つ可能性があります。これを過剰制御と呼びます。 例えば、被説明変数を賃金、興味がある説明変数を教育として、教育が賃金に与える影響を分析する場合を考えます。ここで、変数として職業を入れると、職業は教育が賃金に影響を与える経路の中間に位置します。この時、教育の係数は、職業を固定した場合に教育の変化が賃金に与える影響を表すと解釈できますが、教育の変化は職業の変化を通じて賃金にも影響すると想定されるため、教育の係数は教育が賃金に与える影響すべてを捉えることができなくなります。 3. 推定量の分散を小さくできるか 重回帰モデルに変数を追加する場合、興味がある説明変数と相関している変数を追加すると、興味がある説明変数の係数の推定精度が悪化します。一方で、被説明変数に影響を与える変数を追加すると、モデルの推定誤差が改善します。このように、変数の追加にはトレードオフがあります。 1～3のどの基準を優先すべきか 重回帰分析をどのような目的で用いるかによって、変数選択の際に優先すべき基準が変化します。 興味がある説明変数の係数を推定する場合： 欠落変数バイアスの回避（1）と、説明変数の影響の解釈（2）を優先すべきで、推定量の分散（3）は副次的な問題になります。 被説明変数の予測を行う場合： モデルの推定誤差を減少させること（3）を優先すべきで、欠落変数バイアスの回避（1）や、説明変数の影響の解釈（2）は副次的な問題になります。 6.6 Rの単回帰モデル推定 線形単回帰モデル\\(Y = \\beta_0 + \\beta_1 X + u\\)を最小2乗法で推定するには、statsパッケージのlm()関数でformula引数をY ~ Xと設定します。 lm()関数が返すオブジェクトをsummary()関数で出力すると、係数の推定値、標準誤差、t値、p値、信頼区間の下限・上限値、決定係数、F検定の結果が表示されます。なお、lm()関数が計算する標準誤差は均一分散を仮定した標準誤差であり、不均一分散に頑健な標準誤差（HC・ホワイトの標準誤差）や、不均一分散と自己相関に頑健な標準誤差（HAC・ニューウィー＝ウェストの標準誤差）ではないことに注意してください。 ホワイトの標準誤差やニューウィー＝ウェストの標準誤差で係数の仮説検定を行うには、lmtestパッケージのcoeftest()関数と、sandwichパッケージのvcovHC()関数、NeweyWest()関数を使用します。 vcovHC()関数ではtype引数にconstを指定すると通常の均一分散を仮定した標準偏差が、HC0を指定するとホワイトの標準誤差が出力されます。また、HC1を指定するとSTATAのvce(robust)と同じ結果が出力され、西山 他（2019）実証例4.1でもこの標準誤差が記載されています。先行研究では線形回帰のシミュレーションに基づきHC3（vcovHC()関数のデフォルト値）、HC4、HC5の使用が推奨されています。 その他に、estimatrパッケージのlm_robust()関数でもホワイトの標準誤差を用いた仮説検定を行うことができます。lm_robust()関数のse_type引数にHC1を指定するとホワイトの標準誤差が計算されます。stats::lm()関数とestimatr::lm_robust()関数の違いについてはTokyoRの森下光之助氏のスライドを、estimatr::lm_robust()関数の数学的な説明は公式ウェブサイトを参照してください。 6.6.1 実例：線形単回帰 西山 他（2019）P.128～129に掲載されている「実証例4.1 労働生産性と実質賃金の関係」のデータセットを用い、実質賃金（wage）を被説明変数、労働生産性（productivity）を説明変数として、線形単回帰モデルを推定します。 このデータセットは、縦方向（行）が時系列、横方向（列）が変数の時系列データ（time series data）であり、時系列の頻度は年次、期間は1994〜2014年です。 なお、標準誤差は不均一分散に対して頑健な標準誤差（ホワイトの標準誤差）を使用します。 推定結果は、 \\[ \\text{実質賃金の推定量} = 276.13 + 0.55\\,\\text{労働生産性} \\] を意味しており、定数項と労働生産性の係数はどちらもp値が0.05より小さく、有意水準5％で帰無仮説が棄却されます。この結果から、統計的には労働生産性が100円上昇すると賃金が55円上がる傾向があると解釈できます。 # CSVデータを読み込み data &lt;- readr::read_csv(file = &quot;data_nishiyama/ch04/ch04_wage.csv&quot;, col_names = TRUE, col_types = &quot;ddd&quot;, skip = 0 ) # GGally::ggpairs()関数でペアプロットを出力 data %&gt;% dplyr::select(productivity, wage) %&gt;% GGally::ggpairs() # stats::lm()関数でwageをproductivityに回帰 lm_model &lt;- stats::lm(formula = wage ~ productivity, data = data ) # 回帰した結果を出力 summary(lm_model) ## ## Call: ## stats::lm(formula = wage ~ productivity, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -47.618 -17.612 4.186 21.946 37.052 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 276.12961 87.61057 3.152 0.00525 ** ## productivity 0.54682 0.02442 22.395 4.04e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 25.77 on 19 degrees of freedom ## Multiple R-squared: 0.9635, Adjusted R-squared: 0.9616 ## F-statistic: 501.5 on 1 and 19 DF, p-value: 4.037e-15 # ホワイトの標準誤差で係数の仮説検定を実施 lmtest::coeftest(lm_model, vcov. = sandwich::vcovHC(lm_model, type = &quot;HC1&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 276.129607 71.255590 3.8752 0.001019 ** ## productivity 0.546820 0.020463 26.7223 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # 回帰した結果をデータフレーム形式で出力する場合 estimatr::tidy(lm_model) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 276. 87.6 3.15 5.25e- 3 ## 2 productivity 0.547 0.0244 22.4 4.04e-15 6.6.2 実例：回帰診断プロット 西山 他（2019）の実証例4.1で推定した線形単回帰モデルの結果を視覚的に検証するには、lm()関数で出力したモデルオブジェクトに対しplot()関数を適用し、回帰診断プロットを表示します。 回帰診断プロットでは推定した残差の動きを確認し、外れ値の有無やモデルの妥当性を判断します。 # 作図環境設定関数で2行2列の図表配置を指定 par(mfrow = c(2, 2)) # lm()関数で推定した結果をplot()関数で可視化 lm(formula = wage ~ productivity, data = data ) %&gt;% plot() Residual vs Fitted：横軸に予測値、縦軸に残差をとった散布図です。赤色の実線は回帰曲線を示しています。図中の数字はデータの行番号であり、数字が表示されている点は残差の絶対値が大きいことを意味します。この図は、残差の全体像を把握し、残差の独立性や系列相関の有無を検証するために用います。特に、予測値に対する点の分布が一様でなく、曲線的な傾向になっている場合は、説明変数と被説明変数の関係が線形ではない可能性が疑われます。 Normal Q-Q：横軸に標準正規分布の累積分布関数の分位点、縦軸に標準化した残差を大きさの順に並べたものの分位点をとったQQ（Quantile-Quantile）プロットです。残差が正規分布に従っているかどうかを検証します。データが直線上にある場合は、残差が正規分布に従っていることを意味します。 Scale-Location：横軸に予測値、縦軸に残差の絶対値の平方根をとった散布図です。予測値に対する点の分布が一様でなく、増加または減少する傾向がある場合は、誤差項の均一分散の仮定が成立していないと考えられます。 Residuals vs Leverage：横軸に梃子（てこ）値、縦軸に標準化した残差をとった散布図です。個々のデータが回帰係数の推定値へ与える影響度を判断します。赤色の点線はクック距離（Cook’s distance）と呼ばれ、特定のデータをモデルに含む場合と含まない場合で予測値がどれだけ変化するかを示します。クック距離が大きいと（一般的には0.5を超える場合）、そのデータが外れ値である可能性が高いと考えられます。 6.6.3 実例：対数・変化率モデルの推定 西山 他（2019）の実証例4.1のデータセットを利用して、両辺が自然対数のモデル、両辺が対数階差変化率のモデル、両辺が変化率のモデルを推定します。 自然対数はlog()関数で計算します。対数階差変化率は対数変換した値にtidyquant::CHANGE()関数を適用して対数階差を計算します。変化率はtidyquant::PCT_CHANGE()関数で求めることができます。 # 自然対数、対数階差変化率、変化率を計算 data %&lt;&gt;% dplyr::mutate(ln_wage = log(wage), ln_productivity = log(productivity), dlog_wage = 100 * tidyquant::CHANGE(ln_wage), dlog_productivity = 100 * tidyquant::CHANGE(ln_productivity), pch_wage = 100 * tidyquant::PCT_CHANGE(wage), pch_productivity = 100 * tidyquant::PCT_CHANGE(productivity) ) 対数モデルにおける労働生産性の係数0.874は、労働生産性が1％変化すると実質賃金が0.874％変化することを示しています（弾性値）。また、同係数のp値は0.05を下回っており、統計的に有意であるといえます。 # 対数モデル lm_model &lt;- stats::lm(formula = ln_wage ~ ln_productivity, data = data ) summary(lm_model) ## ## Call: ## stats::lm(formula = ln_wage ~ ln_productivity, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.021358 -0.007777 0.001910 0.009589 0.016859 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.55758 0.31276 1.783 0.0906 . ## ln_productivity 0.87424 0.03823 22.869 2.75e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.01138 on 19 degrees of freedom ## Multiple R-squared: 0.9649, Adjusted R-squared: 0.9631 ## F-statistic: 523 on 1 and 19 DF, p-value: 2.748e-15 lmtest::coeftest(lm_model, vcov. = sandwich::vcovHC(lm_model, type = &quot;HC1&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.55758 0.26022 2.1427 0.0453 * ## ln_productivity 0.87424 0.03191 27.3970 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 対数階差変化率は変化率の近似値なので、対数階差変化率モデルと変化率モデルはほぼ同じ推定結果になります。労働生産性の係数のp値はどちらも0.05を上回っており、有意ではありません。 # 対数階差変化率モデル lm_model &lt;- stats::lm(formula = dlog_wage ~ dlog_productivity, data = data, ) summary(lm_model) ## ## Call: ## stats::lm(formula = dlog_wage ~ dlog_productivity, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4550 -0.6120 -0.1980 0.7568 1.7000 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.7637 0.2642 2.890 0.00975 ** ## dlog_productivity 0.2251 0.1469 1.532 0.14286 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9659 on 18 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.1154, Adjusted R-squared: 0.06623 ## F-statistic: 2.348 on 1 and 18 DF, p-value: 0.1429 lmtest::coeftest(lm_model, vcov. = sandwich::vcovHC(lm_model, type = &quot;HC1&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.76371 0.29002 2.6333 0.01687 * ## dlog_productivity 0.22506 0.17911 1.2566 0.22497 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # 変化率モデル lm_model &lt;- stats::lm(formula = pch_wage ~ pch_productivity, data = data, ) summary(lm_model) ## ## Call: ## stats::lm(formula = pch_wage ~ pch_productivity, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4699 -0.6224 -0.2023 0.7606 1.7141 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.7725 0.2682 2.881 0.00995 ** ## pch_productivity 0.2225 0.1476 1.507 0.14906 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9774 on 18 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.1121, Adjusted R-squared: 0.06276 ## F-statistic: 2.272 on 1 and 18 DF, p-value: 0.1491 lmtest::coeftest(lm_model, vcov. = sandwich::vcovHC(lm_model, type = &quot;HC1&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.77250 0.29402 2.6274 0.01708 * ## pch_productivity 0.22252 0.18008 1.2357 0.23247 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 6.7 Rの重回帰モデル推定 重回帰モデルも、単回帰モデルと同様にstatsパッケージのlm()関数で推定します。重回帰モデルの場合は、formula引数にY ~ X1 + X2といった形で複数の説明変数を設定します。 6.7.1 実例：線形重回帰 西山 他（2019）の「実証例5.1 信頼と規範が経済成長に与える影響の重回帰分析」（P.151）と「実証例5.4 信頼と規範が経済成長に与える影響の重回帰分析の標準誤差」のデータセットを用い、信頼関係の強さと規範意識が経済成長に与える影響を、教育水準と初期時点での豊かさをコントロールして推定します。 このデータセットは縦方向（行）が都道府県、横方向（列）が変数の横断面データ（cross section data）です。都道府県名はid列の都道府県番号で表されています。 使用する変数は次の通りです。詳細は、西山 他（2019）第5章の9節（P.191）と補論A（P.203）を参照してください。 y80： 1980年時点の人口1人当たりGDP（100万円） y99： 1999年時点の人口1人当たりGDP（100万円） trust80： 1980年時点の信頼関係の強さ（標準化した値） norm80： 1980年時点の規範意識の強さ（標準化した値） education80： 1980年時点の教育水準（標準化した値） did： 人口集中地区に居住する人口の割合 # CSVデータを読み込み data &lt;- readr::read_csv(file = &quot;data_nishiyama/ch05/youdou.csv&quot;, col_names = TRUE, col_types = NULL, skip = 0 ) ## Rows: 47 Columns: 23 ## ── Column specification ──────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (22): id, y80, y90, y99, education80, trust80, trust96, norm80, norm96, ... ## lgl (1): X ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # 西山 他（2019）P.143～144に従い、各都道府県の人口1人当たりGDPの年平均経済成長率を計算 data %&lt;&gt;% dplyr::mutate(ln_y80 = log(y80), ln_y99 = log(y99), gr_80_99 = 100 * (ln_y99 - ln_y80) / 19 ) # GGally::ggpairs()関数でペアプロットを出力 data %&gt;% dplyr::select(gr_80_99, trust80, norm80, education80, ln_y80, ln_y99, did) %&gt;% GGally::ggpairs(progress = FALSE) まず、1980～1999年の年平均経済成長率を、信頼関係の強さと規範意識それぞれで説明する単回帰モデルを推定します。 これらモデルは、信頼関係の強さと規範意識の係数のp値がともに0.05より小さく、係数が有意であることを示しています。 lm_model &lt;- stats::lm(formula = gr_80_99 ~ trust80, data = data, ) summary(lm_model) ## ## Call: ## stats::lm(formula = gr_80_99 ~ trust80, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.68018 -0.29900 -0.00955 0.34100 0.80625 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.13936 0.06002 52.305 &lt; 2e-16 *** ## trust80 0.22468 0.07172 3.133 0.00304 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4112 on 45 degrees of freedom ## Multiple R-squared: 0.179, Adjusted R-squared: 0.1608 ## F-statistic: 9.813 on 1 and 45 DF, p-value: 0.003045 lmtest::coeftest(lm_model, vcov. = sandwich::vcovHC(lm_model, type = &quot;HC1&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.139355 0.060438 51.9434 &lt; 2.2e-16 *** ## trust80 0.224683 0.066402 3.3837 0.001491 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 lm_model &lt;- stats::lm(formula = gr_80_99 ~ norm80, data = data, ) summary(lm_model) ## ## Call: ## stats::lm(formula = gr_80_99 ~ norm80, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.96828 -0.15563 -0.03077 0.23198 0.61507 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.09054 0.04966 62.236 &lt; 2e-16 *** ## norm80 0.55973 0.09109 6.145 1.9e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3346 on 45 degrees of freedom ## Multiple R-squared: 0.4563, Adjusted R-squared: 0.4442 ## F-statistic: 37.76 on 1 and 45 DF, p-value: 1.902e-07 lmtest::coeftest(lm_model, vcov. = sandwich::vcovHC(lm_model, type = &quot;HC1&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.090543 0.048265 64.0330 &lt; 2.2e-16 *** ## norm80 0.559733 0.070578 7.9307 4.348e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 次に、説明変数に、教育水準と、豊かさの代理変数として1980年時点の対数GDP水準を追加した重回帰モデルを推定します。 信頼関係の強さの係数の推定値は、単回帰モデルの0.22から0.02へと大幅に低下していることが確認できます。単回帰モデルでは、教育水準や豊かさが経済成長率に与える影響が、誤って信頼関係の強さの影響として推定されていた可能性が示唆されます。また、信頼関係の強さの係数のp値は0.79と大きく、係数は有意ではありません。 lm_model &lt;- stats::lm(formula = gr_80_99 ~ trust80 + education80 + ln_y80, data = data, ) summary(lm_model) ## ## Call: ## stats::lm(formula = gr_80_99 ~ trust80 + education80 + ln_y80, ## data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.45839 -0.21925 -0.00947 0.13042 1.03231 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.04885 0.48340 12.513 6.31e-16 *** ## trust80 0.02058 0.07066 0.291 0.772 ## education80 2.61208 2.25858 1.157 0.254 ## ln_y80 -2.38309 0.46001 -5.180 5.59e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3073 on 43 degrees of freedom ## Multiple R-squared: 0.5619, Adjusted R-squared: 0.5313 ## F-statistic: 18.38 on 3 and 43 DF, p-value: 7.983e-08 lmtest::coeftest(lm_model, vcov. = sandwich::vcovHC(lm_model, type = &quot;HC1&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.048849 0.426429 14.1849 &lt; 2.2e-16 *** ## trust80 0.020585 0.075639 0.2721 0.7868 ## education80 2.612075 2.708569 0.9644 0.3403 ## ln_y80 -2.383094 0.491469 -4.8489 1.658e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 一方、規範意識の係数の推定値は重回帰モデルでも0.34と小さくなく、経済学的に意味のある結果（規範意識は経済成長率にとって重要な要素）であると考えられます。また係数のp値は0.018と、有意水準5％で統計的に有意であるといえます。 lm_model &lt;- stats::lm(formula = gr_80_99 ~ norm80 + education80 + ln_y80, data = data, ) summary(lm_model) ## ## Call: ## stats::lm(formula = gr_80_99 ~ norm80 + education80 + ln_y80, ## data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.49006 -0.20688 -0.00209 0.14005 0.69585 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.2909 0.4838 10.935 5.34e-14 *** ## norm80 0.3383 0.1109 3.051 0.0039 ** ## education80 4.3872 2.0275 2.164 0.0361 * ## ln_y80 -1.9911 0.4340 -4.588 3.85e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2789 on 43 degrees of freedom ## Multiple R-squared: 0.6391, Adjusted R-squared: 0.614 ## F-statistic: 25.39 on 3 and 43 DF, p-value: 1.31e-09 lmtest::coeftest(lm_model, vcov. = sandwich::vcovHC(lm_model, type = &quot;HC1&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.29087 0.66824 7.9176 6.204e-10 *** ## norm80 0.33827 0.13698 2.4695 0.017578 * ## education80 4.38724 1.96109 2.2371 0.030513 * ## ln_y80 -1.99114 0.57457 -3.4654 0.001213 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 6.7.2 実例：多項式モデル 西山 他（2019）「実証例5.5 信頼と規範が経済成長に与える影響の多項式モデル」（P.170）を参考に、1980～1999年の年平均経済成長率を1980年時点のGDP水準（対数をとらない値）の2次多項式で説明するモデルを推定します。 2乗の項は、あらかじめdplyr::mutate()関数などを使用して計算しておくか、もしくはI(変数 ^ 2)としてlm()関数の回帰式に含めます。 推定結果は、1980年時点GDPの項、その2乗項ともに係数のp値が0.05より大きく、有意ではありません。ただし、それだけでは1980年時点GDPが経済成長率に影響を与えるかどうかは判断できません。判断するためには、1980年時点GDPの項と2乗項の係数がどちらも同時にゼロであるという結合仮説をF検定する必要があります。 推定結果においてF検定のp値をみると0.05より小さく、「1980年時点GDPの項と2乗項の係数がどちらも同時にゼロ（＝1980年時点GDPの影響はない）」という帰無仮説が5％有意水準で棄却されます。 lm_model &lt;- stats::lm(formula = gr_80_99 ~ y80 + I(y80 ^ 2), data = data, ) summary(lm_model) ## ## Call: ## stats::lm(formula = gr_80_99 ~ y80 + I(y80^2), data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.46772 -0.24882 -0.02486 0.15161 0.96513 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.51866 1.46508 4.449 5.8e-05 *** ## y80 -1.22615 0.73163 -1.676 0.101 ## I(y80^2) 0.08935 0.08953 0.998 0.324 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3077 on 44 degrees of freedom ## Multiple R-squared: 0.5503, Adjusted R-squared: 0.5299 ## F-statistic: 26.93 on 2 and 44 DF, p-value: 2.308e-08 lmtest::coeftest(lm_model, vcov. = sandwich::vcovHC(lm_model, type = &quot;HC1&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.518664 1.385378 4.7053 2.535e-05 *** ## y80 -1.226154 0.707909 -1.7321 0.09027 . ## I(y80^2) 0.089351 0.088612 1.0083 0.31880 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 なお、推定結果から限界効果は、 \\[ -1.226 + 2 \\times 0.089 \\times \\text{1980年時点GDP} \\] と計算できます。平均限界効果は、1980年時点GDPの平均値が3.876なので、 \\[ -1.226 + 2 \\times 0.089 \\times 3.876 \\approx -0.536 \\] となります。 # 平均限界効果の計算 lm_model$coefficients[&quot;y80&quot;] + 2 * lm_model$coefficients[&quot;I(y80^2)&quot;] * mean(data$y80) ## y80 ## -0.5335568 # 多項式モデルを推定した回帰曲線をプロット data %&gt;% dplyr::mutate(fitted = lm_model$fitted.values) %&gt;% ggplot() + geom_point(mapping = aes(x = y80, y = gr_80_99)) + geom_line(mapping = aes(x = y80, y = fitted)) 6.7.3 実例：ダミー・交互作用モデル 西山 他（2019）「実証例5.6 都市化の度合いと初期時点GDPの交互作用」（P.173）を参考に、1980～1999年の年平均経済成長率を1980年時点の対数GDP水準で説明するモデルに、切片ダミー変数と傾きダミー変数を追加します。なお、使用するデータセットは実証例5.1、5.4、5.5と同じです。 追加するダミー変数は、人口集中地区に居住する割合を示す変数（did）が0.4より大きいときに1をとる都市化ダミーdummy_1です。 # 都市化ダミー変数を追加 data %&lt;&gt;% dplyr::mutate(dummy_1 = 1 * (did &gt; 0.4)) 傾きダミー変数と1980年時点の対数GDP水準の交差項は、dummy * ln_y80としてlm()関数の回帰式に含めます。 推定結果をみると、切片ダミーの係数は-0.176、傾きダミーの係数は0.064となっていますが、p値はどちらも0.05より大きく有意ではありません。 なお、1980年時点の対数GDP水準の係数-1.911は、1980年時点のGDPが1％増加すると、1980～1999年の年平均経済成長率が1.911％ポイント低下することを意味しています。 lm_model &lt;- stats::lm(formula = gr_80_99 ~ dummy_1 + ln_y80 + dummy_1 * ln_y80, data = data, ) summary(lm_model) ## ## Call: ## stats::lm(formula = gr_80_99 ~ dummy_1 + ln_y80 + dummy_1 * ln_y80, ## data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.51040 -0.21003 -0.02406 0.16516 0.90189 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.74905 0.57763 9.953 9.96e-13 *** ## dummy_1 -0.17551 0.82444 -0.213 0.832421 ## ln_y80 -1.91120 0.45104 -4.237 0.000117 *** ## dummy_1:ln_y80 0.06441 0.61108 0.105 0.916546 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3092 on 43 degrees of freedom ## Multiple R-squared: 0.5564, Adjusted R-squared: 0.5254 ## F-statistic: 17.98 on 3 and 43 DF, p-value: 1.041e-07 lmtest::coeftest(lm_model, vcov. = sandwich::vcovHC(lm_model, type = &quot;HC1&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.74906 0.80190 7.1692 7.325e-09 *** ## dummy_1 -0.17551 0.94977 -0.1848 0.854258 ## ln_y80 -1.91120 0.65454 -2.9199 0.005554 ** ## dummy_1:ln_y80 0.06441 0.74806 0.0861 0.931785 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 6.7.4 実例：ダミー変数同士の交互作用モデル 西山 他（2019）「実証例5.7 都市化の度合いと初期時点GDPのダミー変数同士の交互作用」（P.175）を参考に、1980～1999年の年平均経済成長率を、実証例5.6で作成した都市化ダミー変数dummy_1と、1980年時点の対数GDP水準が1.4より大きい場合に1をとる豊かさダミー変数dummy_2で説明するモデルを推定します。 2つのダミー変数同士の交差項を含む交互作用モデルは、この2つのダミー変数で標本を4つに分け、それぞれの標本平均を計算する場合と同じ結果になります。ここで、 \\[ Y_i = \\beta_0 + \\beta_1D1_i + \\beta_2D2_i + \\beta_3D1_iD2_i + u_i \\] とし、\\(D1_i = 0\\)かつ\\(D2_i = 0\\)である観測値の標本平均を\\(\\overline{Y}_{00}\\)、同様に\\(\\overline{Y}_{01}\\)、\\(\\overline{Y}_{10}\\)、\\(\\overline{Y}_{11}\\)とすると、推計結果を用い、 \\[ \\begin{aligned} \\overline{Y}_{00} &amp;= \\hat{\\beta}_0 = 3.455 \\\\ \\overline{Y}_{10} &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 = 3.222 \\\\ \\overline{Y}_{01} &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_2 = 2.875 \\\\ \\overline{Y}_{11} &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1 + \\hat{\\beta}_2 + \\hat{\\beta}_3 = 2.689 \\end{aligned} \\] と計算できます。この結果は、都市化率が低く（\\(\\mathit{dummy}_1 = 0\\)）、1980年時点のGDPが小さい（\\(\\mathit{dummy}_2 = 0\\)）場合に、平均経済成長率が高い傾向があることを示唆しています。 # 豊かさダミー変数を追加 data %&lt;&gt;% dplyr::mutate(dummy_2 = 1 * (ln_y80 &gt; 1.4)) lm_model &lt;- stats::lm(formula = gr_80_99 ~ dummy_1 + dummy_2 + dummy_1 * dummy_2, data = data, ) summary(lm_model) ## ## Call: ## stats::lm(formula = gr_80_99 ~ dummy_1 + dummy_2 + dummy_1 * ## dummy_2, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.50919 -0.25376 -0.01148 0.24475 0.85353 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.45474 0.07544 45.793 &lt; 2e-16 *** ## dummy_1 -0.23329 0.12459 -1.872 0.067953 . ## dummy_2 -0.58003 0.15400 -3.767 0.000498 *** ## dummy_1:dummy_2 0.04725 0.20827 0.227 0.821596 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3288 on 43 degrees of freedom ## Multiple R-squared: 0.4982, Adjusted R-squared: 0.4631 ## F-statistic: 14.23 on 3 and 43 DF, p-value: 1.401e-06 lmtest::coeftest(lm_model, vcov. = sandwich::vcovHC(lm_model, type = &quot;HC1&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.454739 0.074531 46.3533 &lt; 2.2e-16 *** ## dummy_1 -0.233289 0.112116 -2.0808 0.043445 * ## dummy_2 -0.580030 0.200571 -2.8919 0.005985 ** ## dummy_1:dummy_2 0.047252 0.235254 0.2009 0.841761 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],["時系列データ操作-1.html", "7 時系列データ操作 7.1 第7章の準備 7.2 時系列データとは 7.3 ts形式データ 7.4 ts形式データの変換 7.5 日付型データ関数 7.6 様々な時系列データ系列の作成 7.7 時系列データの頻度変換 7.8 季節調整（X-13） 7.9 季節調整（STL分解） 7.10 トレンド 7.11 構造変化", " 7 時系列データ操作 第7章「時系列データ操作」では、時系列データに特有の操作方法について解説します。時系列データ特有の操作には、変化率やラグなどのデータ変換、日次・月次などの頻度変換、季節調整、トレンドや構造変化の推定などがあります。 7.1 第7章の準備 7.1.1 パッケージのインポート library(forecast) library(mFilter) library(mgcv) library(pbapply) library(seasonal) library(strucchange) library(tidyquant) library(tidyverse) library(zoo) 7.1.2 外部データセットの取得 この章では、外部データセットとして以下のデータを使用します。第1章のコードを使用してあらかじめウェブからデータセットを取得してください。 OWIDのCOVID-19データセット： data_owid 日本の産業別就業者数： data_labor また、この章では、西山 他（2019）のデータセットを使用します。西山 他（2019）のサポートウェブサイトからデータファイルを取得し、各自の実行環境のワーキングディレクトリ直下にdata_nishiyamaフォルダを作成して、その中に格納してください。 7.2 時系列データとは 時系列データは、観察対象の特徴を特定の時間間隔で記録したデータです。観測の時間間隔は観測者によって日次、週次、月次、四半期、年次など任意に設定されます。一般的に時系列データは、観測したデータ本体と、観測時点を示す日付・時間情報がセットになっています。四半期毎に公表されるGDPデータなどの経済指標や、毎日の株価・為替データといった金融指標は、代表的な時系列データです。 Rで時系列データを扱う方法には、主に次の2つがあります。 まず、第3章で使用したtibble形式です。tibble形式は複数の列を含むデータフレームの形状をしており、データそのものを格納する列と、日付型データを格納する列を組み合わせることで、時系列データを扱うことができます。 もう一つはts形式です。ts形式はデータと日付があらかじめセットになった一次元のデータ構造で、季節調整を行うseasonalパッケージなどで使用されます。 7.3 ts形式データ まず、時系列データ特有のデータ構造であるts形式の取り扱い方法について解説します。 ts形式データのサンプルデータセットとして、seasonalパッケージのunempデータセット（月次データ）と、fpp2パッケージのgasolineデータセット（週次データ）を用います。 7.3.1 ts形式データの表示 月次のts形式データは、行方向に年、列方向に月、の行列の形でコンソールに表示されます。 ただし、これはあくまで表示方法の問題であり、ts形式データが行列（2次元）の構造であることを意味するわけではありません。 seasonal::unemp ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1990 7413 7296 6852 6620 6533 6884 7137 7008 7003 6892 7396 7525 ## 1991 8787 9121 8995 8226 8435 8992 8792 8439 8270 8232 8494 8755 ## 1992 10186 10401 9913 9135 9389 10366 10099 9605 9316 8813 9086 9045 ## 1993 10158 10004 9478 8835 8807 9473 9223 8655 8302 8296 8094 7959 ## 1994 9492 9262 8874 8078 7656 8251 8281 7868 7379 7155 6973 6690 ## 1995 8101 7685 7480 7378 7185 7727 7892 7457 7167 6884 7024 6872 ## 1996 8270 7858 7700 7124 7166 7377 7693 6868 6700 6577 6816 6680 ## 1997 7933 7647 7399 6551 6398 7094 6981 6594 6403 5995 5914 5957 ## 1998 7069 6804 6816 5643 5764 6534 6567 6173 6039 5831 5711 5565 ## 1999 6604 6563 6119 5688 5507 6271 6319 5826 5661 5372 5380 5245 ## 2000 6316 6284 6069 5212 5460 5959 6028 5863 5359 5153 5336 5264 ## 2001 6647 6523 6509 6004 5901 6816 6858 7017 6766 7175 7617 7773 ## 2002 9051 8823 8776 8255 7969 8758 8693 8271 7790 7769 8170 8209 ## 2003 9395 9260 9018 8501 8500 9649 9319 8830 8436 8169 8269 7945 ## 2004 9144 8770 8834 7837 7792 8616 8518 7940 7545 7531 7665 7599 ## 2005 8444 8549 7986 7335 7287 7870 7839 7327 7259 6964 7271 6956 ## 2006 7608 7692 7255 6804 6655 7341 7602 7086 6625 6272 6576 6491 ## 2007 7649 7400 6913 6532 6486 7295 7556 7088 6952 6773 6917 7371 ## 2008 8221 7953 8027 7287 8076 8933 9433 9479 9199 9469 10015 10999 ## 2009 13009 13699 13895 13248 13973 15095 15201 14823 14538 14547 14407 14740 ## 2010 16147 15991 15678 14609 14369 14885 15137 14759 14140 13903 14282 13997 ## 2011 14937 14542 14060 13237 13421 14409 14428 14008 13520 13102 12613 12692 ## 2012 13541 13430 12904 11910 12271 13184 13400 12696 11742 11741 11404 11844 ## 2013 13181 12500 11815 11014 11302 12248 12083 11462 10885 10773 10271 9984 ## 2014 10855 10893 10537 9079 9443 9893 10307 9787 8962 8680 8630 8331 ## 2015 9498 9095 8682 7966 8370 8638 8805 8162 7628 7597 7573 7542 ## 2016 8309 8219 8116 7413 7207 8144 8267 7996 7658 7447 7066 一方、週次のts形式データの場合はベクトルの形で出力されます。 fpp2::gasoline %&gt;% head(200) ## Time Series: ## Start = 1991.1 ## End = 1994.91382614648 ## Frequency = 52.1785714285714 ## [1] 6.621 6.433 6.582 7.224 6.875 6.947 7.328 6.777 7.503 6.916 7.045 6.956 ## [13] 6.976 7.185 6.899 7.396 7.287 7.220 7.651 7.720 7.543 7.961 7.327 7.380 ## [25] 7.501 7.605 7.584 7.401 7.505 7.427 6.993 7.402 7.391 7.088 6.949 7.506 ## [37] 7.396 7.421 7.159 7.379 7.369 6.652 6.769 6.828 7.344 7.447 7.186 6.642 ## [49] 6.788 6.783 7.037 7.112 7.039 6.576 7.002 6.770 7.002 6.806 7.221 7.379 ## [61] 7.027 7.744 6.464 7.376 7.899 7.318 7.174 7.086 7.774 7.104 7.129 7.407 ## [73] 7.701 7.572 7.813 7.422 7.792 7.433 7.796 7.700 7.365 7.814 7.326 7.473 ## [85] 7.181 7.286 7.293 7.514 7.204 7.276 7.509 6.923 7.056 7.240 7.138 7.432 ## [97] 7.354 7.400 7.596 7.358 6.852 6.726 7.131 6.603 7.322 6.779 7.325 6.961 ## [109] 7.188 7.100 7.239 7.194 6.964 6.797 7.561 7.392 7.470 7.657 7.649 7.295 ## [121] 7.364 7.018 7.740 7.392 7.804 7.868 7.578 8.113 7.916 7.336 8.033 7.581 ## [133] 8.187 7.629 7.386 7.889 7.478 7.485 7.387 7.276 7.613 7.704 7.788 7.640 ## [145] 7.552 7.992 6.845 7.430 8.074 7.684 7.411 7.567 7.124 7.124 7.157 6.321 ## [157] 6.953 7.487 6.993 7.372 7.286 7.214 7.521 7.481 7.398 7.187 7.720 7.042 ## [169] 7.892 6.824 7.552 7.938 8.028 7.461 7.770 7.650 7.909 7.930 7.562 8.006 ## [181] 7.523 7.581 7.638 8.187 7.836 7.750 8.061 6.955 7.601 7.763 7.733 7.515 ## [193] 7.562 8.143 8.204 7.432 7.377 7.929 7.431 7.593 7.3.2 ts形式データ情報の取得 ts形式データの日付情報を取得するには、frequency()、start()、end()などの関数を用います。 月次データの周期は12であり、開始・終了時点は1～12の整数で表されます。 # 日付の周期（1年当たりのデータ頻度） frequency(seasonal::unemp) ## [1] 12 # データの開始時点 start(seasonal::unemp) ## [1] 1990 1 # データの終了時点 end(seasonal::unemp) ## [1] 2016 11 一方、週次データの周期は約52.18です。これは、うるう年を考慮した1年の平均日数365.25日を7で割った値であり、1年間が平均して52.18週であることを示しています。 # 日付の周期（1年当たりのデータ頻度） frequency(fpp2::gasoline) ## [1] 52.17857 # データの開始時点 start(fpp2::gasoline) ## [1] 1991.1 # データの終了時点 end(fpp2::gasoline) ## [1] 2017.049 7.3.3 ts形式データのフィルタ ts形式データをフィルタするには、window()関数を使用してstart引数とend引数に年を指定します。 window(seasonal::unemp, start = 1999, end = 2000) ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1999 6604 6563 6119 5688 5507 6271 6319 5826 5661 5372 5380 5245 ## 2000 6316 window(fpp2::gasoline, start = 2016, end = 2017) ## Time Series: ## Start = 2016.0144421629 ## End = 2016.99185489391 ## Frequency = 52.1785714285714 ## [1] 8.500 9.079 8.941 8.341 9.122 9.203 9.576 9.121 9.411 9.458 9.503 9.244 ## [13] 9.224 9.633 9.444 9.315 9.502 9.658 9.755 9.516 9.716 9.568 9.762 9.815 ## [25] 9.709 9.755 9.671 9.785 9.797 9.752 9.769 9.762 9.659 9.511 9.595 9.406 ## [37] 9.650 8.880 9.390 9.264 8.798 9.118 9.183 9.213 9.359 9.024 9.080 8.757 ## [49] 8.874 9.269 9.278 8.465 7.3.4 ts形式データのプロット ts形式データをプロットするには、plot()関数を使用します。ts形式データはデータと日付がセットになっているため、X軸には自動的に日付が表示されます。 plot(seasonal::unemp) plot(fpp2::gasoline) 7.4 ts形式データの変換 data.frame形式やtibble形式といった通常のデータフレームからts形式へ変換するには、ts()関数でts形式データの行列に変換する方法と、tsibbleパッケージのas_tsibble()関数でts形式データフレームであるtsibble形式データに変換する方法の2種類があります。 7.4.1 ts形式に変換：ts() ts()関数を使用して、通常のデータフレーム形式のデータを、ts形式の行列に変換します。 まず、縦型データの場合は一般的に日付列の要素が重複していますので、日付列の要素に重複が無くなるよう、行をフィルタするか、横型データに変換します。次に、dplyr::arrange()関数を用いてデータを日付列で昇順に並べ替えます。最後に、ts()関数でfrequency引数に周期を、start引数にデータ開始時点を指定します。 年次データ 年次データをts()関数でts形式に変換します。 # 日付列の要素が重複している場合は、重複がなくなるように行をフィルタ data_ts &lt;- data_gdp_pref %&gt;% dplyr::filter(pref_name == &quot;東京都&quot;) # 日付列で昇順に並べ替え data_ts %&lt;&gt;% dplyr::arrange(year) # ts()関数でts形式に変換 data_ts %&lt;&gt;% ts(frequency = 1, # 周期：年次データのため周期は1 start = 2006 # データ開始時点は2006年 ) data_ts ## Time Series: ## Start = 2006 ## End = 2018 ## Frequency = 1 ## pref_code pref_name year gdp_nominal gdp_nominal_pchg ## 2006 13 1 2006 104897928 NA ## 2007 13 1 2007 105331451 0.4132808 ## 2008 13 1 2008 103814636 -1.4400400 ## 2009 13 1 2009 97556925 -6.0277734 ## 2010 13 1 2010 97911461 0.3634145 ## 2011 13 1 2011 100277084 2.4160839 ## 2012 13 1 2012 99824147 -0.4516855 ## 2013 13 1 2013 101234576 1.4129137 ## 2014 13 1 2014 101769228 0.5281318 ## 2015 13 1 2015 104494963 2.6783489 ## 2016 13 1 2016 105126848 0.6047038 ## 2017 13 1 2017 105964694 0.7969858 ## 2018 13 1 2018 107041763 1.0164414 frequency(data_ts) ## [1] 1 ts()関数で変換したデータはts形式の行列になるため、$演算子で列を選択することができません。列を選択するには、行列[, 要素の列インデックス]もしくは、行列[, \"列名\"]とします。 data_ts[, &quot;gdp_nominal&quot;] ## Time Series: ## Start = 2006 ## End = 2018 ## Frequency = 1 ## [1] 104897928 105331451 103814636 97556925 97911461 100277084 99824147 ## [8] 101234576 101769228 104494963 105126848 105964694 107041763 as.list()関数でリスト形式に変換すると、データフレーム形式と同様に$演算子で列を選択することができます。リスト[[列インデックス]]でも各列にアクセス可能です。 data_ts %&lt;&gt;% as.list() data_ts$gdp_nominal ## Time Series: ## Start = 2006 ## End = 2018 ## Frequency = 1 ## [1] 104897928 105331451 103814636 97556925 97911461 100277084 99824147 ## [8] 101234576 101769228 104494963 105126848 105964694 107041763 四半期データ 四半期データをts()関数でts形式に変換します。 # 日付列で昇順に並べ替え data_ts &lt;- data_gdp_jp %&gt;% dplyr::arrange(日付) %&gt;% dplyr::select(-日付) # ts()関数でts形式に変換 data_ts %&lt;&gt;% ts(frequency = 4, # 周期：四半期データのため周期は4 start = c(1991, 1) # データ開始時点は1991年1～3月期のため、c(1991, 1)と指定 ) data_ts %&gt;% head() ## 国内総生産 民間最終消費支出 民間住宅 民間企業設備 民間在庫変動 ## 1991 Q1 446307.6 247534.8 29812.0 66259.2 4229.0 ## 1991 Q2 443743.7 248757.7 31118.5 66065.0 -2884.5 ## 1991 Q3 448945.0 250618.8 33628.1 65999.2 -280.5 ## 1991 Q4 447169.1 250780.0 32218.2 67028.9 -1601.9 ## 1992 Q1 452113.0 252928.4 31121.5 68647.7 2253.7 ## 1992 Q2 456276.2 254883.7 30381.5 71088.0 1116.7 ## 政府最終消費支出 公的固定資本形成 公的在庫変動 純輸出 開差 ## 1991 Q1 71357.3 47106.5 -542.3 -7720.2 -11728.6 ## 1991 Q2 72243.7 48020.5 622.8 -8364.3 -11835.8 ## 1991 Q3 72789.0 46302.7 806.2 -9067.4 -11851.0 ## 1991 Q4 72949.9 45588.7 853.3 -9379.0 -11269.1 ## 1992 Q1 74470.4 43641.5 -27.7 -10500.2 -10422.3 ## 1992 Q2 74611.2 45422.2 199.2 -11408.7 -10017.6 frequency(data_ts) ## [1] 4 月次データ 月次データをts()関数でts形式に変換します。 # 日付列で昇順に並べ替え data_ts &lt;- data_labor %&gt;% dplyr::arrange(date) %&gt;% dplyr::select(-date) # ts()関数でts形式に変換 data_ts %&lt;&gt;% ts(frequency = 12, # 周期：四半期データのため周期は4 start = c(2002, 1) # データ開始時点は2002年1月のため、c(2002, 1)と指定 ) data_ts %&gt;% head() ## 総数 農林 建設 製造 情報通信 運輸・郵便 卸・小売 金融・保険 ## Jan 2002 6267 224 589 1210 153 331 1131 165 ## Feb 2002 6248 227 611 1201 156 324 1075 164 ## Mar 2002 6297 242 628 1238 156 323 1081 161 ## Apr 2002 6333 278 624 1213 159 315 1121 167 ## May 2002 6356 307 607 1194 157 322 1134 171 ## Jun 2002 6373 311 594 1223 159 330 1115 170 ## 不動産・物品賃貸 学術・専門・技術 宿泊・飲食 生活関連・娯楽 教育 ## Jan 2002 95 208 393 243 284 ## Feb 2002 96 208 395 246 273 ## Mar 2002 99 207 397 246 266 ## Apr 2002 95 200 394 235 267 ## May 2002 96 203 391 233 270 ## Jun 2002 108 202 390 242 280 ## 医療・福祉 複合サービス その他サービス 公務 ## Jan 2002 462 76 373 219 ## Feb 2002 471 76 374 218 ## Mar 2002 467 76 375 218 ## Apr 2002 473 73 383 212 ## May 2002 476 72 375 217 ## Jun 2002 469 77 368 211 frequency(data_ts) ## [1] 12 週次データ 週次データをts()関数でts形式に変換します。 # 日次データのdata_owidから水曜日のデータを抽出して週次データに変換 data_ts &lt;- data_owid %&gt;% dplyr::filter(lubridate::wday(date) == 4) # 日付列の要素が重複している場合は、重複がなくなるように行をフィルタ data_ts %&lt;&gt;% dplyr::filter(location == &quot;Japan&quot;) # 日付列で昇順に並べ替え data_ts %&lt;&gt;% dplyr::arrange(date) # ts()関数でts形式に変換 data_ts %&lt;&gt;% ts(frequency = 365.25 / 7, # 周期：週次データのため周期は365.25 / 7 start = c(lubridate::year(min(data_ts$date)), lubridate::week(min(data_ts$date))) # データ開始時点の年と週番号を指定 ) data_ts %&gt;% head() ## Time Series: ## Start = 2020.01916495551 ## End = 2020.11498973306 ## Frequency = 52.1785714285714 ## continent location date total_cases new_cases new_cases_smoothed ## 2020.019 1 1 18269 NA 0 0.000 ## 2020.038 1 1 18276 1 0 0.143 ## 2020.057 1 1 18283 1 0 0.000 ## 2020.077 1 1 18290 7 3 0.857 ## 2020.096 1 1 18297 23 3 2.286 ## 2020.115 1 1 18304 28 2 0.714 ## total_deaths new_deaths total_cases_per_million new_cases_per_million ## 2020.019 NA 0 NA 0.000 ## 2020.038 NA 0 0.008 0.000 ## 2020.057 NA 0 0.008 0.000 ## 2020.077 NA 0 0.056 0.024 ## 2020.096 NA 0 0.186 0.024 ## 2020.115 NA 0 0.226 0.016 ## total_deaths_per_million new_deaths_per_million ## 2020.019 NA 0 ## 2020.038 NA 0 ## 2020.057 NA 0 ## 2020.077 NA 0 ## 2020.096 NA 0 ## 2020.115 NA 0 ## people_fully_vaccinated stringency_index ## 2020.019 NA 2.78 ## 2020.038 NA 2.78 ## 2020.057 NA 2.78 ## 2020.077 NA 2.78 ## 2020.096 NA 19.44 ## 2020.115 NA 19.44 frequency(data_ts) ## [1] 52.17857 日次データ 日次データをts()関数でts形式に変換します。 # 日付列の要素が重複している場合は、重複がなくなるように行をフィルタ data_ts &lt;- data_owid %&gt;% dplyr::filter(location == &quot;Japan&quot;) # 日付列で昇順に並べ替え data_ts %&lt;&gt;% dplyr::arrange(date) # ts()関数でts形式に変換 data_ts %&lt;&gt;% ts(frequency = 365.25, # 周期：日次データのため周期は365.25 start = c(lubridate::year(min(data_ts$date)), as.numeric(min(data_ts$date) - as.Date(&quot;2020-01-01&quot;))) # データ開始時点の年と、年初からの日数を指定 ) data_ts %&gt;% head() ## Time Series: ## Start = 2020.00273785079 ## End = 2020.01642710472 ## Frequency = 365.25 ## continent location date total_cases new_cases new_cases_smoothed ## 2020.003 1 1 18264 NA 0 NA ## 2020.005 1 1 18265 NA 0 NA ## 2020.008 1 1 18266 NA 0 NA ## 2020.011 1 1 18267 NA 0 NA ## 2020.014 1 1 18268 NA 0 NA ## 2020.016 1 1 18269 NA 0 0 ## total_deaths new_deaths total_cases_per_million new_cases_per_million ## 2020.003 NA 0 NA 0 ## 2020.005 NA 0 NA 0 ## 2020.008 NA 0 NA 0 ## 2020.011 NA 0 NA 0 ## 2020.014 NA 0 NA 0 ## 2020.016 NA 0 NA 0 ## total_deaths_per_million new_deaths_per_million ## 2020.003 NA 0 ## 2020.005 NA 0 ## 2020.008 NA 0 ## 2020.011 NA 0 ## 2020.014 NA 0 ## 2020.016 NA 0 ## people_fully_vaccinated stringency_index ## 2020.003 NA 0.00 ## 2020.005 NA 0.00 ## 2020.008 NA 0.00 ## 2020.011 NA 0.00 ## 2020.014 NA 2.78 ## 2020.016 NA 2.78 frequency(data_ts) ## [1] 365.25 7.4.2 ts形式に変換：as_tsibble() as_tsibble()関数を使用して、通常のデータフレーム形式のデータを、tsibble形式のデータフレームに変換します。tsibble形式とは、ts形式の情報をもったtibbleのようなものです。 as_tsibble()関数のindex引数に日付情報を格納した列を指定します。tsibble形式では日付を格納した列において要素が重複することを許容しないため、縦型データで日付要素が重複している場合は、key引数に日付要素を一意にするための列名を指定します。 as_tsibble()関数では、index引数に指定した列の内容で周期が自動的に判断されます。YYYY-MM-DDの形になっているDate型の列は日次（day）と判断されるため、四半期、月次、週次データは、それぞれyearquarter型、yearmonth型、yearweek型にあらかじめ変換してからas_tsibble()関数を適用する必要があります。なお、YYYYの形は年次データと判断されます。 年次データ 年次データをas_tsibble()関数でtsibble形式に変換します。周期は1年になります。 # 日付列で昇順に並べ替え data_tsibble &lt;- data_gdp_pref %&gt;% dplyr::arrange(year) # key引数にpref_nameを指定してtsibble形式に変換 data_tsibble %&lt;&gt;% tsibble::as_tsibble(index = year, key = pref_name) data_tsibble ## # A tsibble: 611 x 5 [1Y] ## # Key: pref_name [47] ## pref_code pref_name year gdp_nominal gdp_nominal_pchg ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 24 三重県 2006 8391255 NA ## 2 24 三重県 2007 8586062 2.32 ## 3 24 三重県 2008 7858164 -8.48 ## 4 24 三重県 2009 7491413 -4.67 ## 5 24 三重県 2010 7694240 2.71 ## 6 24 三重県 2011 7435912 -3.36 ## 7 24 三重県 2012 7626982 2.57 ## 8 24 三重県 2013 7919655 3.84 ## 9 24 三重県 2014 7671850 -3.13 ## 10 24 三重県 2015 7916818 3.19 ## # … with 601 more rows frequency(data_tsibble) ## [1] 1 四半期データ 四半期データをas_tsibble()関数でtsibble形式に変換します。周期は4四半期になります。 # 日付列で昇順に並べ替え data_tsibble &lt;- data_gdp_jp %&gt;% dplyr::arrange(日付) # 日付列をyearquarter型に変換してからtsibble形式に変換 data_tsibble %&lt;&gt;% dplyr::mutate(日付 = 日付 %&gt;% tsibble::yearquarter()) %&gt;% tsibble::as_tsibble(index = 日付) data_tsibble ## # A tsibble: 113 x 11 [1Q] ## 日付 国内総生産 民間最終…¹ 民間…² 民間企…³ 民間…⁴ 政府最…⁵ 公的…⁶ 公的在…⁷ ## &lt;qtr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1994 Q1 446308. 247535. 29812 66259. 4229 71357. 47106. -542. ## 2 1994 Q2 443744. 248758. 31118. 66065 -2884. 72244. 48020. 623. ## 3 1994 Q3 448945 250619. 33628. 65999. -280. 72789 46303. 806. ## 4 1994 Q4 447169. 250780 32218. 67029. -1602. 72950. 45589. 853. ## 5 1995 Q1 452113 252928. 31122. 68648. 2254. 74470. 43642. -27.7 ## 6 1995 Q2 456276. 254884. 30382. 71088 1117. 74611. 45422. 199. ## 7 1995 Q3 461709. 256356. 29742. 72334. 2106. 75543 48926 173. ## 8 1995 Q4 462860. 258171. 30431. 73525. 1033. 75789 49134. 235. ## 9 1996 Q1 466623. 257412. 31806. 73285 1643 76459 51647. 597. ## 10 1996 Q2 472494. 260510 33278. 75804 2979. 76392. 50920 191. ## # … with 103 more rows, 2 more variables: 純輸出 &lt;dbl&gt;, 開差 &lt;dbl&gt;, and ## # abbreviated variable names ¹​民間最終消費支出, ²​民間住宅, ³​民間企業設備, ## # ⁴​民間在庫変動, ⁵​政府最終消費支出, ⁶​公的固定資本形成, ⁷​公的在庫変動 frequency(data_tsibble) ## [1] 4 月次データ 月次データをas_tsibble()関数でtsibble形式に変換します。周期は12カ月になります。 # 日付列で昇順に並べ替え data_tsibble &lt;- data_labor %&gt;% dplyr::arrange(date) # date列をyearmonth型に変換してからtsibble形式に変換 data_tsibble %&lt;&gt;% dplyr::mutate(date = date %&gt;% tsibble::yearmonth()) %&gt;% tsibble::as_tsibble(index = date) data_tsibble ## # A tsibble: 259 x 18 [1M] ## date 総数 農林 建設 製造 情報通信 運輸・郵…¹ 卸・小…² 金融…³ 不動産…⁴ ## &lt;mth&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2002 Jan 6267 224 589 1210 153 331 1131 165 95 ## 2 2002 Feb 6248 227 611 1201 156 324 1075 164 96 ## 3 2002 Mar 6297 242 628 1238 156 323 1081 161 99 ## 4 2002 Apr 6333 278 624 1213 159 315 1121 167 95 ## 5 2002 May 6356 307 607 1194 157 322 1134 171 96 ## 6 2002 Jun 6373 311 594 1223 159 330 1115 170 108 ## 7 2002 Jul 6374 294 622 1199 160 328 1116 180 110 ## 8 2002 Aug 6371 288 620 1207 164 326 1085 176 103 ## 9 2002 Sep 6353 286 621 1183 171 333 1081 174 100 ## 10 2002 Oct 6355 271 642 1176 160 333 1116 172 102 ## # … with 249 more rows, 8 more variables: `学術・専門・技術` &lt;dbl&gt;, ## # `宿泊・飲食` &lt;dbl&gt;, `生活関連・娯楽` &lt;dbl&gt;, 教育 &lt;dbl&gt;, `医療・福祉` &lt;dbl&gt;, ## # 複合サービス &lt;dbl&gt;, その他サービス &lt;dbl&gt;, 公務 &lt;dbl&gt;, and abbreviated ## # variable names ¹​`運輸・郵便`, ²​`卸・小売`, ³​`金融・保険`, ## # ⁴​`不動産・物品賃貸` frequency(data_tsibble) ## [1] 12 週次データ 週次データをas_tsibble()関数でtsibble形式に変換します。周期は52.18週になります。 # 日次データのdata_owidから水曜日のデータを抽出して週次データに変換 data_tsibble &lt;- data_owid %&gt;% dplyr::filter(lubridate::wday(date) == 4) # 日付列で昇順に並べ替え data_tsibble %&lt;&gt;% dplyr::arrange(date) data_tsibble %&lt;&gt;% dplyr::mutate(date = date %&gt;% tsibble::yearweek()) %&gt;% tsibble::as_tsibble(index = date, key = location) data_tsibble ## # A tsibble: 48,790 x 14 [1W] ## # Key: location [255] ## continent location date total…¹ new_c…² new_c…³ total…⁴ new_d…⁵ total…⁶ ## &lt;chr&gt; &lt;chr&gt; &lt;week&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Asia Afghanist… 2020 W02 NA 0 0 NA 0 NA ## 2 Asia Afghanist… 2020 W03 NA 0 0 NA 0 NA ## 3 Asia Afghanist… 2020 W04 NA 0 0 NA 0 NA ## 4 Asia Afghanist… 2020 W05 NA 0 0 NA 0 NA ## 5 Asia Afghanist… 2020 W06 NA 0 0 NA 0 NA ## 6 Asia Afghanist… 2020 W07 NA 0 0 NA 0 NA ## 7 Asia Afghanist… 2020 W08 NA 0 0 NA 0 NA ## 8 Asia Afghanist… 2020 W09 1 1 0.143 NA 0 0.024 ## 9 Asia Afghanist… 2020 W10 1 0 0 NA 0 0.024 ## 10 Asia Afghanist… 2020 W11 4 0 0.429 NA 0 0.097 ## # … with 48,780 more rows, 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​total_cases, ²​new_cases, ³​new_cases_smoothed, ## # ⁴​total_deaths, ⁵​new_deaths, ⁶​total_cases_per_million frequency(data_tsibble) ## [1] 52.18 日次データ 日次データをas_tsibble()関数でtsibble形式に変換します。周期は7日になります。 # 日付列で昇順に並べ替え data_tsibble &lt;- data_owid %&gt;% dplyr::arrange(date) # tsibble形式に変換 data_tsibble %&lt;&gt;% tsibble::as_tsibble(index = date, key = location) data_tsibble ## # A tsibble: 341,376 x 14 [1D] ## # Key: location [255] ## continent location date total…¹ new_c…² new_c…³ total…⁴ new_d…⁵ total…⁶ ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Asia Afghani… 2020-01-03 NA 0 NA NA 0 NA ## 2 Asia Afghani… 2020-01-04 NA 0 NA NA 0 NA ## 3 Asia Afghani… 2020-01-05 NA 0 NA NA 0 NA ## 4 Asia Afghani… 2020-01-06 NA 0 NA NA 0 NA ## 5 Asia Afghani… 2020-01-07 NA 0 NA NA 0 NA ## 6 Asia Afghani… 2020-01-08 NA 0 0 NA 0 NA ## 7 Asia Afghani… 2020-01-09 NA 0 0 NA 0 NA ## 8 Asia Afghani… 2020-01-10 NA 0 0 NA 0 NA ## 9 Asia Afghani… 2020-01-11 NA 0 0 NA 0 NA ## 10 Asia Afghani… 2020-01-12 NA 0 0 NA 0 NA ## # … with 341,366 more rows, 5 more variables: new_cases_per_million &lt;dbl&gt;, ## # total_deaths_per_million &lt;dbl&gt;, new_deaths_per_million &lt;dbl&gt;, ## # people_fully_vaccinated &lt;dbl&gt;, stringency_index &lt;dbl&gt;, and abbreviated ## # variable names ¹​total_cases, ²​new_cases, ³​new_cases_smoothed, ## # ⁴​total_deaths, ⁵​new_deaths, ⁶​total_cases_per_million frequency(data_tsibble) ## [1] 7 7.5 日付型データ関数 日付型データを扱う際に便利な関数を紹介します。 7.5.1 日付型データの生成 連続した日付型データを生成するには、seq()関数を使用します。 # 開始日と終了日を指定して1年毎の日付データを生成 seq(from = as.Date(&quot;2010-01-01&quot;), to = as.Date(&quot;2020-01-01&quot;), by = &quot;1 year&quot;) ## [1] &quot;2010-01-01&quot; &quot;2011-01-01&quot; &quot;2012-01-01&quot; &quot;2013-01-01&quot; &quot;2014-01-01&quot; ## [6] &quot;2015-01-01&quot; &quot;2016-01-01&quot; &quot;2017-01-01&quot; &quot;2018-01-01&quot; &quot;2019-01-01&quot; ## [11] &quot;2020-01-01&quot; # 開始日と終了日を指定して1四半期毎の日付データを生成 seq(from = as.Date(&quot;2010-01-01&quot;), to = as.Date(&quot;2012-12-31&quot;), by = &quot;1 quarter&quot;) ## [1] &quot;2010-01-01&quot; &quot;2010-04-01&quot; &quot;2010-07-01&quot; &quot;2010-10-01&quot; &quot;2011-01-01&quot; ## [6] &quot;2011-04-01&quot; &quot;2011-07-01&quot; &quot;2011-10-01&quot; &quot;2012-01-01&quot; &quot;2012-04-01&quot; ## [11] &quot;2012-07-01&quot; &quot;2012-10-01&quot; # 開始日と終了日を指定して2カ月毎の日付データを生成 seq(from = as.Date(&quot;2022-01-01&quot;), to = as.Date(&quot;2022-12-31&quot;), by = &quot;2 month&quot;) ## [1] &quot;2022-01-01&quot; &quot;2022-03-01&quot; &quot;2022-05-01&quot; &quot;2022-07-01&quot; &quot;2022-09-01&quot; ## [6] &quot;2022-11-01&quot; # 開始日と終了日を指定して3週間毎の日付データを生成 seq(from = as.Date(&quot;2022-01-01&quot;), to = as.Date(&quot;2022-03-30&quot;), by = &quot;3 week&quot;) ## [1] &quot;2022-01-01&quot; &quot;2022-01-22&quot; &quot;2022-02-12&quot; &quot;2022-03-05&quot; &quot;2022-03-26&quot; # 開始日と終了日を指定して4日毎の日付データを生成 seq(from = as.Date(&quot;2022-01-01&quot;), to = as.Date(&quot;2022-01-31&quot;), by = &quot;4 day&quot;) ## [1] &quot;2022-01-01&quot; &quot;2022-01-05&quot; &quot;2022-01-09&quot; &quot;2022-01-13&quot; &quot;2022-01-17&quot; ## [6] &quot;2022-01-21&quot; &quot;2022-01-25&quot; &quot;2022-01-29&quot; # 開始日とデータ数を指定して1カ月毎の日付データを生成 seq(from = as.Date(&quot;2022-01-01&quot;), by = &quot;1 month&quot;, length.out = 12) ## [1] &quot;2022-01-01&quot; &quot;2022-02-01&quot; &quot;2022-03-01&quot; &quot;2022-04-01&quot; &quot;2022-05-01&quot; ## [6] &quot;2022-06-01&quot; &quot;2022-07-01&quot; &quot;2022-08-01&quot; &quot;2022-09-01&quot; &quot;2022-10-01&quot; ## [11] &quot;2022-11-01&quot; &quot;2022-12-01&quot; # 開始日とデータ数を合わせるベクトルを指定して2日毎の日付データを生成 seq(from = as.Date(&quot;2022-01-01&quot;), by = &quot;1 day&quot;, along.with = letters) ## [1] &quot;2022-01-01&quot; &quot;2022-01-02&quot; &quot;2022-01-03&quot; &quot;2022-01-04&quot; &quot;2022-01-05&quot; ## [6] &quot;2022-01-06&quot; &quot;2022-01-07&quot; &quot;2022-01-08&quot; &quot;2022-01-09&quot; &quot;2022-01-10&quot; ## [11] &quot;2022-01-11&quot; &quot;2022-01-12&quot; &quot;2022-01-13&quot; &quot;2022-01-14&quot; &quot;2022-01-15&quot; ## [16] &quot;2022-01-16&quot; &quot;2022-01-17&quot; &quot;2022-01-18&quot; &quot;2022-01-19&quot; &quot;2022-01-20&quot; ## [21] &quot;2022-01-21&quot; &quot;2022-01-22&quot; &quot;2022-01-23&quot; &quot;2022-01-24&quot; &quot;2022-01-25&quot; ## [26] &quot;2022-01-26&quot; 7.5.2 日付型データの計算 日付型データの年・月・日を足し引きするには、years()、months()、days()関数を使用します。 # 年 as.Date(&quot;2023-01-01&quot;) + years(5) ## [1] &quot;2028-01-01&quot; # 月 as.Date(&quot;2023-01-01&quot;) - months(3) ## [1] &quot;2022-10-01&quot; # 日 as.Date(&quot;2023-01-01&quot;) + days(100) ## [1] &quot;2023-04-11&quot; # 年・月・日 as.Date(&quot;2023-01-01&quot;) + years(1) + months(1) + days(1) ## [1] &quot;2024-02-02&quot; 7.5.3 日付型と文字列型の変換 日付型データを文字列型データに変換するには、strftime()関数を使用します。一方、文字列型データを日付型データに変換するには、strptime()関数や、lubridateパッケージのymd()関数を使用します。 format引数は以下のように指定します。 %Y：4桁の年 %y：下2桁の年 %m：2桁の月（01～12） %d：2桁の日（01～31） %e：1～2桁の日（1～31、1桁の日は前に半角スペースあり） # 日付型データを文字列型データに変換 strftime(as.Date(&quot;2022-01-01&quot;), format = &quot;%Y年%m月%d日&quot;) ## [1] &quot;2022年01月01日&quot; # 日付型データを文字列型データに変換する際、月と日を1桁表示して半角スペースを削除 strftime(as.Date(&quot;2022-01-01&quot;), format = &quot;%m/%e&quot;) %&gt;% str_replace(pattern = &quot;^0&quot;, replacement = &quot;&quot;) %&gt;% str_replace_all(pattern = &quot; &quot;, replacement = &quot;&quot;) ## [1] &quot;1/1&quot; # 文字列型データを日付型データに変換 strptime(&quot;20221231&quot;, format = &quot;%Y%m%d&quot;) ## [1] &quot;2022-12-31 JST&quot; # 年月日の順番で記載されている文字列型データを日付型データに変換 lubridate::ymd(c(&quot;20000101&quot;, &quot;010203&quot;, &quot;020505&quot;)) ## [1] &quot;2000-01-01&quot; &quot;2001-02-03&quot; &quot;2002-05-05&quot; 7.5.4 日付情報の抽出 日付型データから日付情報を抽出するには、lubridateパッケージの関数を用います。 # 年 lubridate::year(as.Date(&quot;2022-01-01&quot;)) ## [1] 2022 # 月 lubridate::month(as.Date(&quot;2022-12-31&quot;)) ## [1] 12 # 日 lubridate::year(as.Date(&quot;2022-01-15&quot;)) ## [1] 2022 # 四半期 lubridate::quarter(lubridate::ymd(c(&quot;2022-01-10&quot;, &quot;2022-05-01&quot;, &quot;2022-09-30&quot;, &quot;2022-11-20&quot;))) ## [1] 1 2 3 4 # 曜日番号（日曜：1～土曜：7） lubridate::wday(lubridate::ymd(c(&quot;2022-01-01&quot;, &quot;2022-01-02&quot;, &quot;2022-01-03&quot;, &quot;2022-01-08&quot;))) ## [1] 7 1 2 7 # 週番号（1月1日始まり） lubridate::week(lubridate::ymd(c(&quot;2022-01-01&quot;, &quot;2022-01-02&quot;, &quot;2022-01-03&quot;, &quot;2022-01-08&quot;))) ## [1] 1 1 1 2 # 週番号（最初の月曜始まり） lubridate::isoweek(lubridate::ymd(c(&quot;2022-01-01&quot;, &quot;2022-01-02&quot;, &quot;2022-01-03&quot;, &quot;2022-01-08&quot;))) ## [1] 52 52 1 1 # 週番号（最初の日曜始まり） lubridate::epiweek(lubridate::ymd(c(&quot;2022-01-01&quot;, &quot;2022-01-02&quot;, &quot;2022-01-03&quot;, &quot;2022-01-08&quot;))) ## [1] 52 1 1 1 7.5.5 四半期ベースの時系列インデックス 日付型データから四半期ベースの時系列インデックスを生成するにはlubridate::quarter()関数を、四半期ベースの時系列インデックスから日付型データを作成するにはlubridate::date()関数とzoo::as.yearqtr()関数を使用します。 # 日付型データから四半期ベースのインデックスを生成 lubridate::quarter(lubridate::ymd(c(&quot;2012-03-26&quot;, &quot;2012-05-04&quot;, &quot;2012-09-23&quot;, &quot;2012-12-31&quot;)), type = &quot;year.quarter&quot;) ## [1] 2012.1 2012.2 2012.3 2012.4 lubridate::quarter(lubridate::ymd(c(&quot;2012-03-26&quot;, &quot;2012-05-04&quot;, &quot;2012-09-23&quot;, &quot;2012-12-31&quot;)), type = &quot;year.quarter&quot;) %&gt;% str_replace(pattern = &quot;\\\\.&quot;, replacement = &quot;Q&quot;) ## [1] &quot;2012Q1&quot; &quot;2012Q2&quot; &quot;2012Q3&quot; &quot;2012Q4&quot; # 四半期ベースのインデックスから日付型データを生成 lubridate::date(zoo::as.yearqtr(c(&quot;2012 Q1&quot;, &quot;2012 Q2&quot;, &quot;2012 Q3&quot;, &quot;2012 Q4&quot;), format = &quot;%Y Q%q&quot;)) ## [1] &quot;2012-01-01&quot; &quot;2012-04-01&quot; &quot;2012-07-01&quot; &quot;2012-10-01&quot; 7.6 様々な時系列データ系列の作成 ここでは、時系列データのラグ・リード系列、変化率、移動平均を計算する方法を解説します。 まず、Our World in Dataのdata_owidデータセットから使用するサンプルデータを作成します。 data_owid_jp &lt;- data_owid %&gt;% dplyr::select(location, date, new_cases, new_deaths) %&gt;% dplyr::filter(location == &quot;Japan&quot;, date &gt;= &quot;2022-01-01&quot; ) 7.6.1 ラグ・リード系列の作成 dplyr::lag()関数とdplyr::leag()関数で、既存の列のラグ・リード系列を作成します。 # 1期ラグの系列を追加 data_owid_jp %&gt;% dplyr::mutate(new_cases_lag = dplyr::lag(new_cases, n = 1)) ## # A tibble: 621 × 5 ## location date new_cases new_deaths new_cases_lag ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Japan 2022-01-01 504 0 NA ## 2 Japan 2022-01-02 516 2 504 ## 3 Japan 2022-01-03 783 1 516 ## 4 Japan 2022-01-04 1256 1 783 ## 5 Japan 2022-01-05 2506 1 1256 ## 6 Japan 2022-01-06 4194 0 2506 ## 7 Japan 2022-01-07 5983 2 4194 ## 8 Japan 2022-01-08 7930 2 5983 ## 9 Japan 2022-01-09 8144 1 7930 ## 10 Japan 2022-01-10 6394 2 8144 ## # … with 611 more rows 7.6.2 変化率系列の作成 dplyr::lag()関数で、既存の列の変化率系列を作成します。 # 前期比変化率（％表示）の系列を追加 data_owid_jp %&gt;% dplyr::mutate(new_cases_chg = 100 * (new_cases / dplyr::lag(new_cases, n = 1) - 1)) ## # A tibble: 621 × 5 ## location date new_cases new_deaths new_cases_chg ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Japan 2022-01-01 504 0 NA ## 2 Japan 2022-01-02 516 2 2.38 ## 3 Japan 2022-01-03 783 1 51.7 ## 4 Japan 2022-01-04 1256 1 60.4 ## 5 Japan 2022-01-05 2506 1 99.5 ## 6 Japan 2022-01-06 4194 0 67.4 ## 7 Japan 2022-01-07 5983 2 42.7 ## 8 Japan 2022-01-08 7930 2 32.5 ## 9 Japan 2022-01-09 8144 1 2.70 ## 10 Japan 2022-01-10 6394 2 -21.5 ## # … with 611 more rows 7.6.3 移動平均系列の作成 zooパッケージのrollmean()関数で、移動平均系列を作成します。 # 後方7日移動平均の系列を追加 data_owid_jp %&gt;% dplyr::mutate(new_cases_7dma = zoo::rollmean(new_cases, # 移動平均を作成するもとの系列名 k = 7, # 移動平均の期間 na.pad = TRUE, # 系列の先端部分で移動平均を計算できない箇所をNAで埋めるか align = &quot;right&quot;) # left：前方移動平均、center：中央移動平均、right：後方移動平均 ) ## # A tibble: 621 × 5 ## location date new_cases new_deaths new_cases_7dma ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Japan 2022-01-01 504 0 NA ## 2 Japan 2022-01-02 516 2 NA ## 3 Japan 2022-01-03 783 1 NA ## 4 Japan 2022-01-04 1256 1 NA ## 5 Japan 2022-01-05 2506 1 NA ## 6 Japan 2022-01-06 4194 0 NA ## 7 Japan 2022-01-07 5983 2 2249. ## 8 Japan 2022-01-08 7930 2 3310. ## 9 Japan 2022-01-09 8144 1 4399. ## 10 Japan 2022-01-10 6394 2 5201 ## # … with 611 more rows 7.7 時系列データの頻度変換 tidyverseと整合性がある金融時系列データ分析用のtidyquantパッケージに含まれるtq_transmute()関数を用いて、時系列データの頻度変換（高頻度データから低頻度データへの変換）を行います。 なお、tidyquantは頻度変換以外にも様々な分析機能があります。詳しくは公式ウェブサイトを参照してください。 まず、Our World in Dataのdata_owidデータセットから、使用するサンプルデータを作成します。tq_transmute()関数に入力する時系列データは、原則として横型データである点に留意してください。 # サンプルデータ（日次） data_owid_cases_wide &lt;- data_owid %&gt;% dplyr::select(location, date, new_cases) %&gt;% dplyr::filter(date &gt;= &quot;2021-01-01&quot;) %&gt;% dplyr::arrange(date) %&gt;% tidyr::pivot_wider(id_cols = &quot;date&quot;, names_from = &quot;location&quot;, values_from = &quot;new_cases&quot;) 7.7.1 日次データを週次データに変換 tidyquant::tq_transmute()関数を使用して日次データを週次データに変換すると、月曜～日曜のデータがFUNに指定した関数で集計され、日曜の日付で記録されます。 data_owid_cases_wide %&gt;% tidyquant::tq_transmute(select = -date, mutate_fun = apply.weekly, FUN = mean, na.rm = TRUE) ## # A tibble: 143 × 256 ## date Afghanistan Africa Albania Algeria Americ…¹ Andorra Angola Angui…² ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2021-01-03 126. 26506. 526. 283. 0 61 58.3 0.667 ## 2 2021-01-10 111. 31139. 522. 251. 0 60 78.3 0 ## 3 2021-01-17 70.7 30384. 608. 243. 0 64.6 87 0 ## 4 2021-01-24 87.3 26347. 574. 251. 0 65.9 86 0 ## 5 2021-01-31 61.1 19323. 814. 250. 0 55.1 59.3 0.286 ## 6 2021-02-07 44.6 15378. 962. 247. 0 45.9 40 0 ## 7 2021-02-14 22.4 12412. 1108. 237. 0 36.7 38.1 0.143 ## 8 2021-02-21 16 11044. 1011. 179. 0 29.9 24.3 0 ## 9 2021-02-28 15.7 10580. 1046. 171. 0 25.3 40.4 0 ## 10 2021-03-07 19 10119 867. 163. 0 24.3 39 0 ## # … with 133 more rows, 247 more variables: `Antigua and Barbuda` &lt;dbl&gt;, ## # Argentina &lt;dbl&gt;, Armenia &lt;dbl&gt;, Aruba &lt;dbl&gt;, Asia &lt;dbl&gt;, Australia &lt;dbl&gt;, ## # Austria &lt;dbl&gt;, Azerbaijan &lt;dbl&gt;, Bahamas &lt;dbl&gt;, Bahrain &lt;dbl&gt;, ## # Bangladesh &lt;dbl&gt;, Barbados &lt;dbl&gt;, Belarus &lt;dbl&gt;, Belgium &lt;dbl&gt;, ## # Belize &lt;dbl&gt;, Benin &lt;dbl&gt;, Bermuda &lt;dbl&gt;, Bhutan &lt;dbl&gt;, Bolivia &lt;dbl&gt;, ## # `Bonaire Sint Eustatius and Saba` &lt;dbl&gt;, `Bosnia and Herzegovina` &lt;dbl&gt;, ## # Botswana &lt;dbl&gt;, Brazil &lt;dbl&gt;, `British Virgin Islands` &lt;dbl&gt;, … なお、日曜～土曜のデータを集計し日曜の日付で記録したい場合は、rollmean()関数を使用して前方7日移動平均を計算し、日曜の値を抽出します。 data_owid_cases_wide %&gt;% dplyr::mutate(across(-date, rollmean, k = 7, na.pad = TRUE, align = &quot;left&quot;)) %&gt;% dplyr::filter(lubridate::wday(date) == 1) ## # A tibble: 142 × 256 ## date Afghanistan Africa Albania Algeria Americ…¹ Andorra Angola Angui…² ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2021-01-03 116. 29316. 484. 251. 0 53.1 71.1 0.143 ## 2 2021-01-10 76.9 31496. 613. 246. 0 65.3 87.6 0 ## 3 2021-01-17 88.7 26499. 560. 249 0 67.1 84.3 0 ## 4 2021-01-24 64.1 20790. 791. 252. 0 60.1 64.9 0.143 ## 5 2021-01-31 46 15983. 934. 249. 0 47.9 43.9 0.143 ## 6 2021-02-07 20.4 12665. 1112. 239. 0 36.4 37.7 0.143 ## 7 2021-02-14 15.3 11295. 1009. 185. 0 31.1 26.3 0 ## 8 2021-02-21 18.1 10772. 1068. 172. 0 25.3 40.1 0 ## 9 2021-02-28 19 10064. 887. 163. 0 25.1 38.1 0 ## 10 2021-03-07 17 9576. 703 151. 0 32.9 34.1 0.143 ## # … with 132 more rows, 247 more variables: `Antigua and Barbuda` &lt;dbl&gt;, ## # Argentina &lt;dbl&gt;, Armenia &lt;dbl&gt;, Aruba &lt;dbl&gt;, Asia &lt;dbl&gt;, Australia &lt;dbl&gt;, ## # Austria &lt;dbl&gt;, Azerbaijan &lt;dbl&gt;, Bahamas &lt;dbl&gt;, Bahrain &lt;dbl&gt;, ## # Bangladesh &lt;dbl&gt;, Barbados &lt;dbl&gt;, Belarus &lt;dbl&gt;, Belgium &lt;dbl&gt;, ## # Belize &lt;dbl&gt;, Benin &lt;dbl&gt;, Bermuda &lt;dbl&gt;, Bhutan &lt;dbl&gt;, Bolivia &lt;dbl&gt;, ## # `Bonaire Sint Eustatius and Saba` &lt;dbl&gt;, `Bosnia and Herzegovina` &lt;dbl&gt;, ## # Botswana &lt;dbl&gt;, Brazil &lt;dbl&gt;, `British Virgin Islands` &lt;dbl&gt;, … 7.7.2 日次データを月次データに変換 tidyquant::tq_transmute()関数を使用して日次データを月次データに変換すると、月初～月末のデータがFUNに指定した関数で集計され、月末の日付で記録されます。 data_owid_cases_wide %&gt;% tidyquant::tq_transmute(select = -date, mutate_fun = apply.monthly, FUN = mean, na.rm = TRUE) ## # A tibble: 33 × 256 ## date Afghanistan Africa Albania Algeria Americ…¹ Andorra Angola Angui…² ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2021-01-31 86.9 26770. 619. 252. 0 61.4 75.8 0.129 ## 2 2021-02-28 24.7 12354. 1031. 208. 0 34.4 35.7 0.0357 ## 3 2021-03-31 23.9 10559. 619. 132. 0 35.3 45.2 0.226 ## 4 2021-04-30 110. 11586. 215. 160. 0 41.8 142. 2.2 ## 5 2021-05-31 390. 9274. 46.4 221. 0 16.0 256. 0.581 ## 6 2021-06-30 1561. 21463. 7.2 350. 0 6.9 144. 0 ## 7 2021-07-31 919. 39375. 15.7 999. 0 25.1 128. 0.161 ## 8 2021-08-31 196. 36218. 382. 819. 0 11.4 151. 3.32 ## 9 2021-09-30 61.1 17490. 798. 254. 0.0333 5.9 308. 5.73 ## 10 2021-10-31 33.4 6606. 502. 102. 0.0968 9.90 251. 16.2 ## # … with 23 more rows, 247 more variables: `Antigua and Barbuda` &lt;dbl&gt;, ## # Argentina &lt;dbl&gt;, Armenia &lt;dbl&gt;, Aruba &lt;dbl&gt;, Asia &lt;dbl&gt;, Australia &lt;dbl&gt;, ## # Austria &lt;dbl&gt;, Azerbaijan &lt;dbl&gt;, Bahamas &lt;dbl&gt;, Bahrain &lt;dbl&gt;, ## # Bangladesh &lt;dbl&gt;, Barbados &lt;dbl&gt;, Belarus &lt;dbl&gt;, Belgium &lt;dbl&gt;, ## # Belize &lt;dbl&gt;, Benin &lt;dbl&gt;, Bermuda &lt;dbl&gt;, Bhutan &lt;dbl&gt;, Bolivia &lt;dbl&gt;, ## # `Bonaire Sint Eustatius and Saba` &lt;dbl&gt;, `Bosnia and Herzegovina` &lt;dbl&gt;, ## # Botswana &lt;dbl&gt;, Brazil &lt;dbl&gt;, `British Virgin Islands` &lt;dbl&gt;, … 7.7.3 日次データを四半期データに変換 tidyquant::tq_transmute()関数を使用して日次データを四半期データに変換すると、期初～期末のデータがFUNに指定した関数で集計され、期末の日付で記録されます。 data_owid_cases_wide %&gt;% tidyquant::tq_transmute(select = -date, mutate_fun = apply.quarterly, FUN = mean, na.rm = TRUE) ## # A tibble: 11 × 256 ## date Afghanistan Africa Albania Algeria Ameri…¹ Andorra Angola Angui…² ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2021-03-31 45.8 1.67e4 747. 197. 0 44.0 52.8 0.133 ## 2 2021-06-30 684. 1.41e4 88.9 244. 0 21.5 181. 0.923 ## 3 2021-09-30 396. 3.12e4 394. 695. 0.0109 14.2 195. 3.04 ## 4 2021-12-31 32.7 1.58e4 436. 161. 0.109 86.0 220. 14.0 ## 5 2022-03-31 218. 2.31e4 718. 529. 42.3 188. 248. 11.4 ## 6 2022-06-30 52.0 6.42e3 62.0 4.52 29.5 41.2 24.0 8.31 ## 7 2022-09-30 181. 3.89e3 563. 50.0 19.0 26.7 20.8 4.45 ## 8 2022-12-31 92.2 1.36e3 19.4 5.90 0.380 16.6 20.3 0.424 ## 9 2023-03-31 32.5 8.12e2 12.1 4.04 0.456 1.99 2.53 0 ## 10 2023-06-30 141. 3.15e2 2.58 2.98 0.0659 0.934 0.670 0 ## 11 2023-09-18 30.5 7.68e1 0 0 0.12 0 0 0 ## # … with 247 more variables: `Antigua and Barbuda` &lt;dbl&gt;, Argentina &lt;dbl&gt;, ## # Armenia &lt;dbl&gt;, Aruba &lt;dbl&gt;, Asia &lt;dbl&gt;, Australia &lt;dbl&gt;, Austria &lt;dbl&gt;, ## # Azerbaijan &lt;dbl&gt;, Bahamas &lt;dbl&gt;, Bahrain &lt;dbl&gt;, Bangladesh &lt;dbl&gt;, ## # Barbados &lt;dbl&gt;, Belarus &lt;dbl&gt;, Belgium &lt;dbl&gt;, Belize &lt;dbl&gt;, Benin &lt;dbl&gt;, ## # Bermuda &lt;dbl&gt;, Bhutan &lt;dbl&gt;, Bolivia &lt;dbl&gt;, ## # `Bonaire Sint Eustatius and Saba` &lt;dbl&gt;, `Bosnia and Herzegovina` &lt;dbl&gt;, ## # Botswana &lt;dbl&gt;, Brazil &lt;dbl&gt;, `British Virgin Islands` &lt;dbl&gt;, … 7.8 季節調整（X-13） ここでは、seasonalパッケージを用いた時系列データへの季節調整方法について解説します。 seasonalパッケージでは、米国商務省センサス局が開発したX-13ARIMA-SEATSを用いて、ts形式の月次データ、四半期データ、半期データに対し季節調整を適用することができます。 seasonalパッケージや、X-13ARIMA-SEATSの詳細については、Sax &amp; Eddelbuettel（2018）や、奥本（2016）を参照してください。 サンプルデータとして、ts形式のデータであるseasonal::unempデータセットを用い、seasonalパッケージの使用方法を確認します。 7.8.1 ts形式データの可視化 季節調整の前に、データを確認します。ts形式のデータはplot()関数でグラフを作成できます。 plot(seasonal::unemp) 7.8.2 X-13ARIMA-SEATSの実施方法 seasonalパッケージでは、seas()関数を使用してts形式のデータにX-13ARIMA-SEATSを適用します。seas()関数は、季節調整の結果を格納したseas型のオブジェクトを返します。 # seas()関数で季節調整を実行 m &lt;- seasonal::seas(x = seasonal::unemp) # 季節調整の結果を出力 summary(m) ## ## Call: ## seasonal::seas(x = seasonal::unemp) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## AR-Nonseasonal-01 0.94360 0.03441 27.43 &lt;2e-16 *** ## MA-Nonseasonal-01 0.82540 0.05654 14.60 &lt;2e-16 *** ## MA-Seasonal-12 0.85071 0.03362 25.30 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## SEATS adj. ARIMA: (1 1 1)(0 1 1) Obs.: 323 Transform: none ## AICc: 4324, BIC: 4339 QS (no seasonality in final): 0 ## Box-Ljung (no autocorr.): 22.04 Shapiro (normality): 0.9946 plot()関数で、原数値と季節調整値のグラフを作成できます。黒色の線が原数値、赤色の線が季節調整値です。外れ値がある場合はグラフ中に外れ値が表示されます。 plot(m) 季節調整値を出力するには、seasonal::final()関数を使用します。 # 季節調整値をunemp_saに格納 unemp_sa &lt;- seasonal::final(m) # unemp_saの折れ線グラフを作成 plot(unemp_sa, type = &quot;l&quot;) monthplot()関数を使用すると、季節変動（Seasonal component）と不規則変動（Seasonal irregular component）を月別に確認することができます。 monthplot(m) 7.8.3 実例：月次データの季節調整 ここでは、日本の産業別就業者数データセットdata_laborに対し、月次の季節調整を適用します。data_laborはtibble形式のデータフレームの中に、日付型の列と、複数の数値型のデータの列を格納したものです。 tibble形式のデータフレームに格納されている数値型データのベクトルをts形式に変換する tibble形式のデータフレームをリスト形式に変換する pblapply()で一括してseas()関数の季節調整を適用する。季節調整エラーはtry()関数で処理する 季節調整エラーを取得する 季節調整値を取得する エラーが生じた系列は原数値を取得する # data_laborから一部を抽出 data_labor_nsa &lt;- data_labor %&gt;% dplyr::select(date, `総数`, `製造`, `情報通信`) # 数値型データのベクトルをts形式に変換 data_labor_ts &lt;- data_labor_nsa %&gt;% dplyr::select(-date) %&gt;% ts(frequency = 12, # 月次データの場合は12を指定 start = c(lubridate::year(data_labor$date[1]), lubridate::month(data_labor$date[1])) # データ開始年月を指定 ) # tibble形式をリスト形式に変換 data_labor_ts %&lt;&gt;% as.list() # pblapply()関数で一括してseas()関数の季節調整を適用 # 季節調整でエラーが発生する可能性があるため、try()関数でエラー処理を行う result &lt;- pblapply(data_labor_ts, function(e) try(suppressMessages(seas(e, transform.function = &quot;auto&quot;)), silent = TRUE) ) # 季節調整エラーを取得 result_iserror &lt;- sapply(result, class) == &quot;try-error&quot; # 季節調整値を取得し、tibble形式データフレームのdata_labor_saに格納 data_labor_sa &lt;- do.call(cbind, lapply(result[!result_iserror], final)) %&gt;% tibble::as_tibble() %&gt;% dplyr::bind_cols(data_labor[, 1], .) # エラーが生じた系列は原数値を取得してdata_labor_saに追加 for (col in which(result_iserror)) { data_labor_sa &lt;- data_labor[, names(result[col])] %&gt;% dplyr::bind_cols(data_labor_sa) } # data_labor_saの列をdata_labor_nsaの列順で並べ替え data_labor_sa %&lt;&gt;% dplyr::select(all_of(names(data_labor_nsa))) 個別系列の季節調整結果を可視化するには、plot()関数を使用します。一部の系列は季節性がないと判断されるため、季節調整が行われていません。 plot(result[[which(names(result) == &quot;総数&quot;)]]) plot(result[[which(names(result) == &quot;製造&quot;)]]) plot(result[[which(names(result) == &quot;情報通信&quot;)]]) 7.9 季節調整（STL分解） 上で紹介したX-13ARIMA-SEATSは、月次データ、四半期データ、半期データに適用できますが、週次データや日次データには適用できません。そこで、週次データや日次データの季節調整を行うためにstatsパッケージのstl()関数によるSTL分解を使用します。 STL分解とは、Seasonal Decomposition of Time Series by Loessの略で、時系列データを季節変動、トレンド変動、不規則変動に分解する手法です。 7.9.1 STL分解の実施方法 STL分解を行うには、ts形式の時系列データに対し、statsパッケージのstl()関数を適用します。stl()関数はSTL分解の結果を格納したstl型のオブジェクトを返します。 # stl()関数でSTL分解を実行 m &lt;- stats::stl(x = seasonal::unemp, s.window = &quot;periodic&quot; ) # STL分解の結果を可視化 plot(m) 季節調整値を出力するには、stl()関数が返すstl型オブジェクトに格納されているtime.seriesにアクセスします。time.seriesには、季節変動（seasonal）、トレンド変動（trend）、不規則変動（remainder）の順番にデータが格納されています。 # 季節調整値（トレンド）をunemp_saに格納 unemp_sa &lt;- m$time.series[, &quot;trend&quot;] # unemp_saの折れ線グラフを作成 plot(unemp_sa, type = &quot;l&quot;) 7.9.2 週次データのSTL分解 STL分解は、seasonal::unempのような月次データだけでなく、より高頻度なデータにも適用できるのが特徴です。 ここでは、週次データであるfpp2::gasolineデータセットに対し、STL分解を適用します。週次データはts()関数におけるfrequency引数が52.18となっています。これは、うるう年を考慮した1年の平均日数365.25日を7で割った値であり、週次データは一周期が52.18週であることを示しています。 # データの確認 head(fpp2::gasoline) ## Time Series: ## Start = 1991.1 ## End = 1991.19582477755 ## Frequency = 52.1785714285714 ## [1] 6.621 6.433 6.582 7.224 6.875 6.947 # データの可視化 plot(fpp2::gasoline) # stl()関数でSTL分解を実行 m &lt;- stats::stl(x = fpp2::gasoline, s.window = &quot;periodic&quot; ) # STL分解の結果を可視化 plot(m) 7.9.3 日次データのSTL分解（単一周期） さらに、STL分解は日次データにも適用可能です。日次データの周期性は通常、週、月、年の3種類あると考えられます。ただし、ts形式では一種類の周期性しか指定できないため、ここでは週を一周期に設定し、ts()関数のfrequency引数に7を指定して、STL分解を行います。 使用するデータは、Our World in Dataのdata_owidデータセットにおける、日本の新規感染者数です。 # data_owidから日本の新規感染者数（日次）を抽出 data_cases_jp &lt;- data_owid %&gt;% dplyr::filter(location == &quot;Japan&quot;) %&gt;% tidyr::drop_na(new_cases) %&gt;% dplyr::pull(new_cases) # 1週＝7日の周期を設定したts形式のデータに変換 data_cases_jp_ts &lt;- data_cases_jp %&gt;% ts(start = c(2020, 1, 22), frequency = 7 ) # STL分解の結果をmに格納 m &lt;- stats::stl(x = data_cases_jp_ts, s.window = &quot;periodic&quot; ) # STL分解の結果を可視化 plot(m) 7.9.4 日次データのSTL分解（複数周期） データに複数の周期性を設定したい場合は、ts形式の拡張版であるmsts形式のデータを用います。msts形式のデータはforecastパッケージのmsts()関数で作成できます。 ここでは、日次データに対し、1週間＝7日と、1年＝365.25日の2種類の周期性を設定しています。 msts形式のデータに対するSTL分解は、stats::stl()関数ではなく、forecastパッケージのmstl()関数を使用します。 # data_owidから日本の新規感染者数（日次）を抽出 data_cases_jp &lt;- data_owid %&gt;% dplyr::filter(location == &quot;Japan&quot;) %&gt;% tidyr::drop_na(new_cases) %&gt;% dplyr::pull(new_cases) # 1週間＝7日と1年＝365.25日の周期を設定したmsts形式のデータに変換 data_cases_jp_msts &lt;- data_cases_jp %&gt;% forecast::msts(seasonal.period = c(7, 365.25), start = c(2020, 1, 22) ) # STL分解の結果をmに格納 m &lt;- forecast::mstl(x = data_cases_jp_ts, s.window = &quot;periodic&quot;) # STL分解の結果を可視化 plot(m) 7.10 トレンド トレンドには、線形トレンドと非線形トレンドがあります（※）。線形トレンドは、時系列インデックス（毎期1ずつ増加する整数の時間変数）を説明変数とする線形単回帰モデルで推定することができます。一方、非線形トレンドは、HPフィルタや一般化加法モデル（GAM）などの手法を用いて推定します。 ※線形トレンドと非線形トレンドは、確定トレンド（deterministic trend）と呼ばれます。確定トレンドは単純な直線または曲線で表されるもので、一般的にイメージされるトレンドはこちらに当たります。そのほかに、確率トレンド（stochastic trend）と呼ばれるトレンドもあり、こちらは方向が定まらないジグザグ線で表されます。この節では、確定トレンドについて解説します。 7.10.1 線形トレンド 線形トレンド（linear trend）では、次のような線形トレンド・モデルを最小二乗法（OLS）で推定します。\\(t\\)は時系列インデックス（毎期1ずつ増加する整数の時間変数）、\\(u_t\\)は誤差項です。 \\[ Y_t = \\mu + \\delta t + u_t \\] 実例として、西山 他（2019）P.547～549に掲載されている「実証例11.1 GDPの線形トレンド・モデル推定」の一部を再現します。推計に用いるデータは第10章「系列相関と時系列モデル」で用いられている日本の実質GDP（年次、1980～2017年）と同じものを使用します。なお、実証例11.1では1955年からのデータが用いられており、時系列インデックスの値など細かな点で違いがありますので、注意してください。 # XLSXデータを読み込み data &lt;- readxl::read_excel(path = &quot;data_nishiyama/ch10/Fig_1_nominalGDP_annual.xlsx&quot;, # ファイルパス（拡張子が必要、URLは不可） sheet = NULL, # シートインデックス／シート名 col_names = c(&quot;日付&quot;, &quot;名目年率&quot;, &quot;実質年率&quot;), # ヘッダー（列名データ）の有無／列名指定 col_types = NULL, # 各列の型の指定（c：文字列型、d：数値型、D：日付型、l：論理値型） skip = 1 # 読み込み時に上からスキップする行数 ) # 時系列インデックスtimeを追加 data %&lt;&gt;% dplyr::mutate(time = seq_along(日付)) # データの内容を確認 data ## # A tibble: 38 × 4 ## 日付 名目年率 実質年率 time ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1980 250636. 260625. 1 ## 2 1981 268831. 271596 2 ## 3 1982 282582 280592. 3 ## 4 1983 295304. 290478. 4 ## 5 1984 313145. 303555. 5 ## 6 1985 333686 319441. 6 ## 7 1986 350345. 330068. 7 ## 8 1987 366339. 345682. 8 ## 9 1988 393641. 369137. 9 ## 10 1989 421469. 387070. 10 ## # … with 28 more rows ここでは、実質GDPの自然対数に100を掛けた値について線形トレンドを推定します。この操作により、時系列インデックス\\(t\\)の係数\\(\\delta\\)の推定値は実質GDPのトレンド成長率（％表示）を示すことになります。詳細は、西山 他（2019）P.532を参照してください。 # 実質年率データを対数変換し100を掛けた系列を計算 data_lm &lt;- data %&gt;% dplyr::mutate(ln_real = 100 * log(実質年率)) # 推定期間として1991年から2017年を指定 data_lm %&lt;&gt;% dplyr::filter(日付 &gt;= 1991, 日付 &lt;= 2017 ) data_lm ## # A tibble: 27 × 5 ## 日付 名目年率 実質年率 time ln_real ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1991 482845. 419883 12 1295. ## 2 1992 495056. 423444. 13 1296. ## 3 1993 495291 421251. 14 1295. ## 4 1994 501538. 425434. 15 1296. ## 5 1995 512542. 437100. 16 1299. ## 6 1996 525807. 450650. 17 1302. ## 7 1997 534142. 455499. 18 1303. ## 8 1998 527877. 450360. 19 1302. ## 9 1999 519652. 449225. 20 1302. ## 10 2000 526706 461712. 21 1304. ## # … with 17 more rows 推定した結果、時系列インデックスの係数推定値のp値が0.05を下回っているため、トレンドがないとの帰無仮説（\\(\\delta = 0\\)）は棄却されます。したがって、1991～2017年の実質GDPには平均成長率約0.85％のトレンドがあると判断できます。 # 線形トレンド・モデルを推定 result &lt;- estimatr::lm_robust(formula = ln_real ~ time, data = data_lm, se_type = &quot;classical&quot;, # 標準誤差の設定（classical, HC0, HC1, HC2, HC3, CR0, CR2） alpha = 0.05 # 有意水準 ) summary(result) ## ## Call: ## estimatr::lm_robust(formula = ln_real ~ time, data = data_lm, ## se_type = &quot;classical&quot;, alpha = 0.05) ## ## Standard error type: classical ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF ## (Intercept) 1285.5341 1.23019 1044.99 1.567e-59 1283.0005 1288.0677 25 ## time 0.8491 0.04698 18.07 7.259e-16 0.7524 0.9459 25 ## ## Multiple R-squared: 0.9289 , Adjusted R-squared: 0.9261 ## F-statistic: 326.7 on 1 and 25 DF, p-value: 7.259e-16 # 実質GDPを対数変換して100を掛けた値と、線形トレンド・モデルの推定結果をプロット data_lm %&gt;% dplyr::mutate(fitted = result$fitted.values) %&gt;% ggplot() + geom_line(mapping = aes(x = 日付, y = ln_real), color = &quot;blue&quot;) + geom_line(mapping = aes(x = 日付, y = fitted), color = &quot;red&quot;) 7.10.2 区分線形トレンド 次のように、期間によって異なる線形トレンドを推定するモデルを区分線形トレンドといいます。 \\[ Y_t = \\begin{cases} {\\mu_1 + \\delta_1 t + u_t \\quad (1990年以前)}\\\\ {\\mu_2 + \\delta_2 t + u_t \\quad (1991年以降)} \\end{cases} \\] こうした区分線形トレンド・モデルは、1990年以前に0、1991年以降に1をとるダミー変数\\(D_t\\)を定義して、次のように書き換えることができます。 \\[ Y_t = \\beta_0 + \\beta_1 D_t + \\beta_2 t + \\beta_3 (D_t \\times t) + u_t \\] これをダミー変数\\(D_t\\)を使用せずに記述すると、次のとおりです。 \\[ Y_t = \\begin{cases} {\\beta_0 + \\beta_2 t + u_t \\quad (1990年以前)}\\\\ {\\beta_0 + \\beta_1 + (\\beta_2 + \\beta_3) t + u_t \\quad (1991年以降)} \\end{cases} \\] 線形トレンド・モデルと同様に、日本の実質GDPデータを用いて区分線形トレンド・モデルを推定します。ダミー変数はdplyrパッケージのif_else()関数を使って作成します。 # XLSXデータを読み込み data &lt;- readxl::read_excel(path = &quot;data_nishiyama/ch10/Fig_1_nominalGDP_annual.xlsx&quot;, # ファイルパス（拡張子が必要、URLは不可） sheet = NULL, # シートインデックス／シート名 col_names = c(&quot;日付&quot;, &quot;名目年率&quot;, &quot;実質年率&quot;), # ヘッダー（列名データ）の有無／列名指定 col_types = NULL, # 各列の型の指定（c：文字列型、d：数値型、D：日付型、l：論理値型） skip = 1 # 読み込み時に上からスキップする行数 ) # 時系列インデックスtimeを追加 data %&lt;&gt;% dplyr::mutate(time = seq_along(日付)) # データの内容を確認 data ## # A tibble: 38 × 4 ## 日付 名目年率 実質年率 time ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1980 250636. 260625. 1 ## 2 1981 268831. 271596 2 ## 3 1982 282582 280592. 3 ## 4 1983 295304. 290478. 4 ## 5 1984 313145. 303555. 5 ## 6 1985 333686 319441. 6 ## 7 1986 350345. 330068. 7 ## 8 1987 366339. 345682. 8 ## 9 1988 393641. 369137. 9 ## 10 1989 421469. 387070. 10 ## # … with 28 more rows # 実質年率データを対数変換し100を掛けた系列を計算 data_lm &lt;- data %&gt;% dplyr::mutate(ln_real = 100 * log(実質年率)) # 1990年以前に0、1991年以降に1をとるダミー変数dummyを作成 data_lm %&lt;&gt;% dplyr::mutate(dummy = if_else(日付 &gt;= 1991, 1, 0)) data_lm ## # A tibble: 38 × 6 ## 日付 名目年率 実質年率 time ln_real dummy ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1980 250636. 260625. 1 1247. 0 ## 2 1981 268831. 271596 2 1251. 0 ## 3 1982 282582 280592. 3 1254. 0 ## 4 1983 295304. 290478. 4 1258. 0 ## 5 1984 313145. 303555. 5 1262. 0 ## 6 1985 333686 319441. 6 1267. 0 ## 7 1986 350345. 330068. 7 1271. 0 ## 8 1987 366339. 345682. 8 1275. 0 ## 9 1988 393641. 369137. 9 1282. 0 ## 10 1989 421469. 387070. 10 1287. 0 ## # … with 28 more rows 推定の結果、時系列インデックスtimeの係数推定値のp値が0.05を下回っており、1990年以前にトレンドがないとの帰無仮説が棄却されます。したがって、1980～1990年の実質GDPには平均成長率約4.44％のトレンドがあると判断できます。 また、傾きダミーdummy:timeの係数推定値のp値が0.05を下回っており、1991年以降のトレンドは1990年以前のトレンドと差がないとの帰無仮説が棄却されます。この結果は、1991～2017年のトレンド成長率が約0.85％（4.444から傾きダミーdummy:timeの係数推定値3.595を減じた値）であることを示していると解釈できます。 # 区分線形トレンド・モデルを推定 result &lt;- estimatr::lm_robust(formula = ln_real ~ dummy + time + dummy * time, data = data_lm, se_type = &quot;classical&quot;, # 標準誤差の設定（classical, HC0, HC1, HC2, HC3, CR0, CR2） alpha = 0.05 # 有意水準 ) summary(result) ## ## Call: ## estimatr::lm_robust(formula = ln_real ~ dummy + time + dummy * ## time, data = data_lm, se_type = &quot;classical&quot;, alpha = 0.05) ## ## Standard error type: classical ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF ## (Intercept) 1241.194 1.1311 1097.33 6.258e-79 1238.896 1243.493 34 ## dummy 44.340 1.6000 27.71 6.404e-25 41.088 47.591 34 ## time 4.444 0.1668 26.65 2.297e-24 4.105 4.783 34 ## dummy:time -3.595 0.1723 -20.86 5.865e-21 -3.945 -3.244 34 ## ## Multiple R-squared: 0.9932 , Adjusted R-squared: 0.9926 ## F-statistic: 1654 on 3 and 34 DF, p-value: &lt; 2.2e-16 # 実質GDPを対数変換して100を掛けた値と、線形トレンド・モデルの推定結果をプロット data_lm %&gt;% dplyr::mutate(fitted_1 = if_else(日付 &lt;= 1990, result$fitted.values, NA_real_), fitted_2 = if_else(日付 &gt;= 1991, result$fitted.values, NA_real_) ) %&gt;% ggplot() + geom_line(mapping = aes(x = 日付, y = ln_real), color = &quot;blue&quot;) + geom_line(mapping = aes(x = 日付, y = fitted_1), color = &quot;red&quot;) + geom_line(mapping = aes(x = 日付, y = fitted_2), color = &quot;red&quot;) 7.10.3 HPフィルタ HPフィルタ（Hodrick Prescott filter）は、非線形トレンド推定手法の一種です。 具体的には、時系列データがトレンド成分\\(g_t\\)と循環成分\\(c_t\\)で構成されると仮定し、次の式のように、全期間を通して「循環成分の2乗の総和」と「トレンド成分の2階階差の2乗の総和」の加重和が最小になるような\\(g_t\\)を計算します（肥後・中田（1998））。 \\[ \\min \\Bigg\\{ \\sum_{t=1}^{T}{c_t^2} + \\lambda \\sum_{t=1}^{T}[(g_t - g_{t-1})-(g_{t-1}-g_{t-2})]^2 \\Bigg\\} \\] ここで、\\(\\lambda\\)はトレードオフの関係にある2つの項にウェイトをつける調整パラメータです。\\(\\lambda\\)が大きいほどトレンド成分が直線に近く、\\(\\lambda\\)が小さいほどトレンド成分が元のデータに近くなります。 一般的にマクロ経済分析では、四半期データに対し\\(\\lambda = 1600\\)が用いられます。これにより、8年周期以下の景気循環は必ず循環成分に含まれるようになり、通常の景気循環の認識に近くなります。また、年次データでは\\(\\lambda = 100\\)、月次データでは\\(\\lambda = 14400\\)が伝統的に用いられています（西山 他（2019）P.536）。 以下では、seasonalパッケージのサンプルデータセットunemp（米国の失業者数、原数値）を対象にして、HPフィルタでトレンド推定を行う方法を解説します。 HPフィルタを適用するには、時系列データに対し、mFilterパッケージのhpfilter()関数を使用します。データ構造は数値型ベクトル、ts形式どちらでもOKです。 推定したトレンド成分、循環成分は、推定結果を格納したオブジェクトに、それぞれtrend、cycleの名称で格納されています。 result &lt;- mFilter::hpfilter(x = seasonal::unemp, # HPフィルタを適用する時系列データ（数値型ベクトル、ts形式） type = &quot;lambda&quot;, freq = 14400, # ラムダの値（四半期：1600、月次：14400） drift = FALSE # ドリフト項の有無 ) summary(result) ## ## Title: ## Hodrick-Prescott Filter ## ## Call: ## mFilter::hpfilter(x = seasonal::unemp, freq = 14400, type = &quot;lambda&quot;, ## drift = FALSE) ## ## Method: ## hpfilter ## ## Filter Type: ## lambda ## ## Series: ## seasonal::unemp ## ## Descriptive Statistics: ## ## seasonal::unemp Trend Cycle ## Min. : 5153 Min. : 5915 Min. :-2235.97 ## 1st Qu.: 7012 1st Qu.: 7236 1st Qu.: -473.54 ## Median : 8078 Median : 8115 Median : -78.87 ## Mean : 8766 Mean : 8766 Mean : 0.00 ## 3rd Qu.: 9476 3rd Qu.: 9235 3rd Qu.: 437.43 ## Max. :16147 Max. :14235 Max. : 2441.18 ## ## In-sample error measures: ## ME MSE MAE MPE MAPE ## 2.935e-14 5.236e+05 5.652e+02 -9.939e-03 6.582e-02 # hpfilter()関数の出力結果から開始日と終了日を確認 start(result$trend) ## [1] 1990 1 end(result$trend) ## [1] 2016 11 # hpfilter()関数の出力結果からtibble形式のデータフレームを作成 data_result &lt;- tibble::tibble(date = seq(from = as.Date(&quot;1990-01-01&quot;), to = as.Date(&quot;2016-11-01&quot;), by = &quot;1 month&quot;), cycle = result$cycle %&gt;% as.vector(), trend = result$trend %&gt;% as.vector(), original = result$x %&gt;% as.vector() ) data_result_long &lt;- data_result %&gt;% tidyr::pivot_longer(cols = -&quot;date&quot;, names_to = &quot;name&quot;, values_to = &quot;value&quot;) # HPフィルタの結果をプロット ggplot(data = data_result_long, mapping = aes(x = date, y = value, color = name)) + geom_line() # トレンド推定結果を可視化（コンソールでEnterを押すと表示される） plot(result) 7.10.4 一般化加法モデル（GAM） 一般化加法モデル（Generalizes Additive Model、GAM）は非線形モデルの一種であり、非線形モデルによる高い説明力と、線形モデルがもつ解釈性の良さを両立させることができる手法です。 具体的には次のように、被説明変数\\(Y\\)を、説明変数\\(X\\)を要素とする非線形関数\\(f(X)\\)の和として説明するモデルです。非線形関数を用いることにより、説明変数と被説明変数の間の複雑な関係を説明できる一方で、被説明変数がそれぞれの非線形関数の和になっていることで、被説明変数の変動要因を説明変数毎に分解することができ、高い解釈性を維持している点が特徴です。 \\[ Y_i = \\beta_0 + f_1(X_{1i}) + f_2(X_{2i}) + \\dots + f_k(X_{ki}) + u_i \\] ここでは\\(X\\)として時系列インデックスを用いることでトレンドを推定しますが、GAMはトレンド推定以外にも様々な用途に使用可能です。例えば、服部直樹（2020）「新型コロナウイルス感染症（COVID-19）の感染拡大要因は何か」では、GAMの拡張版である「交互作用項付き一般化加法モデル（GA2M）」を使用して、新型コロナウイルス感染者数の変動要因の説明を試みています（ただし、同レポートではPythonによるGA2Mの実装を用いています）。 一般化加法モデルは、mgcvパッケージのgam()関数で推定します。事前に時系列インデックスを作成し、gam()関数のformula引数で被説明変数 ~ s(時系列インデックス)と指定します。説明変数である時系列インデックスをs()に配置することにより、非線形関数を適用します（s()を用いなければ、通常の線形回帰と同じ結果になります）。 gam()関数の使用方法の詳細については、こちらのウェブサイトを参照してください。 # unempデータをベクトル形式に変換してdataに格納 data &lt;- tibble::tibble( date = seq(from = as.Date(&quot;1990-01-01&quot;), to = as.Date(&quot;2016-11-01&quot;), by = &quot;1 month&quot;), unemp = seasonal::unemp %&gt;% as.vector() ) # dataに時系列インデックスtimeを追加 data %&lt;&gt;% dplyr::mutate(time = seq_along(date)) # GAMでトレンド推定 result &lt;- mgcv::gam(formula = unemp ~ s(time), family = gaussian(), # 分布関数 sp = NULL, # 平滑度を決めるパラメータ（大きいほど直線に近く、小さいほど元データに近い。NULLで自動最適化） data = data ) # 推定結果を出力 summary(result) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## unemp ~ s(time) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8765.87 44.51 196.9 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(time) 8.974 9 326.7 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.901 Deviance explained = 90.4% ## GCV = 6.6028e+05 Scale est. = 6.399e+05 n = 323 推定したトレンド系列は、推定結果を格納したオブジェクトにfitted.valuesの名称で格納されています。 data %&gt;% dplyr::mutate(unemp_fitted = result$fitted.values) ## # A tibble: 323 × 4 ## date unemp time unemp_fitted ## &lt;date&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1990-01-01 7413 1 7055. ## 2 1990-02-01 7296 2 7139. ## 3 1990-03-01 6852 3 7224. ## 4 1990-04-01 6620 4 7308. ## 5 1990-05-01 6533 5 7392. ## 6 1990-06-01 6884 6 7476. ## 7 1990-07-01 7137 7 7559. ## 8 1990-08-01 7008 8 7642. ## 9 1990-09-01 7003 9 7724. ## 10 1990-10-01 6892 10 7806. ## # … with 313 more rows # トレンド推定結果を可視化 plot(result, # gam()関数で推定した結果を格納したオブジェクト residuals = TRUE, # 元データを表示するか se = TRUE, # 信頼区間を表示するか pch = &quot;*&quot; # 元データのマーカー ) # 推定したモデルのチェック mgcv::gam.check(result) ## ## Method: GCV Optimizer: magic ## Smoothing parameter selection converged after 12 iterations. ## The RMS GCV score gradient at convergence was 2.293679 . ## The Hessian was positive definite. ## Model rank = 10 / 10 ## ## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may ## indicate that k is too low, especially if edf is close to k&#39;. ## ## k&#39; edf k-index p-value ## s(time) 9.00 8.97 0.23 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.11 構造変化 線形トレンド・モデルを応用して構造変化の推定を行うことができます。 バブル崩壊やアジア通貨危機、リーマンショック、コロナ禍など、構造変化したと考えられる時点が既知の場合は、その時点における構造変化の有無を仮説検定します。 一方、構造変化したと考えられる時点が未知の場合は、構造変化が発生した可能性が最も高い時点を推定します。 7.11.1 構造変化点が既知の場合 構造変化した時点が既知の場合は、構造変化点を\\(T^*\\)とした次の重回帰モデル \\[ Y_t = \\begin{cases} {\\beta_0 + \\beta_1 X_{1t} + \\dots + \\beta_k X_{kt} + u_t \\quad (t \\lt T^*)}\\\\ {\\beta^*_0 + \\beta^*_1 X_{1t} + \\dots + \\beta^*_k X_{kt} + u_t \\quad (t \\geq T^*)} \\end{cases} \\] について、構造変化がない帰無仮説 \\[ H_0: \\beta_0 = \\beta^*_0, \\; \\beta_1 = \\beta^*_1, \\; \\ldots \\; \\beta_k = \\beta^*_k \\] を、少なくとも1つの係数に対して等式が成立しない対立仮説に対して検定します。こうした検定は、一般的にチャウ検定（Chow test）と呼ばれています。 構造変化点が既知の場合の構造変化の検定には、strucchangeパッケージのsctest()関数を使用します。ここでは、線形トレンド・モデルの推定で使用した日本の実質GDPデータ（対数変換して100を掛けた値、1980～2017年）に対し、1991年に構造変化が起きたか否かを検定します。 # XLSXデータを読み込み data &lt;- readxl::read_excel(path = &quot;data_nishiyama/ch10/Fig_1_nominalGDP_annual.xlsx&quot;, # ファイルパス（拡張子が必要、URLは不可） sheet = NULL, # シートインデックス／シート名 col_names = c(&quot;日付&quot;, &quot;名目年率&quot;, &quot;実質年率&quot;), # ヘッダー（列名データ）の有無／列名指定 col_types = NULL, # 各列の型の指定（c：文字列型、d：数値型、D：日付型、l：論理値型） skip = 1 # 読み込み時に上からスキップする行数 ) # 時系列インデックスtimeを追加 data %&lt;&gt;% dplyr::mutate(time = seq_along(日付)) # 実質年率データを対数変換し100を掛けた系列を計算 data_lm &lt;- data %&gt;% dplyr::mutate(ln_real = 100 * log(実質年率)) # データの内容を確認 data_lm ## # A tibble: 38 × 5 ## 日付 名目年率 実質年率 time ln_real ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1980 250636. 260625. 1 1247. ## 2 1981 268831. 271596 2 1251. ## 3 1982 282582 280592. 3 1254. ## 4 1983 295304. 290478. 4 1258. ## 5 1984 313145. 303555. 5 1262. ## 6 1985 333686 319441. 6 1267. ## 7 1986 350345. 330068. 7 1271. ## 8 1987 366339. 345682. 8 1275. ## 9 1988 393641. 369137. 9 1282. ## 10 1989 421469. 387070. 10 1287. ## # … with 28 more rows # 実質GDPを対数変換して100を掛けた値をプロット ggplot(data = data_lm, mapping = aes(x = time, y = ln_real) ) + geom_point() stricchange::sctest()関数で1991年（データの行インデックスが12）について構造変化の検定を行うと、p値が0.05を下回るため、構造変化がない帰無仮説を棄却します。したがって、日本の実質GDPは1991年に構造変化が生じていたと解釈できます。 # sctest()関数で構造変化の検定を実施 strucchange::sctest(formula = ln_real ~ time, # 構造変化を検定するモデル data = data_lm, # データ type = &quot;Chow&quot;, # チャウ検定を意味する&quot;Chow&quot;を指定 point = 12 # 検定する構造変化点（データの行インデックス番号）を指定 ) ## ## Chow test ## ## data: ln_real ~ time ## F = 388.35, p-value &lt; 2.2e-16 7.11.2 構造変化点が未知の場合 構造変化した時点が未知の場合は、strucchange::breakpoints()関数を使用し、構造変化した可能性が最も高い時点を推定します。 ここでは、「構造変化が既知の場合」と同じ日本の実質GDPデータ（対数変換して100を掛けた値、1980～2017年）を用い、構造変化点を推定します。 # XLSXデータを読み込み data &lt;- readxl::read_excel(path = &quot;data_nishiyama/ch10/Fig_1_nominalGDP_annual.xlsx&quot;, # ファイルパス（拡張子が必要、URLは不可） sheet = NULL, # シートインデックス／シート名 col_names = c(&quot;日付&quot;, &quot;名目年率&quot;, &quot;実質年率&quot;), # ヘッダー（列名データ）の有無／列名指定 col_types = NULL, # 各列の型の指定（c：文字列型、d：数値型、D：日付型、l：論理値型） skip = 1 # 読み込み時に上からスキップする行数 ) # 時系列インデックスtimeを追加 data %&lt;&gt;% dplyr::mutate(time = seq_along(日付)) # 実質年率データを対数変換し100を掛けた系列を計算 data_lm &lt;- data %&gt;% dplyr::mutate(ln_real = 100 * log(実質年率)) # データの内容を確認 data_lm ## # A tibble: 38 × 5 ## 日付 名目年率 実質年率 time ln_real ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1980 250636. 260625. 1 1247. ## 2 1981 268831. 271596 2 1251. ## 3 1982 282582 280592. 3 1254. ## 4 1983 295304. 290478. 4 1258. ## 5 1984 313145. 303555. 5 1262. ## 6 1985 333686 319441. 6 1267. ## 7 1986 350345. 330068. 7 1271. ## 8 1987 366339. 345682. 8 1275. ## 9 1988 393641. 369137. 9 1282. ## 10 1989 421469. 387070. 10 1287. ## # … with 28 more rows strucchange::breakpoints()関数では、h引数に最小セグメントサイズを、breaks引数に構造変化の数を指定します。 最小セグメントサイズは、構造変化点とデータ端点（もしくは別の構造変化点）の間の区間に含まれるサンプルサイズの最小値を定めるものです。例えば、最小セグメントサイズを0.15、構造変化の数を1に指定すると、データの両端から15％の区間は構造変化点の候補から除外されます。最小セグメントサイズは0.15や0.05といった値が用いされるのが一般的です（西山 他（2019）P.565）。 この例では、構造変化点として12が推定されました。これは、データの行インデックス12、すなわち1991年に構造変化が発生した可能性が最も大きいことを意味しています。 result &lt;- strucchange::breakpoints(formula = ln_real ~ time, # 構造変化点を推定するモデル data = data_lm, # データ h = 0.15, # 最小セグメントサイズ breaks = 1 # 構造変化の数 ) summary(result) ## ## Optimal (m+1)-segment partition: ## ## Call: ## breakpoints.formula(formula = ln_real ~ time, h = 0.15, breaks = 1, ## data = data_lm) ## ## Breakpoints at observation number: ## ## m = 1 12 ## ## Corresponding to breakdates: ## ## m = 1 0.31578947368421 ## ## Fit: ## ## m 0 1 ## RSS 2456.3 103.0 ## BIC 277.2 167.6 data_lm %&gt;% dplyr::slice(12) ## # A tibble: 1 × 5 ## 日付 名目年率 実質年率 time ln_real ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1991 482845. 419883 12 1295. "],["時系列分析-1.html", "8 時系列分析 8.1 第8章の準備 8.2 定常性 8.3 非定常過程 8.4 単位根検定（ADF検定） 8.5 系列相関・自己相関 8.6 ボックス＝ジェンキンス法 8.7 共和分 8.8 VARモデル 8.9 VARモデルに関連する分析 8.10 VARモデルの分析フロー 8.11 VARモデルの実例", " 8 時系列分析 第8章「時系列分析」では、時系列データを対象とする分析手法について解説します。 時系列分析は、観測値と観測時点がセットになった時系列データについて、観測時点間のデータの関係性（データの並び順や前後関係に関する性質）を明らかにしたり、過去のデータの特徴から将来の値を予測したりすることを目的としています。 なお、この章の説明は、エンダース（2019）、北岡 他（2013）、西山 他（2019）、ニールセン（2021）、馬場（2018）、宮尾（2006）、村尾（2019）、横内・青木（2014）を参考にしています。詳細は各参考文献を参照してください。 8.1 第8章の準備 8.1.1 パッケージのインポート library(estimatr) library(forecast) library(fpp) library(ggfortify) library(ggplot2) library(magrittr) library(tidyverse) library(tseries) library(vars) library(urca) 8.1.2 外部データセットの取得 この章では、西山 他（2019）のデータセットを使用します。西山 他（2019）のサポートウェブサイトからデータファイルを取得し、各自の実行環境のワーキングディレクトリ直下にdata_nishiyamaフォルダを作成して、その中に格納してください。 8.2 定常性 時系列分析の目的は時系列データの特徴を明らかにし、将来の予測に役立てることです。そのためには、分析対象である時系列データの特徴が時間を通じて安定している必要があります。時系列データの安定性を担保する前提条件を、定常性（stationarity）といいます。 8.2.1 定常性の定義 本書では、定常性を次のように「平均\\(E(Y_t)\\)、分散\\(\\mathit{Var}(Y_t)\\)、自己共分散\\(\\mathit{Cov}(Y_t, Y_{t-p})\\)が時間\\(t\\)を通じて変わらないこと」と定義します。このような性質をもつ時系列データを定常過程（stationary process）といいます。 \\[ \\begin{aligned} &amp; E(Y_t) = \\mu \\lt \\infty \\\\ &amp; \\mathit{Var}(Y_t) = E[(Y_t - \\mu)^2] = \\sigma^2 \\lt \\infty \\\\ &amp; \\mathit{Cov}(Y_t, Y_{t-s}) = E[(Y_t - \\mu)(Y_{t-s} - \\mu)] = \\gamma_s, \\quad s \\gt 0 \\end{aligned} \\] 特に、原系列に何ら変換を加えない状態で定常である時系列データをレベル定常過程と呼びます。レベル定常過程のデータは、ある水準の周りで安定的に推移し、一定の幅をもって変動します。 なお、定常性にはいくつかの定義があり、本書では「弱定常」や「共分散定常」と呼ばれる定常性の定義を採用しています。定常性の詳細については、ニールセン（2021）P.77、西山 他（2019）P.476、村尾（2019）P.36、横内・青木（2014）P.82を参照してください。 8.2.2 ホワイトノイズ 定常な時系列データの中で、最も簡単かつ重要な系列がホワイトノイズ（white noise）です。ホワイトノイズは、「平均が0、分散がある一定の値、自己共分散が0」が成立する時系列データと定義され、ここでは\\(u_t\\)と表します。ホワイトノイズの厳密な定義については、村尾（2019）P.32を参照してください。 \\[ \\begin{aligned} &amp; E(u_t) = 0 \\\\ &amp; \\mathit{Var}(u_t) = E(u_t u_t) = \\sigma^2 \\lt \\infty \\\\ &amp; \\mathit{Cov}(u_t, u_s) = E(u_t, u_s) = 0, \\quad t \\ne s \\end{aligned} \\] 例えば、平均が0の正規分布（標準正規分布など）に従う時系列データはホワイトノイズの一種です。ホワイトノイズを生成する場合は、rnorm()関数などを用いて正規乱数を生成し、ホワイトノイズとして利用することが一般的です。 # 標準正規分布に従うサンプル数400の乱数を生成し、折れ線グラフをプロット rnorm(n = 400, mean = 0, sd = 1) %&gt;% plot(type = &quot;l&quot;) 8.3 非定常過程 定常性を満たさない時系列データを非定常過程（non-stationary process）といいます。 ここでは、定常過程と非定常過程を区別する方法である単位根や、代表的な非定常過程の例である階差定常過程（和分過程）とトレンド定常過程について解説します。 8.3.1 単位根 時系列データの安定性を示す定常性とコインの裏表の関係にある概念が、単位根（unit root）です。時系列データが定常か非定常かを判断するうえでは、単位根の有無が重要です。 ある時系列データ\\(Y_t\\)の当期の値\\(Y_t\\)と1期前の値\\(Y_{t-1}\\)について、次のような関係を考えます。なお、\\(u_t\\)はホワイトノイズです。 \\[ Y_t = \\rho Y_{t-1} + u_t \\] ここで、ある時に変数\\(u_t\\)に生じたショックがその後の時系列データ\\(Y_t\\)に与える波及効果は、データの時間を通じた影響力を表す係数\\(\\rho\\)の絶対値\\(|\\rho|\\)が取りうる範囲によって、次の3つに分類できます。 \\(|\\rho| \\lt 1\\)のケース データの時間を通じた影響力が1より小さく、変数\\(u_t\\)に生じたショックは時間の経過に伴って減衰します。すなわち、波及効果は過渡的・一時的であり、このケースは定常過程になります。 \\(|\\rho| = 1\\)のケース データの時間を通じた影響力が1であり、変数\\(u_t\\)に生じたショックは時間の経過に伴って減衰することなく一定で永遠に続きます。すなわち、波及効果は恒久的であり、このケースは非定常過程になります。このように\\(|\\rho| = 1\\)となる\\(\\rho\\)を単位根といいます。このケースはランダムウォーク（random walk）とも呼ばれます。 \\(|\\rho| \\gt 1\\)のケース データの時間を通じた影響力が1より大きく、変数\\(u_t\\)に生じたショックは時間の経過に伴って拡大します。すなわち、波及効果は爆発的（発散）であり、このケースも非定常過程になります。 本書では2のケースと3のケースが成立するとき、すなわち\\(|\\rho| \\ge 1\\)の場合に、当該データが単位根過程（unit root process）であると呼びます。単位根過程は非定常過程の十分条件です。また、単位根過程はこの後に示す階差定常過程（和分過程）と同義です。 ケース1〜3のデータ例をRで作成して図示すると、次のようになります。 定常過程のケース1（\\(Y^1_t = 0.1 \\times Y^1_{t-1} + u_t\\)）では\\(Y=0\\)から離れてもすぐに回帰し、\\(Y=0\\)の周りで安定的に変動しています。一方、ケース2（\\(Y^2_t = Y^2_{t-1} + u_t\\)）ではデータがすぐ\\(Y=0\\)に戻らずに一定期間プラスやマイナスの領域で推移し続け、不安定な千鳥足的変動になっています。最後のケース3（\\(Y^3_t = 1.03 \\times Y^3_{t-1} + u_t\\)）ではデータが\\(Y=0\\)に戻ることなく、加速度的に\\(Y=0\\)から離れていきます。 # 時系列データを格納する要素数100の数値ベクトル y1 &lt;- vector(mode = &quot;numeric&quot;, length = 100) y2 &lt;- vector(mode = &quot;numeric&quot;, length = 100) y3 &lt;- vector(mode = &quot;numeric&quot;, length = 100) # 要素数100のホワイトノイズ（標準正規乱数） set.seed(0) u &lt;- rnorm(n = 100, mean = 0, sd = 1) # y1〜y3の初期値（t=1）にu[1]を代入しておく y1[1] &lt;- u[1] y2[1] &lt;- u[1] y3[1] &lt;- u[1] # forループでt=2以降のデータを作成 for (t in 2:100) { y1[t] &lt;- 0.1 * y1[t-1] + u[t] # ケース1 y2[t] &lt;- 1.0 * y2[t-1] + u[t] # ケース2 y3[t] &lt;- 1.03 * y3[t-1] + u[t] # ケース3 } # 折れ線グラフをプロット tibble::tibble( X = 1:100, Y1 = y1, Y2 = y2, Y3 = y3 ) %&gt;% tidyr::pivot_longer(cols = -X, values_to = &quot;Y&quot;, names_to = &quot;series&quot;) %&gt;% ggplot(mapping = aes(x = X, y = Y, color = series)) + geom_line() 上の例において、\\(t=25\\)時点で一時的なショックを発生させると、次のような結果になります。ケース1ではショックが次第に消失し\\(Y=0\\)に回帰します。一方、ケース2ではショックが消失せずに残り、ショック発生後に水準がシフトします。ケース3ではショックが増幅され、ショックがない場合に比べ増加ペースが加速します。 # 時系列データを格納する要素数100の数値ベクトル y1 &lt;- vector(mode = &quot;numeric&quot;, length = 100) y2 &lt;- vector(mode = &quot;numeric&quot;, length = 100) y3 &lt;- vector(mode = &quot;numeric&quot;, length = 100) # 要素数100のホワイトノイズ（標準正規乱数） set.seed(0) u &lt;- rnorm(n = 100, mean = 0, sd = 1) # y1〜y3の初期値（t=1）にu[1]を代入しておく y1[1] &lt;- u[1] y2[1] &lt;- u[1] y3[1] &lt;- u[1] # forループでt=2以降のデータを作成。t=25にshockが発生 for (t in 2:100) { if (t == 25) {shock &lt;- 10} else {shock &lt;- 0} y1[t] &lt;- 0.1 * y1[t-1] + u[t] + shock # ケース1 y2[t] &lt;- 1.0 * y2[t-1] + u[t] + shock # ケース2 y3[t] &lt;- 1.03 * y3[t-1] + u[t] + shock # ケース3 } # 折れ線グラフをプロット tibble::tibble( X = 1:100, Y1 = y1, Y2 = y2, Y3 = y3 ) %&gt;% tidyr::pivot_longer(cols = -X, values_to = &quot;Y&quot;, names_to = &quot;series&quot;) %&gt;% ggplot(mapping = aes(x = X, y = Y, color = series)) + geom_line() 8.3.2 階差定常過程・和分過程 非定常過程のデータは平均や分散といった性質が時間を通じて変化するため、そのままでは定常過程のデータと同様の分析を行うことができません。しかし、一定の処置を行なって定常過程に変換（定常化）すれば、定常過程のデータと同じように扱うことができます。 そうした特定の方法で定常化できる非定常過程の代表例として、原系列の階差を取ることで定常になる階差定常過程（和分過程）と、タイムトレンドを除去することで定常になるトレンド定常過程があります。 階差定常過程（difference stationary）は、原系列の階差を取ることで定常になる非定常過程であり、\\(|\\rho| \\ge 1\\)のケースに該当します。 階差定常過程は「何回階差を取れば定常過程になるか」が重要です。非定常過程\\(Y_t\\)が1次の階差変換で定常になるとき、\\(Y_t\\)は1次の和分過程（integrated process）であるといい、\\(Y_t \\sim I(1)\\)と表記します。同様に2次の階差変換で定常になる場合は2次の和分過程と呼び、\\(Y_t \\sim I(2)\\)と表記します。なお、原系列のままで定常なレベル定常過程は\\(I(0)\\)過程に分類されます。 経済・金融分野では一般的に多くの時系列データが\\(I(0)\\)過程もしくは\\(I(1)\\)過程であり、\\(I(2)\\)過程はきわめて稀です。したがって実務的には\\(I(0)\\)過程と\\(I(1)\\)過程を見分けることが重要です。 8.3.3 ランダムウォーク 階差定常過程の代表例がランダムウォークです。ランダムウォークは\\(Y_t = c + Y_{t-1} + u_t\\)の形を取ります。\\(c\\)はドリフト項といい、ランダムウォークなどの\\(I(1)\\)過程ではタイムトレンドを表現します（村尾（2019）P.66）。 次の例はドリフト項がない最も基本的なランダムウォーク\\(Y_t = Y_{t-1} + u_t\\)であり、\\(Y=0\\)の周りで不安定な千鳥足的変動になっています。 # 時系列データを格納する要素数100の数値ベクトル y &lt;- vector(mode = &quot;numeric&quot;, length = 100) # 要素数100のホワイトノイズ（標準正規乱数） set.seed(0) u &lt;- rnorm(n = 100, mean = 0, sd = 1) # yの初期値（t=1）にu[1]を代入しておく y[1] &lt;- u[1] # forループでt=2以降のデータを作成 for (t in 2:100) {y[t] &lt;- y[t-1] + u[t]} # 折れ線グラフにプロット plot(y, type = &quot;l&quot;) 一方、次の例はドリフト項があるランダムウォーク\\(Y_t = c + Y_{t-1} + u_t\\)であり、タイムトレンド（ここでは\\(c=0.2\\)）の周りで不安定な千鳥足的変動になっています。一見すると次に示すトレンド定常過程と似ていますが、ランダムウォークはタイムトレンドを除去（ディトレンド）しても非定常過程のままであり、それだけでは定常過程に変換できません。 # 時系列データを格納する要素数100の数値ベクトル y &lt;- vector(mode = &quot;numeric&quot;, length = 100) # 要素数100のホワイトノイズ（標準正規乱数） set.seed(0) u &lt;- rnorm(n = 100, mean = 0, sd = 1) # yの初期値（t=1）にu[1]を代入しておく y[1] &lt;- u[1] # forループでt=2以降のデータを作成 for (t in 2:100) {y[t] &lt;- 0.2 + y[t-1] + u[t]} # 折れ線グラフにプロット plot(y, type = &quot;l&quot;) 8.3.4 トレンド定常過程 トレンド定常過程（trend stationary）は、タイムトレンド（時間のみの関数）を取り除くことによって定常になる非定常過程です。タイムトレンドを除去するには、1階の階差変換を行なったり、線形トレンドを当てはめてトレンドラインからの乖離幅を求めたりする方法があります。 トレンド定常過程は、レベル定常過程にタイムトレンド項を加えた構造をしています。平均値が時間を通じて一定でないため、厳密には定常性の条件を満たしませんが、タイムトレンドの周りで一定の幅をもって安定的に変動するため、定常過程に近い性質があります。 トレンド定常過程のデータを扱う際の注意点は次のとおりです。 単位根の有無だけでは、定常過程とトレンド定常過程を区別できません。定常過程とトレンド定常過程はどちらも単位根をもたず、ともに\\(I(0)\\)過程に分類されます。定常過程とトレンド定常過程を見分けるには、データを図示してトレンドの有無をチェックするか、次に示すADF検定フローで「トレンド項がある\\(I(0)\\)過程」であることを確認する必要があります。 トレンド定常過程のデータに定常性を前提とする時系列分析手法は使えません。トレンド定常過程は単位根を持たない\\(I(0)\\)過程である点が定常過程と共通していますが、平均値が一定でなく定常性の条件を満たしていないため、後述するボックス＝ジェンキンス法のARMAモデルのように、定常性を前提とする時系列分析手法を使うことはできず、階差変換を行うARIMAモデルを用いる必要があります。 1階の階差変換による定常化だけでは、階差定常過程（1次の和分過程）とトレンド定常過程を区別できません。1階の階差変換はタイムトレンドを除去する手段でもあるため、1階の階差変換により階差定常過程（1次の和分過程）とトレンド定常過程をどちらも定常化することができます。一方、それ以外のタイムトレンド除去方法（例えば線形トレンドの当てはめなど）を階差定常過程データに用いても定常化することはできません。\\(I(1)\\)過程データと\\(I(0)\\)過程データは、後述する共和分の有無の確認などで取り扱いが異なるため、次に示すADF検定フローを通じて区別する必要があります。 最も単純なトレンド定常過程の例が、ホワイトノイズ\\(u_t\\)にタイムトレンド項\\(\\delta t\\)を加えた\\(Y_t = \\delta t + u_t\\)です。ここでは例として\\(\\delta = 0.1\\)とします。 # 時系列データを格納する要素数100の数値ベクトル y &lt;- vector(mode = &quot;numeric&quot;, length = 100) # 要素数100のホワイトノイズ（標準正規乱数） set.seed(0) u &lt;- rnorm(n = 100, mean = 0, sd = 1) # forループでタイムトレンド項1〜100のデータを作成 for (t in 1:100) {y[t] &lt;- 0.1 * t + u[t]} # 折れ線グラフにプロット plot(y, type = &quot;l&quot;) また、\\(|\\rho| \\lt 1\\)のケースのレベル定常過程データにタイムトレンド項を加えた\\(Y_t = \\rho Y_{t-1} + \\delta t + u_t \\; (|\\rho| \\lt 1)\\)もトレンド定常過程になります。ここでは例として\\(\\rho = 0.8\\)、\\(\\delta = 0.1\\)とします。 # 時系列データを格納する要素数100の数値ベクトル y &lt;- vector(mode = &quot;numeric&quot;, length = 100) # 要素数100のホワイトノイズ（標準正規乱数） set.seed(0) u &lt;- rnorm(n = 100, mean = 0, sd = 1) # yの初期値（t=1）にu[1]を代入しておく y[1] &lt;- u[1] # forループでt=2以降のデータを作成 for (t in 2:100) {y[t] &lt;- 0.8 * y[t-1] + 0.1 * t + u[t]} # 折れ線グラフにプロット plot(y, type = &quot;l&quot;) 上記式にドリフト項\\(c\\)を加えた\\(Y_t = c + \\rho Y_{t-1} + \\delta t + u_t \\; (|\\rho| \\lt 1)\\)も、トレンド定常過程になります。ここでは例として\\(c = 5\\)とします。トレンド定常過程におけるドリフト項\\(c\\)は初期時点の一時的なショックを表現し、ショックの消化に伴いトレンド定常過程全体が\\(Y=0\\)からレベルシフトします。ショックの消化に要する期間は\\(Y_{t-1}\\)の係数\\(\\rho\\)に、レベルシフトの幅はドリフト項\\(c\\)と\\(Y_{t-1}\\)の係数\\(\\rho\\)両方に依存します。 # 時系列データを格納する要素数100の数値ベクトル y &lt;- vector(mode = &quot;numeric&quot;, length = 100) # 要素数100のホワイトノイズ（標準正規乱数） set.seed(0) u &lt;- rnorm(n = 100, mean = 0, sd = 1) # yの初期値（t=1）にu[1]を代入しておく y[1] &lt;- u[1] # forループでt=2以降のデータを作成 for (t in 2:100) {y[t] &lt;- 5 + 0.8 * y[t-1] + 0.1 * t + u[t]} # 折れ線グラフにプロット plot(y, type = &quot;l&quot;) 8.4 単位根検定（ADF検定） 単位根が分析対象の時系列データにあるかどうかを判断するための方法が、単位根検定（unit root test）です。単位根検定にはいくつかの種類がありますが、ここでは最も一般的な拡張ディッキー＝フラー検定（Augmented Dickey-Fuller test、ADF検定）を用います。ADF検定の最終的な目的は、単位根の有無をもとに、分析対象である時系列データの和分次数（\\(I(0)\\)、\\(I(1)\\)、\\(I(2)\\)など）を判断することです。 ADF検定では、分析対象の時系列データが定数項やタイムトレンド項をもつ可能性を考慮し、次の3種類のモデルについて「単位根がある（\\(\\rho = 1\\)）」ことを帰無仮説とする検定を行います。ここで、定数項は\\(\\beta_1\\)、タイムトレンド項は\\(\\beta_2t\\)です。また、\\(\\sum^{p-1}_{s=1}{\\gamma_sY_{t-s}}\\)はラグ次数\\(p\\)の系列相関の影響を制御（除去）する役割があります。 trendモデル trendモデルは定数項\\(\\beta_1\\)とタイムトレンド項\\(\\beta_2t\\)の両方を含む回帰モデルです。 \\[ \\Delta{Y_t} = \\beta_1 + \\beta_2t + (\\rho - 1)Y_{t-1} + \\sum^{p-1}_{s=1}{\\gamma_sY_{t-s}} + u_t \\] driftモデル driftモデルは定数項\\(\\beta_1\\)を含む回帰モデルです。 \\[ \\Delta{Y_t} = \\beta_1 + (\\rho - 1)Y_{t-1} + \\sum^{p-1}_{s=1}{\\gamma_sY_{t-s}} + u_t \\] noneモデル noneモデルは定数項もタイムトレンド項も含まない回帰モデルです。 \\[ \\Delta{Y_t} = (\\rho - 1)Y_{t-1} + \\sum^{p-1}_{s=1}{\\gamma_sY_{t-s}} + u_t \\] ADF検定の具体的なフローは次のとおりです。 まず、時系列データの原系列に対し、trendモデル、driftモデル、noneモデルの順番で「単位根がある（\\(\\rho = 1\\)）」ことを帰無仮説とする検定を行います。帰無仮説が棄却できればそこでADF検定が終了し、時系列データが\\(I(0)\\)過程であると判断します。 一方、3つのモデル全てにおいて帰無仮説が棄却できなければ、時系列データを階差変換し、再びtrendモデル、driftモデル、noneモデルの順番で検定を繰り返します。1次の階差変換で帰無仮説が棄却できれば時系列データは\\(I(1)\\)過程、2次の階差変換で帰無仮説が棄却できれば時系列データは\\(I(2)\\)過程と判断します。 なお、上記のフローは単純化したもので、実際には3つのモデルそれぞれについて複数の帰無仮説を検定し、その結果によって手順が変わる枝分かれのフロー構造になっています。ADF検定フローの詳細については、村尾（2019）P.137を参照してください。 8.4.1 ADF検定フロー自作関数 上記のように、ADF検定を用いた和分次数の判断はシステマチックに実施することができます。本書では、ADF検定フローを分析対象の時系列データに自動で適用する自作関数adf_test_flow()を定義します。 adf_test_flow()自作関数の引数は次のとおりです。 y：ADF検定を適用する時系列データ。数値型ベクトルもしくはts形式データを指定します。 lag_criterion：ラグ次数の選択基準。\"AIC\"、\"BIC\"、\"Fixed\"の3つのうち1つを指定します。\"AIC\"もしくは\"BIC\"を指定すると、それぞれ赤池情報量規準（Akaike Information Criterion）とベイズ情報量規準（Bayesian Information Criterion）に基づき最適なラグ次数が自動で選択されます（このときlags_max引数に指定した値が自動で選択されるラグ次数の上限になります）。\"Fixed\"を指定する場合は、適用するラグ次数をlags_fixed引数に手動で指定します。 lags_max：ラグ次数の最大値（正の整数、デフォルト値は12）。lag_criterion引数に\"AIC\"か\"BIC\"を指定した場合に使用します。 lags_fixed：ラグ次数（正の整数、デフォルト値は1）。lag_criterion引数に\"Fixed\"を指定した場合に使用します。 sig_level：有意水準。0.01、0.05、0.1の3つのうち1つを指定します。 これらの引数を設定してadf_test_flow()自作関数を実行すると、ADF検定フローの結果を記載したtibble形式のデータフレームが出力されます。trendモデル3種類、driftモデル3種類、noneモデル1種類の合計7種類のADF検定について、yに指定した時系列データが\\(I(0)\\)過程、\\(I(1)\\)過程、\\(I(2)\\)過程のどれに従うかが判断できるまで階差次数を増やして検定を繰り返し、各検定の結果に基づく判断経路を「＊」で示します。最後に「＊」が記載されている箇所が最終的な判断になります。 library(docstring) # 時系列データに対しADF検定フローを自動で適用する関数 adf_test_flow &lt;- function(y, lag_criterion = c(&quot;AIC&quot;, &quot;BIC&quot;, &quot;Fixed&quot;), lags_max = 12, lags_fixed = 1, sig_level = c(0.01, 0.05, 0.1)) { #&#39; Automatic Augmented Dickey-Fuller Unit Root Test Flow #&#39; #&#39; @description #&#39; This function automatically performs the test flow of the Augmented Dickey-Fuller (ADF) unit root test. Written by Naoki Hattori. #&#39; #&#39; @param y Numeric vector or ts. Time series data for ADF test. #&#39; @param lag_criterion Character. Specifies how the lag order is set. &quot;AIC&quot; for Akaike Information Criterion, &quot;BIC&quot; for Bayesian Information Criterion. The maximum number of lags considered is set by lags_max. &quot;Fixed&quot; for manually set by lags_fixed. #&#39; @param lags_max Integer. Specifies maximum number of lags considered when choose &quot;AIC&quot; or &quot;BIC&quot; for lag_criterion. Default is 12. #&#39; @param lags_fixed Integer. Specifies the lag order manually when choose &quot;Fixed&quot; for lag_criterion. Default is 1. #&#39; @param sig_level Numeric. Significance level, either 0.01, 0.05, 0.1. # 引数sig_levelをurca::ur_df()関数用に文字列へ変換 sig_level_str &lt;- str_c(sig_level * 100, &quot;pct&quot;) # 出力用tibble res &lt;- tibble::tibble() %&gt;% dplyr::mutate(`フロー` = NA_real_, `階差次数` = NA_real_, `モデル` = NA_character_, `ラグ次数` = NA_real_, `帰無仮説` = NA_character_, `検定統計量` = NA_character_, `棄却点の分布` = NA_character_, `棄却点` = NA_real_, `検定値` = NA_real_, `結果` = NA_character_, `判断` = NA_character_, `判断経路` = NA_character_, ) # フローカウンタ i &lt;- 0 # 判断経路記入用ベクトル（最大21要素＝7フロー × 3ループ） flow_vec &lt;- rep(FALSE, times = 21) flow_vec[1] &lt;- TRUE #flow_vec[c(1, 8, 15)] &lt;- TRUE # 判断経路記入可否フラグ flow_cont_flag &lt;- TRUE # ラグ次数の指定 if (lag_criterion == &quot;Fixed&quot;) { lags &lt;- lags_fixed } else { lags &lt;- lags_max } # 階差次数dのループ（0〜2次） for (d in 0:2) { # データの階差変換 if (d &gt;= 1) {y &lt;- diff(y, lag = 1)} # 正規分布の棄却点 norm_cval &lt;- qnorm(p = sig_level, lower.tail = TRUE) # ADF検定 urdf_trend &lt;- urca::ur.df(y, type = &quot;trend&quot;, lags = lags, selectlags = lag_criterion) urdf_drift &lt;- urca::ur.df(y, type = &quot;drift&quot;, lags = lags, selectlags = lag_criterion) urdf_none &lt;- urca::ur.df(y, type = &quot;none&quot;, lags = lags, selectlags = lag_criterion) # ループ継続判断用ベクトル continue_vec &lt;- rep(FALSE, 3) # ADF検定フロー1 trendモデル H0：ρ - 1 = 0 i &lt;- i + 1 res %&lt;&gt;% tibble::add_row(`フロー` = i, `階差次数` = d, `モデル` = &quot;trend 第1検定&quot;, `ラグ次数` = nrow(urdf_trend@testreg$coefficients) - 3, `帰無仮説` = &quot;単位根あり&quot;, `検定統計量` = &quot;tau3&quot;, `棄却点の分布` = &quot;tau3&quot;, `棄却点` = urdf_trend@cval[&quot;tau3&quot;, sig_level_str], `検定値` = urdf_trend@teststat[, &quot;tau3&quot;], `判断経路` = if_else(flow_vec[i], &quot;＊&quot;, &quot;&quot;) ) if (urdf_trend@teststat[, &quot;tau3&quot;] &lt; urdf_trend@cval[&quot;tau3&quot;, sig_level_str]) { res$結果[i] &lt;- &quot;帰無仮説を棄却&quot; res$判断[i] &lt;- str_c(&quot;単位根なし：I(&quot;, d, &quot;)過程&quot;) if (flow_vec[i]) {flow_cont_flag &lt;- FALSE} } else { res$結果[i] &lt;- &quot;帰無仮説を棄却できない&quot; res$判断[i] &lt;- str_c(&quot;単位根の判断保留：フロー&quot;, 2 + 7 * d, &quot;へ&quot;) if (flow_cont_flag) {flow_vec[i + 1] &lt;- TRUE} } # ADF検定フロー2 trendモデル H0：β2 = 0 and ρ - 1 = 0 i &lt;- i + 1 res %&lt;&gt;% tibble::add_row(`フロー` = i, `階差次数` = d, `モデル` = &quot;trend 複合検定&quot;, `ラグ次数` = nrow(urdf_trend@testreg$coefficients) - 3, `帰無仮説` = &quot;トレンド項なし＆単位根あり&quot;, `検定統計量` = &quot;phi3&quot;, `棄却点の分布` = &quot;phi3&quot;, `棄却点` = urdf_trend@cval[&quot;phi3&quot;, sig_level_str], `検定値` = urdf_trend@teststat[, &quot;phi3&quot;], `判断経路` = if_else(flow_vec[i], &quot;＊&quot;, &quot;&quot;) ) if (urdf_trend@teststat[, &quot;phi3&quot;] &gt; urdf_trend@cval[&quot;phi3&quot;, sig_level_str]) { res$結果[i] &lt;- &quot;複合帰無仮説を棄却&quot; res$判断[i] &lt;- str_c(&quot;トレンド項あり：フロー&quot;, 3 + 7 * d, &quot;へ&quot;) if (flow_cont_flag) {flow_vec[i + 1] &lt;- TRUE} } else { res$結果[i] &lt;- &quot;複合帰無仮説を棄却できない&quot; res$判断[i] &lt;- str_c(&quot;トレンド項なし：フロー&quot;, 4 + 7 * d, &quot;へ&quot;) if (flow_cont_flag) {flow_vec[i + 2] &lt;- TRUE} } # ADF検定フロー3 trendモデル H0：ρ - 1 = 0 i &lt;- i + 1 res %&lt;&gt;% tibble::add_row(`フロー` = i, `階差次数` = d, `モデル` = &quot;trend 第2検定&quot;, `ラグ次数` = nrow(urdf_trend@testreg$coefficients) - 3, `帰無仮説` = &quot;単位根あり&quot;, `検定統計量` = &quot;tau3&quot;, `棄却点の分布` = &quot;正規分布&quot;, `棄却点` = norm_cval, `検定値` = urdf_trend@teststat[, &quot;tau3&quot;], `判断経路` = if_else(flow_vec[i], &quot;＊&quot;, &quot;&quot;) ) if (urdf_trend@teststat[, &quot;tau3&quot;] &lt; norm_cval) { res$結果[i] &lt;- &quot;帰無仮説を棄却&quot; res$判断[i] &lt;- str_c(&quot;単位根なし：I(&quot;, d, &quot;)過程&quot;) if (flow_vec[i]) {flow_cont_flag &lt;- FALSE} } else { res$結果[i] &lt;- &quot;帰無仮説を棄却できない&quot; res$判断[i] &lt;- &quot;階差変換し再検定&quot; if (flow_cont_flag &amp; (d &lt;= 1)) {flow_vec[(d + 1) * 7 + 1]} continue_vec[1] &lt;- TRUE } # ADF検定フロー4 driftモデル H0：ρ - 1 = 0 i &lt;- i + 1 res %&lt;&gt;% tibble::add_row(`フロー` = i, `階差次数` = d, `モデル` = &quot;drift 第1検定&quot;, `ラグ次数` = nrow(urdf_drift@testreg$coefficients) - 2, `帰無仮説` = &quot;単位根あり&quot;, `検定統計量` = &quot;tau2&quot;, `棄却点の分布` = &quot;tau2&quot;, `棄却点` = urdf_drift@cval[&quot;tau2&quot;, sig_level_str], `検定値` = urdf_drift@teststat[, &quot;tau2&quot;], `判断経路` = if_else(flow_vec[i], &quot;＊&quot;, &quot;&quot;), ) if (urdf_drift@teststat[, &quot;tau2&quot;] &lt; urdf_drift@cval[&quot;tau2&quot;, sig_level_str]) { res$結果[i] &lt;- &quot;帰無仮説を棄却&quot; res$判断[i] &lt;- str_c(&quot;単位根なし：I(&quot;, d, &quot;)過程&quot;) if (flow_vec[i]) {flow_cont_flag &lt;- FALSE} } else { res$結果[i] &lt;- &quot;帰無仮説を棄却できない&quot; res$判断[i] &lt;- str_c(&quot;単位根の判断保留：フロー&quot;, 5 + 7 * d, &quot;へ&quot;) if (flow_cont_flag) {flow_vec[i + 1] &lt;-TRUE} } # ADF検定フロー5 driftモデル H0：β1 = 0 and ρ - 1 = 0 i &lt;- i + 1 res %&lt;&gt;% tibble::add_row(`フロー` = i, `階差次数` = d, `モデル` = &quot;drift 複合検定&quot;, `ラグ次数` = nrow(urdf_drift@testreg$coefficients) - 2, `帰無仮説` = &quot;定数項なし＆単位根あり&quot;, `検定統計量` = &quot;phi1&quot;, `棄却点の分布` = &quot;phi1&quot;, `棄却点` = urdf_drift@cval[&quot;phi1&quot;, sig_level_str], `検定値` = urdf_drift@teststat[, &quot;phi1&quot;], `判断経路` = if_else(flow_vec[i], &quot;＊&quot;, &quot;&quot;) ) if (urdf_drift@teststat[, &quot;phi1&quot;] &gt; urdf_drift@cval[&quot;phi1&quot;, sig_level_str]) { res$結果[i] &lt;- &quot;複合帰無仮説を棄却&quot; res$判断[i] &lt;- str_c(&quot;定数項あり：フロー&quot;, 6 + 7 * d, &quot;へ&quot;) if (flow_cont_flag) {flow_vec[i + 1] &lt;- TRUE} } else { res$結果[i] &lt;- &quot;複合帰無仮説を棄却できない&quot; res$判断[i] &lt;- str_c(&quot;定数項なし：フロー&quot;, 7 + 7 * d, &quot;へ&quot;) if (flow_cont_flag) {flow_vec[i + 2] &lt;- TRUE} } # ADF検定フロー6 driftモデル H0：ρ - 1 = 0 i &lt;- i + 1 res %&lt;&gt;% tibble::add_row(`フロー` = i, `階差次数` = d, `モデル` = &quot;drift 第2検定&quot;, `ラグ次数` = nrow(urdf_drift@testreg$coefficients) - 2, `帰無仮説` = &quot;単位根あり&quot;, `検定統計量` = &quot;tau2&quot;, `棄却点の分布` = &quot;正規分布&quot;, `棄却点` = norm_cval, `検定値` = urdf_drift@teststat[, &quot;tau2&quot;], `判断経路` = if_else(flow_vec[i], &quot;＊&quot;, &quot;&quot;) ) if (urdf_drift@teststat[, &quot;tau2&quot;] &lt; norm_cval) { res$結果[i] &lt;- &quot;帰無仮説を棄却&quot; res$判断[i] &lt;- str_c(&quot;単位根なし：I(&quot;, d, &quot;)過程&quot;) if (flow_vec[i]) {flow_cont_flag &lt;- FALSE} } else { res$結果[i] &lt;- &quot;帰無仮説を棄却できない&quot; res$判断[i] &lt;- &quot;階差変換し再検定&quot; if (flow_cont_flag &amp; (d &lt;= 1)) {flow_vec[(d + 1) * 7 + 1]} continue_vec[2] &lt;- TRUE } # ADF検定フロー7 noneモデル H0：ρ - 1 = 0 i &lt;- i + 1 res %&lt;&gt;% tibble::add_row(`フロー` = i, `階差次数` = d, `モデル` = &quot;none&quot;, `ラグ次数` = nrow(urdf_none@testreg$coefficients) - 1, `帰無仮説` = &quot;単位根あり&quot;, `検定統計量` = &quot;tau1&quot;, `棄却点の分布` = &quot;tau1&quot;, `棄却点` = urdf_none@cval[&quot;tau1&quot;, sig_level_str], `検定値` = urdf_none@teststat[, &quot;tau1&quot;], `判断経路` = if_else(flow_vec[i], &quot;＊&quot;, &quot;&quot;) ) if (urdf_none@teststat[, &quot;tau1&quot;] &lt; urdf_none@cval[&quot;tau1&quot;, sig_level_str]) { res$結果[i] &lt;- &quot;帰無仮説を棄却&quot; res$判断[i] &lt;- str_c(&quot;単位根なし：I(&quot;, d, &quot;)過程&quot;) if (flow_vec[i]) {flow_cont_flag &lt;- FALSE} } else { res$結果[i] &lt;- &quot;帰無仮説を棄却できない&quot; res$判断[i] &lt;- &quot;階差変換し再検定&quot; if (flow_cont_flag &amp; (d &lt;= 1)) {flow_vec[(d + 1) * 7 + 1]} continue_vec[3] &lt;- TRUE } # ループ継続を判断（continue_vecの中に一つでもTRUEがあればbreakしない） if (!any(continue_vec)) {break} } # 結果 return(res %&gt;% dplyr::select(-`検定統計量`, -`棄却点の分布`) ) } # docstring::docstring(adf_test_flow) 8.4.2 実例：ADF検定フロー 定義した自作関数adf_test_flow()を用い、村尾（2019）「6.7 拡張ディッキー＝フラー検定の例」（P.141）及び、「6.10 Rによる拡張ディッキー＝フラー検定」（P.147）の例を再現します。 村尾（2019）6.7及び6.10では、varsパッケージのCanadaデータセットに含まれるカナダの実質労働生産性（prod）データに対してADF検定フローを適用しています。データ頻度は四半期、期間は1980年1-3月期〜2000年10-12月期です。 ここでは、ラグ次数選択基準のlag_criterionに、赤池情報量規準を意味する\"AIC\"を指定します。 adf_test_flow()自作関数を実行した結果、実質労働生産性の原系列（階差次数＝0）では単位根の存在が示唆され、1次の階差系列では単位根がないことが確認できました。したがって実質労働生産性は\\(I(1)\\)過程であると判断できます。 # Canadaデータセットを呼び出し data(Canada) # 実質労働生産性データを変数yに格納しプロット y &lt;- Canada[, &quot;prod&quot;] plot(y) # yにadf_test_flow()関数を適用 adf_prod &lt;- adf_test_flow(y = y, # 検定対象の時系列データ lag_criterion = &quot;AIC&quot;, # ラグ次数選択基準 lags_max = 10, # AICで自動選択する最大ラグ次数 sig_level = 0.05 # 有意水準 ) # 結果をコンソールに出力 # 実際に使用する際は、View(adf_prod) で別ウィンドウに結果を表示する方が結果が確認しやすい adf_prod 8.5 系列相関・自己相関 時系列データ\\(Y\\)について、現在の値\\(Y_t\\)と過去の値\\(Y_{t-1}\\)の間の相関を系列相関（serial correlation）もしくは自己相関（autocorrelation）といいます。 多くのマクロ経済データでは、正の系列相関（前期が正であれば今期も正である可能性が高い）が観察されます。系列相関がないデータは何らかのショックで平均値から乖離してもすぐにまた平均値へ戻りますが、正の系列相関があるデータは一度平均値を離れると戻るまでに一定期間を要する傾向があります。マクロ経済の活動が活発になる期間と停滞する期間が交互に繰り返される「景気循環」の現象は、マクロ時系列データ上では正の系列相関として表現されます（西山 他（2019）P.471）。 系列相関の有無やその構造を確認することは、分析対象である時系列データのモデリングやラグ次数の選択を行う上で重要です。また、最小2乗法（OLS）では誤差項に系列相関があると最小2条推定量の望ましい性質が得られないため、誤差項に対し系列相関の検定が行われます。これは後述するベクトル自己回帰（VAR）モデルでも同様です。 8.5.1 自己相関（ACF） ラグ次数\\(p\\)の（標本）自己相関係数\\(\\hat{\\rho}_p\\)は、標本平均\\(\\bar{Y} = (1/T)\\sum^{T}_{t=1}{Y_t}\\)として、次のように計算できます。 \\[ \\hat{\\rho}_{p} = \\frac{\\hat{\\gamma}_{p}}{\\hat{\\gamma}_{0}} = \\frac{\\sum^{T}_{t=p+1}{(Y_t-\\bar{Y})(Y_{t-p}-\\bar{Y})}}{\\sum^{T}_{t=1}{(Y_t-\\bar{Y})^2}} \\] コレログラム（自己相関プロット）は、横軸にラグ次数\\(p\\)、縦軸に自己相関係数の値をプロットした図で、stats::acf()関数で作成します。ACFはAuto Correlation Function（自己相関関数）を意味します。 コレログラム内に表示されている波線は「自己相関係数が0である」との帰無仮説をラグ次数\\(p = 1\\)から順番に逐次検定するための95％信頼区間を示しています。いずれかのラグ次数\\(p\\)において自己相関係数の値が信頼区間の外側にあれば、系列相関があると判断できます。 ここでは、西山 他（2019）P.478の図10-14で使用されている日本のGDPギャップ（内閣府、1980〜2016年）のデータでコレログラムを作成します。 # XLSXデータを読み込み data &lt;- readxl::read_excel(path = &quot;data_nishiyama/ch10/Fig_12_GDPgap_quarterly.xlsx&quot;, # ファイルパス（拡張子が必要、URLは不可） sheet = NULL, # シートインデックス／シート名 col_names = c(&quot;year&quot;, &quot;quarter&quot;, &quot;date&quot;, &quot;cao&quot;, &quot;boj&quot;), # ヘッダー（列名データ）の有無／列名指定 col_types = NULL, # 各列の型の指定（c：文字列型、d：数値型、D：日付型、l：論理値型） skip = 1 # 読み込み時に上からスキップする行数 ) data %&lt;&gt;% dplyr::select(date, cao, boj) %&gt;% dplyr::mutate(date = lubridate::date(zoo::as.yearqtr(date, format = &quot;%YQ%q&quot;))) %&gt;% dplyr::filter(date &lt;= &quot;2016-12-31&quot;) stats::acf()関数でコレログラムを作成すると、ラグ次数が5次以下の低次の自己相関係数が有意であることが確認できます。このように、低次ラグの自己相関係数が正であり、ラグ次数1次をピークに単調に減少する性質は、正の系列相関がある典型的なマクロ経済変数のコレログラムの例です。 しかし、これをもって、5四半期前までのGDPギャップが全て当該四半期のGDPギャップに関係しているとは判断できません。自己相関係数の計算では、ラグの積み重ねによる間接的な関係が含まれているためです。そうした間接的な影響を排除して、過去のデータと当月のデータの直接的な関係を調べる方法が、次の偏自己相関です。 stats::acf(data$cao, plot = TRUE) 8.5.2 偏自己相関（PACF） 偏自己相関プロットを作成するには、stats::pacf()関数を使用します。PACFはPartial Auto Correlation Function（偏自己相関関数）を意味します。 自己相関プロットと同様に、日本のGDPギャップ（内閣府、1980〜2016年）のデータで偏自己相関プロットを作成すると、ラグ次数1次の偏自己相関係数が有意である一方、2〜5次の偏自己相関係数は有意ではなくなりました。この結果は、当該四半期のGDPギャップと関係しているのは1四半期前のGDPギャップのみであることを示しています。 stats::pacf(data$cao, plot = TRUE) 8.5.3 リュン＝ボックス検定 自己相関プロットや偏自己相関プロットは系列相関の有無を視覚的に判断できる便利な方法ですが、プロット上に表示される95％信頼区間は逐次検定用であり、自己相関の数が大きくなると多重検定の問題（t検定を複数回行うと設定された有意水準5％よりも高い頻度で第1種の過誤が発生すること）が生じます。つまり、実際には系列相関がないにも関わらず、どこかのラグ次数で帰無仮説（自己相関係数が0）を棄却してしまう確率が増加します（西山 他（2019））。 この問題を避けるには「複数の自己相関係数が0である」という結合帰無仮説 \\[ H_0 : \\rho_1 = \\rho_2 = \\cdots = \\rho_m = 0 \\] を一度に検定する必要があります。対立仮説は「少なくとも一つの自己相関係数が0ではない」です。この結合帰無仮説の検定が、リュン＝ボックス検定（Ljung-Box test）です。なお、過去には「ダービン＝ワトソン比」で系列相関の有無を判断する方法が用いられていましたが、現在ではリュン＝ボックス検定を用いるのが一般的です（西山 他（2019）P.484）。 リュン＝ボックス検定を行うにはstats::Box.test()関数を使用し、type引数に\"Ljung-Box\"を、lag引数にラグ次数を指定します。ここでは、偏自己相関プロット（PACF）の結果に基づきラグ次数に1を指定します。 実行すると、X-squaredに修正Q統計量、p-valueにp値が出力されます。この例ではp値が5％を下回り、ラグ次数が1のとき「系列相関がない」という帰無仮説が5％の有意水準で棄却されます。すなわち、日本のGDPギャップはラグ次数が1次のとき系列相関を持つと判断できます。 stats::Box.test(data$cao, # 帰無仮説「自己相関なし」を検定する系列 lag = 1, # ラグ次数 type = &quot;Ljung-Box&quot; ) ## ## Box-Ljung test ## ## data: data$cao ## X-squared = 105.01, df = 1, p-value &lt; 2.2e-16 一方、rnorm()関数で標準正規分布に従うホワイトノイズを生成してリュン＝ボックス検定を実行すると、p値が5％を上回り「系列相関がない」との帰無仮説が5％の有意水準で棄却されません。この結果は、ホワイトノイズには有意な系列相関が確認できないことを意味しています。なお、これは帰無仮説の「系列相関がない」を採択しているわけではなく、統計的に明確な系列相関があるとは言えない、ということです。 stats::Box.test(rnorm(n = 400, mean = 0, sd = 1), # 帰無仮説「自己相関なし」を検定する系列 lag = 1, # ラグ次数 type = &quot;Ljung-Box&quot; ) ## ## Box-Ljung test ## ## data: rnorm(n = 400, mean = 0, sd = 1) ## X-squared = 0.012777, df = 1, p-value = 0.91 8.6 ボックス＝ジェンキンス法 ボックス＝ジェンキンス法（Box-Jenkins method）は、ARMAモデルやARIMAモデルといった最も基本的な時系列分析の手法です。 ボックス＝ジェンキンス法には、原系列のままで定常な「レベル定常データ」に適用できるARMAモデルと、階差変換によって定常になる「トレンド定常過程」や「階差定常過程」（単位根過程、和分過程）に適用するARIMAモデルがあります。 8.6.1 ARMAモデル ARMAモデルには、ARモデル、MAモデル、ARモデルとMAモデルを組み合わせたARMAモデルがあります。ARモデルとMAモデルはARMAモデルの一部です。ARMAモデルを適用可能な時系列データは、原系列のままで定常な「レベル定常過程」データです。 ARモデル 自己回帰（Autoregressive、AR）モデルは、過去の自分のデータを説明変数とする回帰モデルです。\\(p\\)時点前までのデータを使う自己回帰モデルを\\(\\mathit{AR}(p)\\)と表記します。 \\[ Y_t = c + \\phi_1Y_{t-1} + \\phi_2Y_{t-2} + \\phi_3Y_{t-3} + \\dots + \\phi_pY_{t-p} + u_t \\] なお、\\(u_t\\)は平均が0、分散が\\(\\sigma^2\\)の正規分布\\(N(0,\\sigma^2)\\)に従うホワイトノイズ（平均が0、分散がある一定の値、自己共分散が0）です。 MAモデル 移動平均（Moving Average、MA）モデルは、過去の\\(q\\)時点前までの誤差項\\(u_t\\)を説明変数とする回帰モデルで、\\(\\mathit{MA}(q)\\)と表記します。 \\[ Y_t = c + \\theta_1u_{t-1} + \\theta_2u_{t-2} + \\theta_3u_{t-3} + \\dots + \\theta_qu_{t-q} + u_t \\] ARMAモデル 自己回帰移動平均（Autoregressive Moving Average、ARMA）モデルは、ARモデルとMAモデルを組み合わせたモデルです。\\(p\\)次のARモデルと\\(q\\)次のMAモデルを組み合わせたARMAモデルを\\(\\mathit{ARMA}(p,q)\\)と表記します。 \\[ \\begin{aligned} Y_t &amp;= c + \\phi_1Y_{t-1} + \\phi_2Y_{t-2} + \\phi_3Y_{t-3} + \\cdots + \\phi_pY_{t-p} + \\theta_1u_{t-1} + \\theta_2u_{t-2} + \\theta_3u_{t-3} + \\cdots + \\theta_qu_{t-q} + u_t \\\\ &amp;= c + \\sum^p_{i=1}(\\phi_iY_{t-i}) + \\sum^p_{j=1}(\\theta_ju_{t-j}) + u_t \\end{aligned} \\] AR・MAモデルの比較 時系列データの特徴を観察することで、ARモデルとMAモデルのどちらを適用すべきか判断することができます。ここではサンプルとして、\\(\\mathit{AR}(1)\\)過程に従うデータ\\(Y^{\\mathit{AR}}_t = 0.7Y^{\\mathit{AR}}_{t-1} + u_t\\)、\\(\\mathit{MA}(1)\\)過程に従うデータ\\(Y^{\\mathit{MA}}_t = 0.7u_{t-1} + u_t\\)、\\(\\mathit{ARMA}(1,1)\\)過程に従うデータ\\(Y^{\\mathit{ARMA}}_t=0.7Y^{\\mathit{ARMA}}_{t-1} + 0.7u_{t-1} + u_t\\)を生成し、折れ線グラフと自己相関・偏自己相関プロットを作成します。 プロットした結果を見ると、次のような特徴があることが確認できます（馬場（2018）P.46、村尾（2019）P.63、Nielsen（2021）P.180）。 AR過程（ラグ次数\\(p\\)）：自己相関（ACF）が緩やかに減衰 ＆ 偏自己相関（PACF）がラグ\\(p+1\\)以降はゼロ MA過程（ラグ次数\\(q\\)）：自己相関（ACF）がラグ\\(q+1\\)以降はゼロ ＆ 偏自己相関（PACF）が緩やかに減衰 ARMA過程（ラグ次数\\(p,q\\)）：自己相関（ACF）、偏自己相関（PACF）どちらも緩やかに減衰（プロットからはラグ次数が決定できない） #stats::arima.sim()関数でts形式のサンプルデータを生成 y_ar &lt;- stats::arima.sim(model = list(ar = c(0.7)), n = 100) y_ma &lt;- stats::arima.sim(model = list(ma = c(0.7)), n = 100) y_arma &lt;- stats::arima.sim(model = list(ar = c(0.7), ma = c(0.7)), n = 100) forecast::ggtsdisplay(y_ar, main = &quot;AR(1)過程&quot;) forecast::ggtsdisplay(y_ma, main = &quot;MA(1)過程&quot;) forecast::ggtsdisplay(y_arma, main = &quot;ARMA(1)過程&quot;) 8.6.2 ARIMAモデル ARIMAモデルは、データの階差をとってARMAモデルを適用する手法です。基本になるARIMAモデルに加え、季節変動があるデータに用いるSARIMAモデルや、外生変数を組み込むARIMAXモデルなどがあります。 ARIMAモデルを適用する時系列データは、階差変換によって定常化できる「トレンド定常過程」や「階差定常過程（単位根過程、和分過程）」です。 ARIMAモデル 自己回帰和分移動平均（Autoregressive Integrated Moving Average、ARIMA）モデルは、時系列データを階差変換してARMAモデルを適用します。\\(d\\)階の階差をとって\\(\\mathit{ARMA}(p,q)\\)を適用するARIMAモデルを、\\(\\mathit{ARIMA}(p,d,q)\\)と表記します。 ここで、ラグ演算子（lag operator）\\(L\\)を\\(LY_t = Y_{t-1}\\)、階差演算子（difference operator）\\(\\Delta\\)を\\(\\Delta Y_t = Y_t - Y_{t-1}\\)と定義します。例えば2次のラグは\\(L(LY_t) = L^2Y_t = Y_{t-2}\\)、2階の階差は\\(\\Delta(\\Delta Y_t) = \\Delta^2 Y_t = \\Delta(Y_t - Y_{t-1}) = (Y_t - Y_{t-1}) - (Y_{t-1} - Y_{t-2}) = Y_t - 2Y_{t-1} + Y_{t-2}\\)になります。 この表記法を用いると、\\(\\mathit{ARIMA}(p,d,q)\\)は、 \\[ \\Delta^d Y_t = \\sum^p_{i=1}(\\phi_iL^i\\Delta^d Y_t) + \\sum^q_{j=1}(\\theta_jL^ju_t) + u_t \\] と表記でき、これを変形すると次のように定式化できます。 \\[ \\bigl( 1-\\sum^p_{i=1}(\\phi_iL^i) \\bigr) \\Delta^d Y_t = \\bigl( 1+\\sum^q_{j=1}(\\theta_jL^j) \\bigr) u_t \\] SARIMAモデル 季節性ARIMA（Seasonal ARIMA、SARIMA）モデルは、ARIMAモデルに季節成分を入れたモデルです。 例えば月次の時系列データの場合、通常のARIMAでは当月と1カ月前、2カ月前、3カ月前・・・の関係を\\(\\mathit{ARIMA}(p,d,q)\\)でモデル化しますが、SARIMAモデルではそれに加えて、当月と1年前の同月、2年前の同月、3年前の同月・・・の関係を季節性の次数\\((P,D,Q)\\)でモデル化します。 1周期（年間）が\\(s\\)のデータ（月次データであれば\\(s=12\\)）について、ARIMAの次数\\((p,d,q)\\)、季節性の次数\\((P,D,Q)\\)をモデル化したSARIMAモデルを、\\(\\mathit{SARIMA}(p,d,q)(P,D,Q)[s]\\)と表記します。 ここで、1周期が\\(s\\)である時系列データについて季節階差演算子\\(\\Delta_s\\)を\\(\\Delta_s Y_t = Y_t - Y_{t-s}\\)と定義します。例えば2階の季節階差は\\(\\Delta_s(\\Delta_s Y_t) = \\Delta_s^2 Y_t = \\Delta_s(Y_t - Y_{t-s}) = (Y_t - Y_{t-s}) - (Y_{t-s} - Y_{t-2s}) = Y_t - 2Y_{t-s} + Y_{t-2s}\\)になります。 この表記法を用いると、\\(\\mathit{SARIMA}(p,d,q)(P,D,Q)[s]\\)は次のように定式化できます。 \\[ \\bigl( 1-\\sum^p_{i=1}(\\phi_iL^i) \\bigr) \\bigl( 1-\\sum^P_{I=1}(\\Phi_IL^{sI}) \\bigr) \\Delta^d \\Delta^D_s Y_t = \\bigl( 1+\\sum^q_{j=1}(\\theta_jL^j) \\bigr) \\bigl( 1+\\sum^Q_{J=1}(\\Theta_JL^{sJ}) \\bigr) u_t \\] ARIMAXモデル 外生変数付きARIMA（ARIMA with eXogenous variables、ARIMAX）モデルは、ARIMAに外生変数を加えたモデルです。 単変量時系列モデルであるARIMAモデルに回帰の要素を導入したものであり、主にイベント効果、曜日・祝日効果、異常値の補正などを考慮するために用いられます。 ここで、分析対象の非説明変数（目的変数、応答変数ともいう）を\\(Y_t\\)、外性変数として導入する説明変数を\\(X_t\\)とします。\\(r\\)個の説明変数があり、時点\\(t\\)における\\(k\\)番目の説明変数を\\(X_{k,t}\\)と表記すると、\\(Y_t\\)を階差変換しない\\(\\mathit{ARIMAX}(p,0,q)\\)モデルは次のように定式化できます。 \\[ Y_t = c + \\sum^p_{i=1}(\\phi_iY_{t-i}) + \\sum^q_{j=1}(\\theta_iu_{t-j}) + \\sum^r_{k=1}(\\beta_kX_{k,t}) + u_t \\] 8.6.3 ボックス＝ジェンキンス法の分析フロー ボックス＝ジェンキンス法は、次のような一連のフローに基づいて分析を実施します。 探索的データ分析 データの折れ線グラフや自己相関・偏自己相関プロットを作成し、季節性やトレンドといったデータの特徴を把握します。 データ変換 データの特徴に応じてデータの変換を行います。特に、変動幅が徐々に拡大していくデータでは対数変換を、増加トレンドと変動幅の拡大が同時に見られるデータでは対数階差（変化率）変換などを行います。 単位根検定 ADF検定を行い、データの定常性を確認します。原系列のままで定常なレベル定常過程か、単位根はないがトレンドをもつトレンド定常過程か、単位根をもつ階差定常過程（和分過程）か判断します。階差定常過程（和分過程）の場合は、何階の階差変換で定常になるか（和分次数）を確認します。 データ分割 モデルの予測精度を評価するため、データを訓練データとテストデータに分割します。 一般的に、計量経済学の回帰分析では全てのデータを対象にモデルの推定が行われます。分析の目的がモデルの推定そのもの、すなわちデータの背景にあるメカニズムの把握にあればそれで構いませんが、分析の目的が将来予測の場合は、全てのデータを対象にモデルを推定すると未知の値に対する予測精度を評価することができません。 ボックス＝ジェンキンス法の目的はほとんどの場合将来予測であることから、分割した訓練データでモデルを推定し、残しておいた未知のテストデータ（）で予測精度を評価します。これは特に教師あり機械学習において一般的な分析手順であり、ホールドアウト法とも呼ばれます。 モデル同定・推定 ARIMAモデルの同定と推定を行います。同定とは\\(\\mathit{ARIMA}(p,d,q)\\)のラグ・階差次数\\(p,d,q\\)といったモデルの構造を決めることを言います（機械学習でいうパイパーパラメータの決定）。推定とは、モデルの構造を決めた後に係数を求めることを指します。RではARIMAモデルの同定・推定を自動で実施する関数を用います。 なお、モデルを同定する際の指標の一つに尤度（likelihood）があります。尤度とは「パラメータが与えられた時に、手持ちのデータが得られる確率」であり、手持ちのデータに対するモデルの当てはまりの良さを定量化した指標です。通常、尤度は小さな値になるため、対数変換した対数尤度（log likelihood）が用いられます。なお、この尤度（対数尤度）を最大化するようにパラメータを決める方法を最尤推定法といいます。 パラメータの数（ARIMAモデルの場合はラグ次数の\\(p,q\\)）を増やしてモデルを複雑化するほど尤度が大きくなるため、モデルを同定する際にはパラメータの数に対し一定の制約が必要です。その制約が赤池情報量規準（AIC）です。AICは「パラメータを増加させた以上に尤度が改善しているか」を示す指標であり、小さいほどモデルとして優れていることを意味します。 残差チェック モデルを推定した結果として得られる残差について、系列無相関の検定（リュン＝ボックス検定）と、正規分布の検定（ジャック＝ベラ検定）を行います。ARIMAモデルを正しく推定した場合は残差がホワイトノイズになるため、系列無相関の検定では残差系列に自己相関が見られないこと、異常値がないことを確認します。また、時系列モデルの残差項として正規分布に従うホワイトノイズを仮定しているため、正規分布の検定では残差系列が正規分布と異なっていないことを確認します。 予測精度評価 分割しておいたテストデータで予測値を作成し、実績と比較して予測精度を評価します。評価指標にはRMSE（Root Mean Square Error）などが用いられます。加えて、過去の平均値や前期の値を予測値として用いるナイーブ予測（ARIMAなどの複雑なモデルを使わない予測）の精度を上回ることを確認します。 8.6.4 実例：ボックス＝ジェンキンス法 ここではボックス＝ジェンキンス法を実際のデータに適用した例として、馬場（2018）第2部7章「RによるARIMAモデル」（P.95〜116）の実例を再現します。 1. 探索的データ分析 RのSeatbeltsデータセットに含まれる英国の交通事故死傷者数（前席）の月次時系列データfrontを読み込み、forecastパッケージのggtsdisplay()関数で折れ線グラフと自己相関・偏自己相関プロットを作成します。 折れ線グラフに明らかな周期性が見られます。自己相関プロット（ACF）で12・24・36カ月ラグに大きな自己相関があること、偏自己相関プロット（PACF）で12カ月ラグの前後に大きな自己相関があることから、1年周期の変動があると判断できます。 また、時間を通じて値が取る範囲が変化していることから、単位根をもつことが示唆されます。 front &lt;- Seatbelts[, &quot;front&quot;] forecast::ggtsdisplay(front, main = &quot;原系列&quot;) 2. データ変換 個数や人数といったデータは対数変換するとうまくモデル化できる傾向があることから、frontをlog()関数で対数変換したデータについても同様にggtsdisplay()関数でプロットします。 front_log &lt;- log(front) forecast::ggtsdisplay(front_log, main = &quot;対数系列&quot;) 原系列（対数変換した系列も含む）は単位根を持っている可能性があるため、対数系列をdiff()関数で差分をとって対数差分系列に変換し、定常化できるか確認します。 対数差分系列は長期にわたって平均値が変化せず、単位根が無くなっている可能性が示唆されます。短期の自己相関（ACF）は減りましたが、12・24・36カ月ラグではプラスの、6・18・30カ月ラグではマイナスの自己相関が残っており、対数差分系列にも周期性があることが確認できます。 front_log_diff &lt;- diff(front_log) forecast::ggtsdisplay(front_log_diff, main = &quot;対数差分系列&quot;) 12カ月単位の自己相関があるため、季節成分をもっていると判断できます。そこで、forecastパッケージのggsubseriesplot()関数を用いて、原系列を月ごとに分けたグラフを作成します。 12月が最も交通事故死亡者数が多く、2月が最も少ない傾向があることが確認できます。 ggsubseriesplot(front) 季節成分の影響を除去するため、先ほど作成した対数差分系列にさらに季節差分（ここでは12カ月前差）をとり、プロットします。季節階差はdiff()関数のlag引数に周期（ここでは12）を指定して計算します。 自己相関プロット（ACF）、偏自己相関プロット（PACF）ともに12カ月単位の自己相関が残っており、季節階差をとっても季節成分の影響を全て除去することはできませんでした。 front_log_diff_seasdiff &lt;- diff(front_log_diff, lag = 12) forecast::ggtsdisplay(front_log_diff_seasdiff, main = &quot;対数差分系列の季節階差系列&quot;) 3. 単位根検定 折れ線グラフのプロットから原系列が単位根を持っていることはほぼ明らかですが、念のためadf_test_flow()自作関数でfront単位根検定と和分次数の確認を行います。 原系列（階差次数0）に対しADF検定を適用したフロー1〜7では「単位根あり」の帰無仮説を棄却できず、最終的に1階の階差系列に対しtrend第2検定を行ったフロー10で\\(I(1)\\)過程であると判断されました。これは折れ線グラフのプロットと整合的な結果です。 adf_test_flow(y = front, # 検定対象の時系列データ lag_criterion = &quot;AIC&quot;, # ラグ次数選択基準 lags_max = 12, # AICで自動選択する最大ラグ次数 sig_level = 0.05 # 有意水準 ) 4. データ分割 モデルの予測精度を評価するため、データを訓練データとテストデータに分割します。 まず、RのSeatbeltsデータセットから、前席における死傷者数（front）、ガソリン価格（PetrolPrice）、前席のシートベルト着用を義務付ける法律の施行有無を表すフラグ（law）を抽出し、対数変換したデータを作成します。 Seatbelts_log &lt;- Seatbelts[, c(&quot;front&quot;, &quot;PetrolPrice&quot;, &quot;law&quot;)] Seatbelts_log[, &quot;front&quot;] &lt;- log(Seatbelts_log[, &quot;front&quot;]) Seatbelts_log[, &quot;PetrolPrice&quot;] &lt;- log(Seatbelts_log[, &quot;PetrolPrice&quot;]) データ期間（1969年1月〜1984年12月）のうち最後の1年（1984年）をテストデータ、それ以前を訓練データとして分割します。ts形式のデータを分割するにはstats::window()関数を使用してstart引数とend引数に時点を指定します。詳細は、第7章の「ts形式データ」を参照してください。 なお、tibble形式などデータフレーム形式のデータを分割する場合は、dplyr::slice()関数で行インデックスを指定するか、データフレームに含まれる日付などの時点情報を条件にしてdplyr::filter()関数で行をフィルタします。詳細は、第3章の「行のスライスとサンプリング」と「行のフィルタ」を参照してください。 # 訓練データ（1983年12月以前） data_train &lt;- stats::window(Seatbelts_log, end = c(1983, 12)) # テストデータ（1984年1月以降） data_test &lt;- stats::window(Seatbelts_log, start = c(1984, 1)) 5. モデル同定・推定 分割した訓練データを用い、モデルの同定・推定を行います。 まず、forecastパッケージのArima()関数を用いて手動でモデルを同定します。ここでは暫定的に、モデルを\\(\\mathit{SARIMA}(1,1,1)(1,0,0)\\)と同定してパラメータを推定します。 モデルの推定結果の係数を見ると、外生変数のPetrolPriceとlawの係数がともにマイナスになっています。これは、ガソリン価格が上がるか、シートベルト着用義務化法が施行されると交通事故死傷者数が減少することを意味しています。 library(forecast) model_sarimax &lt;- Arima(y = data_train[, &quot;front&quot;], # 分析対象のデータ order = c(1, 1, 1), # ラグ・階差次数(p,d,q) seasonal = list(order = c(1, 0, 0)), # 季節成分のラグ・階差次数(P,D,Q) xreg = data_train[, c(&quot;PetrolPrice&quot;, &quot;law&quot;)] # 外生変数 ) model_sarimax ## Series: data_train[, &quot;front&quot;] ## Regression with ARIMA(1,1,1)(1,0,0)[12] errors ## ## Coefficients: ## ar1 ma1 sar1 PetrolPrice law ## 0.2589 -0.9503 0.6877 -0.3464 -0.3719 ## s.e. 0.0826 0.0303 0.0548 0.0955 0.0467 ## ## sigma^2 = 0.009052: log likelihood = 165.33 ## AIC=-318.66 AICc=-318.18 BIC=-299.54 次に、forecastパッケージのauto.arima()関数を用いて自動でモデルを同定します。実行すると数十秒計算が行われ（PCの性能により変わります）、その後結果が出力されます。 auto.arima()関数では\\(\\mathit{SARIMA}(p,d,q)(P,D,Q)\\)の6つのラグ・階差次数を全て自動で選択することができますが、階差次数（和分次数）はADF検定で事前に確認できるため、引数dに直接指定します。ここでは、先に実施したADF検定の結果に基づきd = 1を指定します。また、1階の階差変換で単位根がなくなり定常化できることから、季節成分の階差次数DはD = 0を指定します。 auto.arima()関数を実行した結果、\\(\\mathit{SARIMA}(2,1,4)(2,0,0)[12]\\)が選択されました。パラメータの推定結果を見ると、外生変数のPetrolPriceとlawの係数はやはり両方マイナスになっています。また、モデル選択基準であるAICは-326.9と先ほど手動で同定・推定した\\(\\mathit{SARIMA}(1,1,1)(1,0,0)\\)の-318.7より小さく、自動選択のモデルが優れていることを意味しています。 library(forecast) model_sarimax_auto &lt;- auto.arima(y = data_train[, &quot;front&quot;], # 分析対象のデータ xreg = data_train[, c(&quot;PetrolPrice&quot;, &quot;law&quot;)], # 外生変数 ic = &quot;aic&quot;, # モデル選択に使用する情報量規準 d = 1, # 階差次数dを指定 D = 0, # 季節成分の階差次数Dを指定 max.order = 10, # SARIMA(p,d,q)(P,D,Q)におけるラグ次数の合計p+q+P+Qの最大値 stepwise = FALSE, # TRUEにするとステップワイズ法を使用して計算を省略 approximation = FALSE, # parallel = TRUE, # TRUEにすると並列化演算で高速化 num.cores = 4 # 並列化演算に使用するCPUのコア数（PCの環境に合わせて指定） ) model_sarimax_auto ## Series: data_train[, &quot;front&quot;] ## Regression with ARIMA(2,1,4)(2,0,0)[12] errors ## ## Coefficients: ## ar1 ar2 ma1 ma2 ma3 ma4 sar1 sar2 ## -0.4325 -0.8034 -0.2811 0.4488 -0.7833 -0.1932 0.4910 0.2800 ## s.e. 0.1156 0.0880 0.1337 0.0713 0.0656 0.0961 0.0808 0.0861 ## PetrolPrice law ## -0.4129 -0.3776 ## s.e. 0.0864 0.0458 ## ## sigma^2 = 0.00824: log likelihood = 174.45 ## AIC=-326.9 AICc=-325.31 BIC=-291.83 6. 残差チェック モデルの同定・推定ができたら、残差系列に自己相関が見られないこと、正規分布と異なっていないことを確認します。 まず、リュン＝ボックス検定で残差に有意な自己相関がないことを確認します。リュン＝ボックス検定を行うにはforecastパッケージのcheckresiduals()関数やstatsパッケージのBox.test()関数を使用します。checkresiduals()関数はリュン＝ボックス検定の結果に加えて残差の折れ線グラフ、自己相関プロット（ACF）、ヒストグラムが出力されるので便利です。 どちらの結果もp値が有意水準の0.05を上回っており、残差に有意な自己相関が見られないことが確認できます。 forecast::checkresiduals(model_sarimax_auto) ## ## Ljung-Box test ## ## data: Residuals from Regression with ARIMA(2,1,4)(2,0,0)[12] errors ## Q* = 15.685, df = 16, p-value = 0.4752 ## ## Model df: 8. Total lags used: 24 stats::Box.test(model_sarimax_auto$residuals, # 帰無仮説「自己相関なし」を検定する系列 lag = 24, # ラグ次数 type = &quot;Ljung-Box&quot; ) ## ## Box-Ljung test ## ## data: model_sarimax_auto$residuals ## X-squared = 15.685, df = 24, p-value = 0.8991 次に、ジャック＝ベラ検定で残差の正規性を確認します。ジャック＝ベラ検定を行うにはtseriesパッケージのjarque.bera.test()関数を使用します。 jarque.bera.test()関数の実行結果を見ると、p値が有意水準の0.05を上回り、正規分布と有意に異なっていないと判断できます。これは、先ほどcheckresiduals()関数で出力された残差のヒストグラムの見た目とも整合的です。 tseries::jarque.bera.test(model_sarimax_auto$residuals) ## ## Jarque Bera Test ## ## data: model_sarimax_auto$residuals ## X-squared = 0.99073, df = 2, p-value = 0.6093 7. 予測精度評価 最後にモデルの予測精度評価として、テストデータを使った予測値のRMSEを計算し、ナイーブ予測の精度を上回ることを確認します。 まず、forecastパッケージのforecast()関数でテストデータを使って予測値を作成します。 library(forecast) forecast_sarimax_auto &lt;- forecast(model_sarimax_auto, # 推定したモデルオブジェクト xreg = data_test[, c(&quot;PetrolPrice&quot;, &quot;law&quot;)], # 外生変数 h = 12, # 何期先まで予測するか level = 95 # 出力する予測値の信頼区間。複数出力するときはc(95, 70)などとベクトルで指定 ) forecast_sarimax_auto ## Point Forecast Lo 95 Hi 95 ## Jan 1984 6.106557 5.928631 6.284482 ## Feb 1984 6.140590 5.955513 6.325668 ## Mar 1984 6.171630 5.981673 6.361587 ## Apr 1984 6.281025 6.090856 6.471194 ## May 1984 6.321275 6.131059 6.511490 ## Jun 1984 6.262684 6.070323 6.455044 ## Jul 1984 6.379276 6.185295 6.573257 ## Aug 1984 6.434001 6.240020 6.627983 ## Sep 1984 6.391382 6.196907 6.585856 ## Oct 1984 6.458118 6.261655 6.654582 ## Nov 1984 6.305571 6.108808 6.502333 ## Dec 1984 6.392778 6.195893 6.589662 予測結果をプロットするには、ggplot2パッケージのautoplot()関数に予測オブジェクトを指定して実行します。テストデータを使った予測期間は、予測値に加えてforecast()関数で出力した信頼区間（ここでは95％）も図示されています。 ggplot2::autoplot(forecast_sarimax_auto) 予測値のRMSE（Root Mean Squared Error）は、forecastパッケージのaccuracy()関数で計算できます。訓練データ（Training set）のRMSEは0.088、テストデータ（Test set）のRMSEは0.104と、テストデータの予測精度（テストスコア）が訓練データの予測精度（訓練スコア）に比べやや悪いことが確認できます。 forecast::accuracy(forecast_sarimax_auto, # 予測オブジェクト x = data_test[, &quot;front&quot;] # テストデータの目的変数 ) ## ME RMSE MAE MPE MAPE MASE ## Training set 0.0005185015 0.08795943 0.07069291 -0.007384246 1.052949 0.6429185 ## Test set 0.0607077026 0.10353372 0.07280132 0.933283584 1.132013 0.6620935 ## ACF1 Theil&#39;s U ## Training set -0.0001765518 NA ## Test set 0.1767095028 1.087542 ここではテストデータの外生変数を使って予測値を計算しましたが、実際にこのモデルを運用して将来予測を行う場合は外生変数のPetrolPriceが将来どのような値をとるか不明なため、予測対象期間のPetrolPriceの値を何らか想定する必要があります（lawは法律の施行フラグであり1983年2月以降は1の値をとり続けます）。 そこで、予測対象期間のPetrolPriceの簡易的な想定として、訓練データにおけるPetrolPriceの平均値と最後の値を用い、frontの予測値を計算してRMSEを求めます。 PetrolPriceの訓練データの平均値を用いて計算したfrontの予測値のRMSEは0.083になります。 # テストデータのPetrolPriceを訓練データの平均値で置換 data_test_exmean &lt;- data_test %&gt;% data.frame() %&gt;% dplyr::mutate(PetrolPrice = mean(data_train[, &quot;PetrolPrice&quot;])) %&gt;% ts(frequency = 12, start = c(1984, 1)) # 予測値を計算 forecast_sarimax_auto_exmean &lt;- forecast(model_sarimax_auto, # 推定したモデルオブジェクト xreg = data_test_exmean[, c(&quot;PetrolPrice&quot;, &quot;law&quot;)], # 外生変数 h = 12, # 何期先まで予測するか level = 95 # 出力する予測値の信頼区間。複数出力するときはc(95, 70)などとベクトルで指定 ) ggplot2::autoplot(forecast_sarimax_auto_exmean) # RMSEを計算 forecast::accuracy(forecast_sarimax_auto_exmean, # 予測オブジェクト x = data_test[, &quot;front&quot;] # テストデータの目的変数 ) ## ME RMSE MAE MPE MAPE ## Training set 0.0005185015 0.08795943 0.07069291 -0.007384246 1.0529490 ## Test set 0.0099561964 0.08337008 0.05793756 0.135357382 0.9038067 ## MASE ACF1 Theil&#39;s U ## Training set 0.6429185 -0.0001765518 NA ## Test set 0.5269147 0.1681499910 0.8978384 PetrolPriceの訓練データの最後の値を用いて計算したfrontの予測値のRMSEは0.108になります。 # テストデータのPetrolPriceを訓練データの最後の値で置換 data_test_exlatest &lt;- data_test %&gt;% data.frame() %&gt;% dplyr::mutate(PetrolPrice = tail(data_train[, &quot;PetrolPrice&quot;], n = 1)) %&gt;% ts(frequency = 12, start = c(1984, 1)) # 予測値を計算 forecast_sarimax_auto_exlatest &lt;- forecast(model_sarimax_auto, # 推定したモデルオブジェクト xreg = data_test_exlatest[, c(&quot;PetrolPrice&quot;, &quot;law&quot;)], # 外生変数 h = 12, # 何期先まで予測するか level = 95 # 出力する予測値の信頼区間。複数出力するときはc(95, 70)などとベクトルで指定 ) ggplot2::autoplot(forecast_sarimax_auto_exlatest) # RMSEを計算 forecast::accuracy(forecast_sarimax_auto_exlatest, # 予測オブジェクト x = data_test[, &quot;front&quot;] # テストデータの目的変数 ) ## ME RMSE MAE MPE MAPE MASE ## Training set 0.0005185015 0.08795943 0.07069291 -0.007384246 1.052949 0.6429185 ## Test set 0.0686420839 0.10753223 0.07817732 1.057960774 1.214970 0.7109857 ## ACF1 Theil&#39;s U ## Training set -0.0001765518 NA ## Test set 0.1681499910 1.131791 最後に、予測精度の比較対象としてナイーブ予測による予測値を計算します。ここではナイーブ予測として、過去の平均値を予測値として用いるモデルと、前時点の値（訓練データの最後の値）を予測値として用いるモデルの2つを考え、RMSEを計算します。 過去の平均値による予測はforecastパッケージのmeanf()関数を、前時点の値による予測は同じくforecastパッケージのrwf()関数を使用します。 ナイーブ予測では、過去の平均値による予測モデルのテストデータRMSEは0.395、前時点の値による予測モデルのテストデータRMSEは0.150になりました。それに比べると\\(\\mathit{SARIMA}(2,1,4)(2,0,0)[12]\\)モデルでは、PetrolPriceの訓練データの平均値を用いて計算した予測値のRMSEが0.083、同様にPetrolPriceの訓練データの最後の値を用いて計算した予測値のRMSEが0.108と、どちらもナイーブ予測モデルのRMSEより良好であり、\\(\\mathit{SARIMA}(2,1,4)(2,0,0)[12]\\)モデルによる予測に付加価値があることが確認できます。 # 過去の平均値による予測 forecast_mean &lt;- forecast::meanf(y = data_train[, &quot;front&quot;], # 訓練データの目的変数 h = 12, # 何期先まで予測するか level = 95 # 出力する予測値の信頼区間。複数出力するときはc(95, 70)などとベクトルで指定 ) ggplot2::autoplot(forecast_mean) forecast::accuracy(forecast_mean, # 予測オブジェクト x = data_test[, &quot;front&quot;] # テストデータの目的変数 ) ## ME RMSE MAE MPE MAPE MASE ## Training set 0.0000000 0.2022141 0.1596050 -0.09183688 2.387333 1.451532 ## Test set -0.3655411 0.3949872 0.3655411 -5.80245549 5.802455 3.324423 ## ACF1 Theil&#39;s U ## Training set 0.7279029 NA ## Test set 0.6688670 4.007374 # 前時点の値（訓練データの最後の値）による予測 forecast_latest &lt;- forecast::rwf(y = data_train[, &quot;front&quot;], # 訓練データの目的変数 h = 12, # 何期先まで予測するか level = 95 # 出力する予測値の信頼区間。複数出力するときはc(95, 70)などとベクトルで指定 ) ggplot2::autoplot(forecast_latest) forecast::accuracy(forecast_latest, # 予測オブジェクト x = data_test[, &quot;front&quot;] # テストデータの目的変数 ) ## ME RMSE MAE MPE MAPE MASE ## Training set -0.002197917 0.1471474 0.1174161 -0.05778118 1.755195 1.067844 ## Test set -0.007163588 0.1498196 0.1262996 -0.16838730 1.994628 1.148635 ## ACF1 Theil&#39;s U ## Training set -0.209498 NA ## Test set 0.668867 1.525489 以上で、ボックス＝ジェンキンス法による分析の流れは終了です。 8.7 共和分 8.7.1 見せかけの回帰 2つの無関係な\\(I(1)\\)過程データを用いて回帰分析を行うと、データ間に関係がないにも関わらず回帰係数のt値や決定係数が大きくなり、「2つのデータに関係がある」と誤って判断してしまうことがあります。これを見せかけの回帰（spurious regression）といいます。 見せかけの回帰を判断するポイントの一つは、回帰分析の結果として得られる残差の系列相関に関する診断です。見せかけの回帰では残差が大きな系列相関を持つ症状があります。偏自己相関プロットやリュン＝ボックス検定で残差が系列相関を持つことが確認できれば、見せかけの回帰であることを疑うべきです。 8.7.2 共和分関係 一方、見せかけの回帰とは異なり、複数の\\(I(1)\\)過程のデータの間に意味のある関係を見出すことができる場合があります。そうした関係を共和分（cointegration）といいます。 共和分は、\\(I(1)\\)過程の2つのデータ\\(Y_t \\sim I(1)\\)と\\(X_t \\sim I(1)\\)の線型結合が\\(I(0)\\)過程に従うこと、すなわち、 \\[ \\beta_1 Y_t + \\beta_2 X_t \\sim I(0) \\] を満たす\\(\\beta_1\\)と\\(\\beta_2\\)が存在することと定義されます。これは、次の回帰式 \\[ Y_t = \\mu + \\theta X_t + u_t \\] を最小2乗法（OLS）で推定して得られる残差\\(\\hat{u}_t = Y_t - \\hat{\\mu} - \\hat{\\theta} X_t\\)が\\(I(0)\\)過程に従うことを意味します。したがって、見せかけの回帰と共和分は「残差が\\(I(1)\\)過程であれば見せかけの回帰」、「残差が\\(I(0)\\)過程であれば共和分」と区別できます。 8.7.3 エングル＝グレンジャー検定 見せかけの回帰では回帰残差が単位根をもつ\\(I(1)\\)過程、共和分関係では回帰残差が定常（\\(I(0)\\)過程）になるため、残差に単位根検定を行なって「単位根あり」の帰無仮説を棄却できなければ「見せかけの回帰」、「単位根あり」の帰無仮説を棄却できれば「共和分関係」であると判断できます。 このように、回帰残差に単位根検定を適用して「見せかけの回帰」か「共和分関係」かを判断する方法を、エングル＝グレンジャー検定（Engle-Granger test）と言います。 Rでは、エングル＝グレンジャー検定を一般化したフィリップ＝オーリアリス検定（Phillips-Ouliaris test、PO 検定）が用いられます（馬場（2018）P.142）。PO検定の帰無仮説は「共和分なし」（回帰残差が単位根をもつ）、対立仮説は「共和分あり」（回帰残差が単位根をもたない）です。 8.7.4 実例：見せかけの回帰 次の例は、西山 他（2019）P.592の「実証例12.3 南極のペンギンの数と日本のGDP」を参考に、日本の実質GDPデータを南極のペンギンの数データに回帰したものです。データ頻度は年次であり、期間は1982〜2014年です。 # XLSXデータを読み込み data &lt;- readxl::read_excel(path = &quot;data_nishiyama/ch12/Fig_2_penguin.xlsx&quot;, # ファイルパス（拡張子が必要、URLは不可） sheet = &quot;Figure&quot;, # シートインデックス／シート名 col_names = c(&quot;date&quot;, &quot;gdp&quot;, &quot;penguin&quot;), # ヘッダー（列名データ）の有無／列名指定 col_types = NULL, # 各列の型の指定（c：文字列型、d：数値型、D：日付型、l：論理値型） skip = 1 # 読み込み時に上からスキップする行数 ) # ts()関数でtibble形式のデータをts形式に変換 data_ts &lt;- ts(data %&gt;% dplyr::select(-date), start = min(data$date), end = max(data$date), frequency = 1 ) forecast::ggtsdisplay()関数で折れ線グラフと自己相関・偏自己相関プロットを図示します。 forecast::ggtsdisplay(data_ts[, &quot;gdp&quot;], main = &quot;日本の実質GDP&quot;) forecast::ggtsdisplay(data_ts[, &quot;penguin&quot;], main = &quot;南極のペンギン数&quot;) まず、実質GDPデータとペンギン数データそれぞれについてadf_test_flow()自作関数で単位根検定を行うと、どちらも\\(I(1)\\)過程と判断できます。 # 日本の実質GDPデータのの単位根検定 adf_gdp &lt;- adf_test_flow(y = data$gdp, # 検定対象の時系列データ lag_criterion = &quot;AIC&quot;, # ラグ次数選択基準 lags_max = 10, # AICで自動選択する最大ラグ次数 sig_level = 0.05 # 有意水準 ) adf_gdp # ペンギン数データの単位根検定 adf_penguin &lt;- adf_test_flow(y = data$penguin, # 検定対象の時系列データ lag_criterion = &quot;AIC&quot;, # ラグ次数選択基準 lags_max = 10, # AICで自動選択する最大ラグ次数 sig_level = 0.05 # 有意水準 ) adf_penguin 次に、実質GDPをペンギン数に回帰すると、両者には関係がないと考えられるにも関わらず、ペンギン数の回帰係数は5％の有意水準で有意に正であり、自由度修正済み決定係数は0.50とまずまずの大きさになっています。 # stats::lm()関数でgdpをpenguinに回帰 model_lm &lt;- stats::lm(formula = gdp ~ penguin, data = data ) # 回帰した結果を出力 summary(model_lm) ## ## Call: ## stats::lm(formula = gdp ~ penguin, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -84219 -25167 15091 34099 73038 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.232e+05 3.706e+04 6.022 1.15e-06 *** ## penguin 5.745e+01 9.958e+00 5.770 2.37e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 49190 on 31 degrees of freedom ## Multiple R-squared: 0.5178, Adjusted R-squared: 0.5022 ## F-statistic: 33.29 on 1 and 31 DF, p-value: 2.367e-06 # ホワイトの標準誤差で係数の仮設検定を実施 lmtest::coeftest(model_lm, vcov. = sandwich::vcovHC(model_lm, type = &quot;HC1&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.2319e+05 3.9867e+04 5.5985 3.862e-06 *** ## penguin 5.7452e+01 9.7492e+00 5.8930 1.664e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 回帰分析の結果を回帰診断プロットで視覚的に検証します。Residuals vs Fittedでは予測値に対し残差が一様に分布しておらず、残差に系列相関があることが示唆されます。 # 回帰診断プロット par(mfrow = c(2, 2)) model_lm %&gt;% plot() 確認のために残差系列の偏自己相関プロット（PACF）を作成すると、ラグ次数1次の偏自己相関係数が有意に正であり、正の系列相関があることがわかります。 また、リュン＝ボックス検定（ラグ次数1次）の結果を見るとp値が0.05を下回り、5％の有意水準で残差が系列相関をもつと判断できます。これらの残差に関する診断は、日本の実質GDPと南極のペンギン数の関係が見せかけの回帰であることを支持しています。 resid &lt;- model_lm$residuals # 残差の偏自己相関プロット stats::pacf(resid, plot = TRUE) # 残差のリュン＝ボックス検定 stats::Box.test(resid, # 帰無仮説「自己相関なし」を検定する系列 lag = 1, # ラグ次数 type = &quot;Ljung-Box&quot; ) ## ## Box-Ljung test ## ## data: resid ## X-squared = 22.877, df = 1, p-value = 1.727e-06 最後に、フィリップ＝オーリアリス検定で残差の単位根検定を行います。フィリップ＝オーリアリス検定を行うにはurcaパッケージのca.po()関数を使用します。 ca.po()関数の第一引数zには時系列データを行列形式で指定します。このとき、行列の列の順番に注意してください。第1列が回帰分析における非説明変数、第2列が説明変数に該当します。順番が異なるとPO検定の結果が異なる場合があります。 demean引数には、モデルがトレンド項と定数項の両方を含む場合はtrend、定数項のみ含む場合はconstant、トレンド項も定数項も含まない場合はnoneを指定します。 フィリップ＝オーリアリス検定の結果、検定統計量（Value of test-statistic）が2.812であり、有意水準5％の棄却点48.8439より小さいことから、「単位根あり」の帰無仮説を棄却することができません。したがって日本の実質GDPと南極のペンギン数の関係は「共和分関係」ではなく「見せかけの回帰」であることが示唆されます。 # データセットからgdp、penguinの順で選択して行列形式に変換 data_mat &lt;- data %&gt;% dplyr::select(gdp, penguin ) %&gt;% as.matrix() # フィリップ＝オーリアリス検定 urca::ca.po(z = data_mat, # データを格納した行列（非説明変数、説明変数の順） demean = &quot;trend&quot; # 検定モデル（&quot;trend&quot;, &quot;constant&quot;, &quot;none&quot;） ) %&gt;% summary() ## ## ######################################## ## # Phillips and Ouliaris Unit Root Test # ## ######################################## ## ## Test of type Pu ## detrending of series with constant and linear trend ## ## ## Call: ## lm(formula = z[, 1] ~ z[, -1] + trd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -46862 -10721 4466 16883 30395 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 393334.303 21530.139 18.269 &lt; 2e-16 *** ## z[, -1] -33.631 8.858 -3.797 0.000666 *** ## trd 9394.786 800.005 11.743 9.56e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 21140 on 30 degrees of freedom ## Multiple R-squared: 0.9138, Adjusted R-squared: 0.9081 ## F-statistic: 159.1 on 2 and 30 DF, p-value: &lt; 2.2e-16 ## ## ## Value of test-statistic is: 2.812 ## ## Critical values of Pu are: ## 10pct 5pct 1pct ## critical values 41.2488 48.8439 65.1714 8.7.5 実例：共和分 次の例は、西山 他（2019）P.597の「実証例12.4 金利の期間構造と共和分」を参考に、長期金利を短期金利に回帰したものです。データ頻度は月次であり、期間は1986年7月〜1995年9月です。 長期金利と短期金利はともに単位根過程ですが、その差である長短金利スプレッドは、リスク・プレミアムが定数である限り定常であることが示唆されます。この時、長期金利と短期金利の間に共和分関係が存在します。 # XLSXデータを読み込み data &lt;- readxl::read_excel(path = &quot;data_nishiyama/ch12/Fig_1_longshortspread.xlsx&quot;, # ファイルパス（拡張子が必要、URLは不可） sheet = 1, # シートインデックス／シート名 col_names = c(&quot;date&quot;, &quot;call&quot;, &quot;yield10&quot;, &quot;spread&quot;), # ヘッダー（列名データ）の有無／列名指定 col_types = NULL, # 各列の型の指定（c：文字列型、d：数値型、D：日付型、l：論理値型） skip = 1# 読み込み時に上からスキップする行数 ) # ts()関数でtibble形式のデータをts形式に変換 data_ts &lt;- ts(data %&gt;% dplyr::select(-date), start = c(1986, 7), end = c(1995, 9), frequency = 12 ) forecast::ggtsdisplay()関数で折れ線グラフと自己相関・偏自己相関プロットを図示します。 forecast::ggtsdisplay(data_ts[, &quot;call&quot;], main = &quot;短期金利（コールレート）&quot;) forecast::ggtsdisplay(data_ts[, &quot;yield10&quot;], main = &quot;長期金利（10年物国債利回り）&quot;) 短期金利データと長期金利データそれぞれについてadf_test_flow()自作関数で単位根検定を行うと、どちらも\\(I(1)\\)過程と判断できます。 # 短期金利データのの単位根検定 adf_call &lt;- adf_test_flow(y = data_ts[, &quot;call&quot;], # 検定対象の時系列データ lag_criterion = &quot;AIC&quot;, # ラグ次数選択基準 lags_max = 12, # AICで自動選択する最大ラグ次数 sig_level = 0.05 # 有意水準 ) adf_call # 長期金利データのの単位根検定 adf_yield10 &lt;- adf_test_flow(y = data_ts[, &quot;yield10&quot;], # 検定対象の時系列データ lag_criterion = &quot;AIC&quot;, # ラグ次数選択基準 lags_max = 12, # AICで自動選択する最大ラグ次数 sig_level = 0.05 # 有意水準 ) adf_yield10 次に、長期金利を短期金利に回帰すると、短期金利の回帰係数は5％の有意水準で有意に正であり、自由度修正済み決定係数は0.82と高い水準です。 # stats::lm()関数でgdpをpenguinに回帰 model_lm &lt;- stats::lm(formula = yield10 ~ call, data = data ) # 回帰した結果を出力 summary(model_lm) ## ## Call: ## stats::lm(formula = yield10 ~ call, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.87358 -0.26822 -0.05509 0.28396 1.34519 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.14184 0.09892 31.76 &lt;2e-16 *** ## call 0.46018 0.02049 22.46 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4242 on 109 degrees of freedom ## Multiple R-squared: 0.8223, Adjusted R-squared: 0.8207 ## F-statistic: 504.5 on 1 and 109 DF, p-value: &lt; 2.2e-16 # ホワイトの標準誤差で係数の仮設検定を実施 lmtest::coeftest(model_lm, vcov. = sandwich::vcovHC(model_lm, type = &quot;HC1&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.141838 0.103495 30.357 &lt; 2.2e-16 *** ## call 0.460176 0.021376 21.527 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 残差系列の偏自己相関プロット（PACF）を作成すると、ラグ次数1〜3次の偏自己相関係数が有意であり、系列相関があることがわかります。また、リュン＝ボックス検定（ラグ次数3次）の結果を見るとp値が0.05を下回り、5％の有意水準で残差が系列相関をもつと判断できます。 resid &lt;- model_lm$residuals # 残差の偏自己相関プロット stats::pacf(resid, plot = TRUE) # 残差のリュン＝ボックス検定 stats::Box.test(resid, # 帰無仮説「自己相関なし」を検定する系列 lag = 3, # ラグ次数 type = &quot;Ljung-Box&quot; ) ## ## Box-Ljung test ## ## data: resid ## X-squared = 90.537, df = 3, p-value &lt; 2.2e-16 最後に、フィリップ＝オーリアリス検定で残差の単位根検定を行います。フィリップ＝オーリアリス検定の結果、検定統計量（Value of test-statistic）が57.429であり、有意水準5％の棄却点48.844より大きいことから、「単位根あり」の帰無仮説を棄却します。したがって長期金利と短期金利の関係は「共和分関係」であると判断できます。 # データセットからyield10、callの順に選択して行列形式に変換 data_mat &lt;- data %&gt;% dplyr::select(yield10, call ) %&gt;% as.matrix() # フィリップ＝オーリアリス検定 urca::ca.po(z = data_mat, # データを格納した行列（非説明変数、説明変数の順） demean = &quot;trend&quot; # 検定モデル（&quot;trend&quot;, &quot;constant&quot;, &quot;none&quot;） ) %&gt;% summary() ## ## ######################################## ## # Phillips and Ouliaris Unit Root Test # ## ######################################## ## ## Test of type Pu ## detrending of series with constant and linear trend ## ## ## Call: ## lm(formula = z[, 1] ~ z[, -1] + trd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.9572 -0.2585 -0.0572 0.2389 1.2716 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.275015 0.148358 22.075 &lt;2e-16 *** ## z[, -1] 0.450544 0.021959 20.517 &lt;2e-16 *** ## trd -0.001620 0.001347 -1.203 0.232 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4234 on 108 degrees of freedom ## Multiple R-squared: 0.8247, Adjusted R-squared: 0.8214 ## F-statistic: 254 on 2 and 108 DF, p-value: &lt; 2.2e-16 ## ## ## Value of test-statistic is: 57.4292 ## ## Critical values of Pu are: ## 10pct 5pct 1pct ## critical values 41.2488 48.8439 65.1714 8.7.6 ヨハンセン検定 変数間の共和分関係を判断するためのもう一つの方法が、ヨハンセン検定（Johansen test）です。エングル＝グレンジャー検定（フィリップ＝オーリアリス検定）が2変数の間の共和分の有無を判断する方法であるのに対し、ヨハンセン検定は2つ以上の多変数の間に共和分関係がいくつ存在するか判断します。 ヨハンセン検定にはトレース検定（trace test）と最大固有値検定（maximum eigenvalue test）の2つの検定方法があります。 トレース検定の帰無仮説\\(H_0\\)は「変数間に\\(r\\)個の共和分関係が存在する」、対立仮説\\(H_1\\)は「変数は全て定常な\\(I(0)\\)過程である」です。通常は、分析対象の各変数に対して事前に単位根検定を行い、非定常な\\(I(1)\\)過程の変数がある場合に共和分検定を行うため、トレース検定で対立仮説を採択するケースはないと想定されます。 最大固有値検定の帰無仮説\\(H_0\\)は「変数間に最大で\\(r\\)個の共和分関係が存在する」、対立仮説\\(H_1\\)は「変数間に\\(r+1\\)個の共和分関係が存在する」です。トレース検定に比べ、最大固有値検定では共和分関係の数をより細かく検討することができます。 ヨハンセン検定の具体的なフローは次のとおりです。 まず、分析対象のデータセットに含まれる各変数について単位根検定を行い、定常な\\(I(0)\\)変数と非定常な\\(I(1)\\)変数がそれぞれいくつあるか確認します。データセットに\\(I(0)\\)変数が\\(K_0\\)個、\\(I(1)\\)変数が\\(K_1\\)個含まれるとすると、ヨハンセン検定で検出される共和分関係の数の最小値は\\(K_0\\)、最大値は\\(K_0+K_1-1\\)になります。 次に、ヨハンセン検定に必要な事前情報としてラグ次数\\(p\\)を確認します。ラグ次数\\(p\\)は、次に説明するベクトル自己回帰（VAR）モデルを暫定的に推定して決定します。ラグ次数を選択するための情報量規準には、赤池情報量規準（Akaike Information Criterion、AIC）、赤池最終予測誤差（Akaike’s Final Prediction Error、FPE）、ハナン＝クイン情報量規準（Hannan-Quinn Information Criterion、HQ）、シュワルツのベイズ情報量規準（Schwarz Bayesian Information Criterion、SC）を用います。これら3つの情報量規準が選ぶラグ次数には、\\(\\mathit{SC} \\leq \\mathit{HQ} \\leq \\mathit{AIC} \\fallingdotseq \\mathit{FPE}\\)の関係があります。時系列データのサンプル数が大きい場合はSCを、小さい場合はAICかFPEを、どちらか迷う場合はHQを選択してください（村尾（2019）P.117）。 また、同様に事前情報として、共和分関係を表す式に定数項・トレンド項を含むか否かを、想定する共和分関係に基づいて設定します。定数項・トレンド項の有無で検定の棄却点が変化し、結果が変わりうるため、設定には注意を要します。 最後にヨハンセン検定を実行します。帰無仮説における共和分関係の数を\\(H_0:r=0\\)、\\(H_0:r=1\\)、\\(H_0:r=2\\)と一つずつ増やしてトレース検定・最大固有値検定を繰り返し、帰無仮説\\(H_0\\)が初めて棄却できなくなった時の\\(r\\)を共和分関係の数と判断します。トレース検定で大まかな共和分関係の数を把握し、最大固有値検定で細かく見て最終的な共和分関係の数を決定します。 なお、ヨハンセン検定の詳細については、村尾（2019）7.3「ヨハンセン検定」（P.153〜）、西山 他（2019）第12章「ヨハンセン検定」（P.608〜）及び「補論」（P.626〜）を参照してください。 8.7.7 ヨハンセン検定フロー自作関数 本書では、ヨハンセン検定フローを分析対象の時系列データセットに自動で適用する自作関数johansen_test_flow()を定義します。 johansen_test_flow()自作関数の引数は次のとおりです。 data：ヨハンセン検定を適用する時系列データセット。数値型データを格納したdata.frame形式（tibble形式を含む）もしくはts形式のデータセットを指定します。 lag_criterion：ラグ次数の選択基準。\"AIC\"、\"HQ\"、\"SC\"、\"Fixed\"のうち1つを指定します。\"AIC\"、\"HQ\"、\"SC\"を指定すると、それぞれ赤池情報量規準（Akaike Information Criterion）、ハナン＝クイン情報量規準（Hannan-Quinn Information Criterion）、シュワルツのベイズ情報量規準（Schwarz Bayesian Information Criterion）に基づき最適なラグ次数が自動で選択されます（このときlags_max引数に指定した値が自動で選択されるラグ次数の上限になります）。\"Fixed\"を指定する場合は、適用するラグ次数をlags_fixed引数に手動で指定します。 lags_max：ラグ次数の最大値（正の整数、デフォルト値は10）。lag_criterion引数に\"AIC\"、\"HQ\"、\"BIC\"を指定した場合に使用します。 lags_fixed：ラグ次数（正の整数、デフォルト値は1）。lag_criterion引数に\"Fixed\"を指定した場合に使用します。 var_type：ラグ次数を決定するために事前推定するVARモデルに含む確定項。\"const\"、\"trend\"、\"both\"、\"none\"のうち1つを指定します。lag_criterion引数に\"AIC\"、\"HQ\"、\"BIC\"を指定した場合に使用します。 johansen_spec：ヨハンセン検定において、共和分関係の均衡からの誤差修正が一時的（\"transitory\"）か、長期的（\"longrun\"）かを指定します。 sig_level：有意水準。0.01、0.05、0.1の3つのうち1つを指定します。 これらの引数を設定してjohansen_test_flow()自作関数を実行すると、共和分関係式にトレンド項を含むモデル、定数項を含むモデル、トレンド項・定数項どちらも含まないモデル、の3種類のヨハンセン検定フローの結果を格納したリストが返ります。 library(docstring) # 時系列データセットに対しヨハンセン検定フローを自動で適用する関数 johansen_test_flow &lt;- function(data, lag_criterion = c(&quot;AIC&quot;, &quot;HQ&quot;, &quot;SC&quot;, &quot;Fixed&quot;), lags_max = 10, var_type = c(&quot;const&quot;, &quot;trend&quot;, &quot;both&quot;, &quot;none&quot;), lags_fixed = 1, johansen_spec = c(&quot;transitory&quot;, &quot;longrun&quot;), sig_level = c(0.01, 0.05, 0.1)) { #&#39; Automatic Johansen Cointegration Test Flow #&#39; #&#39; @description #&#39; This function automatically performs the test flow of the Johansen cointegration test. Written by Naoki Hattori. #&#39; #&#39; @param data Numeric data frame or ts/mts. Time series data set for Johansen test. #&#39; @param lag_criterion Character. Specifies how the lag order is set. &quot;AIC&quot; for Akaike Information Criterion, &quot;HQ&quot; for Hannan-Quinn Information Criterion, &quot;SC&quot; for Schwarz Bayesian Information Criterion. The maximum number of lags considered is set by lags_max. &quot;Fixed&quot; for manually set by lags_fixed. #&#39; @param lags_max Integer. Specifies maximum number of lags considered when choose &quot;AIC&quot;, &quot;HQ&quot; or &quot;SC&quot; for lag_criterion. Default is 10. #&#39; @param lags_fixed Integer. Specifies the lag order manually when choose &quot;Fixed&quot; for lag_criterion. Default is 1. #&#39; @param var_type Character. Type of deterministic regressors to include in Pre-estimated VAR model. #&#39; @param johansen_spec Character. Determines the specification of the VECM, &quot;transitory&quot; or &quot;longrun&quot;. #&#39; @param sig_level Numeric. Significance level, either 0.01, 0.05, 0.1. # 引数sig_levelをurca::ur_df()関数用に文字列へ変換 sig_level_str &lt;- str_c(sig_level * 100, &quot;pct&quot;) # ラグ次数を決定 if (lag_criterion %in% c(&quot;AIC&quot;, &quot;HQ&quot;, &quot;SC&quot;)) { # VARモデルを事前推定 model_varselect &lt;- vars::VARselect(y = data, lag.max = lags_max, type = var_type, season = NULL, exogen = NULL) lags &lt;- model_varselect$selection[str_c(lag_criterion, &quot;(n)&quot;)] %&gt;% unname() } else { lags &lt;- lags_fixed } # 結果を格納するリストを作成 res_list &lt;- list() # ヨハンセン検定に含む確定項のループ for (ecdet in c(&quot;trend&quot;, &quot;const&quot;, &quot;none&quot;)) { # ヨハンセン検定 test_trace &lt;- urca::ca.jo(x = data, type = &quot;trace&quot;, ecdet = ecdet, K = lags, spec = johansen_spec, season = NULL, dumvar = NULL) test_eigen &lt;- urca::ca.jo(x = data, type = &quot;eigen&quot;, ecdet = ecdet, K = lags, spec = johansen_spec, season = NULL, dumvar = NULL) # traceテストの結果を格納したtibbleを作成 res_trace &lt;- tibble::tibble(`タイプ` = &quot;trace&quot;, `確定項` = ecdet, `ラグ次数` = lags, `共和分関係の数` = test_trace@cval %&gt;% rownames() %&gt;% rev() %&gt;% str_extract(&quot;[0-9]&quot;) %&gt;% as.integer(), `棄却点` = test_trace@cval[, sig_level_str] %&gt;% rev() %&gt;% unname(), `検定値` = test_trace@teststat %&gt;% rev(), `判断` = NA_character_ ) for (i in 1:dim(res_trace)[1]) { if (res_trace$検定値[i] &gt;= res_trace$棄却点[i]) { res_trace$判断[i] &lt;- str_c(&quot;H0：共和分関係の数＝&quot;, res_trace$共和分関係の数[i], &quot;を棄却&quot;) } else { res_trace$判断[i] &lt;- str_c(&quot;共和分関係の数＝&quot;, res_trace$共和分関係の数[i], &quot;と判断&quot;) break } } # eigenテストの結果を格納したtibbleを作成 res_eigen &lt;- tibble::tibble(`タイプ` = &quot;eigen&quot;, `確定項` = ecdet, `ラグ次数` = lags, `共和分関係の数` = test_eigen@cval %&gt;% rownames() %&gt;% rev() %&gt;% str_extract(&quot;[0-9]&quot;) %&gt;% as.integer(), `棄却点` = test_eigen@cval[, sig_level_str] %&gt;% rev() %&gt;% unname(), `検定値` = test_eigen@teststat %&gt;% rev(), `判断` = NA_character_ ) for (i in 1:dim(res_eigen)[1]) { if (res_eigen$検定値[i] &gt;= res_eigen$棄却点[i]) { res_eigen$判断[i] &lt;- str_c(&quot;H0：共和分関係の数＝&quot;, res_eigen$共和分関係の数[i], &quot;を棄却&quot;) } else { res_eigen$判断[i] &lt;- str_c(&quot;共和分関係の数＝&quot;, res_eigen$共和分関係の数[i], &quot;と判断&quot;) break } } # traceテストとeigenテストの結果を結合してres_listに格納 eval(parse(text = str_c(&quot;res_list &lt;- append(res_list, list(&quot;, ecdet, &quot;= rbind(res_trace, res_eigen)))&quot;))) } return(res_list) } # docstring::docstring(johansen_test_flow) 8.7.8 実例：ヨハンセン検定 定義した自作関数johansen_test_flow()を用い、村尾（2019）「7.4 ヨハンセン検定の例」（P.158）及び、「7.7 Rによるヨハンセン検定」（P.166）の例を再現します。 村尾（2019）7.4及び7.7では、varsパッケージのCanadaデータセットに含まれるカナダの実質労働生産性（prod、自然対数×100）、雇用者数（e、自然対数×100）、失業率（U、％）、実質賃金（rw、自然対数×100）の4変数（サンプル数84）に対し、ヨハンセン検定を適用しています。なお、Canadaデータセットのデータ頻度は四半期であり、期間は1980年1-3月期〜2000年10-12月期です。 村尾（2019）P.158には、「検定対象の変数は、単位根検定（ADF検定）において、それぞれ\\(I(1)\\)過程と判断した4個の変数である」との記載がありますが、ADF検定フロー自作関数のadf_test_flow()でADF検定を実施したところ、有意水準5％ではCanadaデータセットの実質賃金rwは\\(I(1)\\)過程ではなく\\(I(0)\\)過程と判断できます。すると4変数のうちI(1)過程の変数は3変数であり、共和分関係の数は最大で2になります。 johansen_test_flow()自作関数における事前推定VARモデルのオプションでは、ラグ次数選択基準のlag_criterionにHannan-Quinn情報量規準を意味する\"HQ\"を、確定項のver_typeにトレンド項と定数項の両方を意味する\"both\"を指定します。また、ヨハンセン検定のオプションでは、誤差修正スペックを意味するjohansen_specに「一時的」を意味する\"transitory\"を指定します。なお、村尾（2019）7.4及び7.7では検定の優位水準を10％としていますが、ここでは5％を指定します。 johansen_test_flow()自作関数を実行した結果、trendモデルとnoneモデルでは共和分関係の数が1、constモデルでは2と判断されました（ラグ次数は何も2次）。なお、トレース検定と最大固有値検定の結果はどのモデルにおいても一致しています。村尾（2019）7.4及び7.7ではtrendモデルを採用し、共和分関係の数を1と判断しています。 # Canadaデータセットを呼び出し data(Canada) # Canadaにヨハンセン検定フローを適用 jo_canada &lt;- johansen_test_flow(data = Canada, # ヨハンセン検定を行う時系列データセット lag_criterion = &quot;HQ&quot;, # 事前推定VARモデルでラグ次数を決定する情報量規準 lags_max = 10, # 最大ラグ次数 var_type = &quot;both&quot;, # 事前推定VARモデルに含む確定項 johansen_spec = &quot;transitory&quot;, # 共和分関係の均衡からの乖離が一時的・長期的か（&quot;transitory&quot;, &quot;longrun&quot;） sig_level = 0.05 # 有意水準 ) # 結果をコンソールに表示 jo_canada$trend jo_canada$const jo_canada$none 8.8 VARモデル ベクトル自己回帰（Vector AutoRegression、VAR）モデルは、多変量時系列モデルの一種です。VARモデルでは、将来予測やシミュレーションに加えて、複数の時系列データ間の相互作用を分析することができます。 8.8.1 レベルVARモデル 変数\\(Y_{1,t}\\)と\\(Y_{2,t}\\)をモデル化したラグ次数1のVARモデル\\(VAR(1)\\)は、次のように表すことができます。\\(t\\)期の変数\\(Y_{1,t}\\)と\\(Y_{2,t}\\)が、それぞれ\\(t-1\\)期の\\(Y_{1,t-1}\\)と\\(Y_{2,t-1}\\)によって説明されています。 \\[ \\begin{aligned} Y_{1,t} &amp;= b_{10} + b_{11,1}Y_{1,t-1} + b_{12,1}Y_{2,t-1} + u_{1,t} \\\\ Y_{2,t} &amp;= b_{20} + b_{21,1}Y_{1,t-1} + b_{22,1}Y_{2,t-1} + u_{2,t} \\end{aligned} \\] 誤差項\\(u_{1,t}\\)と\\(u_{2,t}\\)は定常なホワイトノイズであり、過去の自身の誤差項とは相関していない（誤差項に系列相関がない）と仮定します。ただし、同時点の誤差項\\(u_{1,t}\\)、\\(u_{2,t}\\)の間には相関関係があります（その理由は、後の「構造VARモデル」で明らかになります）。なお、VARモデルでは誤差項（error term）を撹乱項（disturbance term）と呼ぶことがありますが、意味は同じです（村尾（2019）P.90）。 上記のモデルでは、変数\\(Y_{1,t}\\)と\\(Y_{2,t}\\)がともに\\(I(0)\\)過程であることが前提になっています。定常過程の線型結合は必ず定常過程になる（ここでは、\\(c_1Y_{1,t} + c_2Y_{2,t} \\sim I(0)\\)）ため、変数が全て\\(I(0)\\)過程であればVARモデルの各式の左辺・右辺がどちらも\\(I(0)\\)になり、VARモデル全体が定常になります（村尾（2019）P.94）。このように、定常な時系列データを原系列のまま用いて構築するVARモデルをレベルVARモデルといいます。 一般的に、\\(n\\)個の変数を持つラグ次数\\(p\\)のVARモデルを\\(\\mathit{VAR}(p)\\)と表記します。 \\[ \\boldsymbol{Y}_t = \\boldsymbol{B_0} + \\boldsymbol{B}_1\\boldsymbol{Y}_{t-1} + \\cdots + \\boldsymbol{B}_{p}\\boldsymbol{Y}_{t-p} + \\boldsymbol{u}_t \\] ここで、\\(\\boldsymbol{Y}_t\\)は\\(n\\)個の変数ベクトル、\\(\\boldsymbol{B_0}\\)は\\(n \\times 1\\)の定数ベクトル、\\(\\boldsymbol{B}_1\\)〜\\(\\boldsymbol{B}_p\\)は\\(n \\times n\\)個の係数行列、\\(\\boldsymbol{u}_t\\)は\\(n\\)個の誤差項ベクトルです。 このレベルVARモデル\\(VAR(p)\\)が定常になるには、\\(\\boldsymbol{Y}_t\\)の\\(n\\)個の変数が全て定常であることが必要です。 8.8.2 階差VARモデル レベルVARモデルでは全ての変数が定常であることを前提としていましたが、次に変数\\(Y_{1,t}\\)と\\(Y_{2,t}\\)が非定常な\\(I(1)\\)過程のケースを考えます。 1つの変数が\\(I(1)\\)のケース このケースでは、変数そのものが\\(I(1)\\)であることに加え、変数の線型結合\\(c_1Y_{1,t} + c_2Y_{2,t}\\)も必ず\\(I(1)\\)過程になります。 全ての変数が\\(I(1)\\)過程のケース このケースでは、変数\\(Y_{1,t}\\)と\\(Y_{2,t}\\)がどちらも\\(I(1)\\)過程であり、さらに両者の間に共和分の関係がない場合（すなわち「見せかけの回帰」の場合）には変数の線形結合\\(c_1Y_{1,t} + c_2Y_{2,t}\\)も\\(I(1)\\)過程になります。 VARモデルの各式の項に一つでも\\(I(1)\\)過程の項があれば、VARモデル全体が非定常になります。非定常なVARモデルには標準的な推定法が使えないため、VARモデル全体を定常化する必要があります（村尾（2019）P.101）。 上記の2つのケースでVARモデルを定常化するには、ボックス＝ジェンキンス法におけるARMAモデルとARIMAモデルの関係と同様に、変数を階差変換します。変数を階差変換すると、変数\\(Y_{1,t}\\)、\\(Y_{2,t}\\)に加えて、線形結合\\(c_1\\Delta{Y_{1,t}} + c_2\\Delta{Y_{2,t}}\\)が\\(I(0)\\)になり、VARモデル全体が定常になります（村尾（2019）P.96）。このように、時系列データを階差変換して構築するVARモデルを階差VARモデルといいます。 8.8.3 VECモデル 変数\\(Y_{1,t}\\)と\\(Y_{2,t}\\)がどちらも\\(I(1)\\)過程であり、かつ両者が共和分の関係にある場合は、変数の線型結合\\(c_1Y_{1,t} + c_2Y_{2,t}\\)が\\(I(0)\\)過程になります。こうした共和分の関係は、変数\\(Y_{1,t}\\)と\\(Y_{2,t}\\)の間に経済学的な均衡関係が成立していると解釈できます。 このとき、変数\\(Y_{1,t}\\)と\\(Y_{2,t}\\)それぞれは\\(I(1)\\)変数なので、レベルVARモデルはモデル全体が定常になりません。しかし、定常性を確保するために階差変換して階差VARモデルを構築すると、共和分による均衡関係の情報が失われてしまいます。 このようなケースにおいて、共和分の関係を活かしつつモデルを定常化する方法が、ベクトル誤差修正（Vector Error Correction、VEC）モデルです（村尾（2019）P.101）。 まず、\\(n\\)個の変数を持つラグ次数2のVARモデル：\\(\\mathit{VAR}(2)\\)を考えます。 \\[ \\boldsymbol{Y}_t = \\boldsymbol{B}_1\\boldsymbol{Y}_{t-1} + \\boldsymbol{B}_{2}\\boldsymbol{Y}_{t-2} + \\boldsymbol{u}_t \\] この両辺から\\(\\boldsymbol{Y}_{t-1}\\)を引いて式を整理すると、 \\[ \\Delta \\boldsymbol{Y}_t = \\boldsymbol{\\Pi}\\boldsymbol{Y}_{t-1} + \\boldsymbol{\\Gamma}_{1}\\Delta\\boldsymbol{Y}_{t-1} + \\boldsymbol{u}_t \\] となります。なお、\\(\\boldsymbol{\\Pi} = - \\boldsymbol{I}_n + \\boldsymbol{B}_1 + \\boldsymbol{B}_2\\)、\\(\\boldsymbol{\\Gamma}_1 = - \\boldsymbol{B}_2\\)であり、また\\(\\boldsymbol{I}_n\\)は\\(n \\times n\\)の単位行列です。 ここで、変数ベクトルを階差変換した\\(\\boldsymbol{Y}_t\\)と\\(\\boldsymbol{Y}_{t-1}\\)は\\(I(0)\\)であり、レベル型の線形結合\\(\\boldsymbol{\\Pi}\\boldsymbol{Y}_{t-1}\\)も\\(I(0)\\)になります。したがって、このモデル全体で定常化が達成されています。 さらに、係数行列\\(\\boldsymbol{\\Pi}\\)が\\(\\boldsymbol{\\Pi} = \\boldsymbol{\\alpha}\\boldsymbol{\\beta&#39;}\\)に分解できるため、上の式は次のように書き換えることができ、 \\[ \\Delta \\boldsymbol{Y}_t = \\boldsymbol{\\alpha}\\boldsymbol{\\beta&#39;}\\boldsymbol{Y}_{t-1} + \\boldsymbol{\\Gamma}_{1}\\Delta\\boldsymbol{Y}_{t-1} + \\boldsymbol{u}_t \\] これをベクトル誤差修正（VEC）モデルと呼びます。 この場合はラグ次数1のVECモデルのため、\\(\\mathit{VEC}(1)\\)と表記します。このように、非定常なラグ次数2のレベルVARモデル：\\(\\mathit{VAR}(2) \\sim I(1)\\)を、共和分関係を利用することで、定常なラグ次数1次のVECモデル：\\(\\mathit{VEC}(1) \\sim I(0)\\)に変換することができます。 ベクトル誤差修正モデルには3つの要素があります。 \\(\\boldsymbol{\\beta&#39;}\\boldsymbol{Y}_{t-1}\\)：誤差修正項（error correction term）は、共和分関係で表される均衡関係からの一時的な乖離を表していると解釈できます。 \\(\\boldsymbol{\\beta}\\)：共和分行列（cointegration matrix）は、\\(n\\)個の内生変数から共和分関係（均衡関係）を選び出す役割があります。 \\(\\boldsymbol{\\alpha}\\)：調整行列（adjusting matrix）は、一時的な乖離が均衡へ向かうスピードを表しています。 8.8.4 構造VARモデル ここまで検討してきたVARモデルは、右辺が定数項、内生変数のラグ項、誤差項のみで構成されており、右辺に内生変数の同時点（ここではt時点）の項が含まれていません。このようにモデルを内生変数について解き、内生変数の変動を先決変数（内生変数のラグ項および外生変数）のみで表す次のようなモデルを、誘導型（reduced form）モデルと呼びます（エンダース（2019）P.263）。 「レベルVARモデル」に記載したとおり、誘導VARモデルの誤差項\\(u_{1,t}\\)、\\(u_{2,t}\\)は互いに相関しています。従って、誘導VARモデルの誤差項\\(u_{1,t}\\)、\\(u_{2,t}\\)は変数\\(Y_{1,t}\\)、\\(Y_{2,t}\\)の純粋なショックではなく、経済学的に意味を持ちません。 \\[ \\begin{aligned} Y_{1,t} &amp;= b_{11,1}Y_{1,t-1} + b_{12,1}Y_{2,t-1} + u_{1,t} \\\\ Y_{2,t} &amp;= b_{21,1}Y_{1,t-1} + b_{22,1}Y_{2,t-1} + u_{2,t} \\end{aligned} \\] 一方、モデルを内生変数について解く前は、右辺に内生変数の同時点の項が含まれています。これは変数が過去の値からだけでなく同時点の値からも影響を受けることを意味しており、経済構造の実態を表していると考えられます（例えば拡張的財政政策による同時点のGDP押し上げなど）。このように、変数間の理論的な相互関係を明示した次のようなモデルを、構造型（structured form）モデルもしくは構造VARモデル（Structural VAR、SVAR）と呼びます。 構造VARモデルの誤差項\\(\\epsilon_{1,t}\\)、\\(\\epsilon_{2,t}\\)は、誘導VARモデルの誤差項と異なり、互いに相関していません。構造VARモデルの誤差項\\(\\epsilon_{1,t}\\)、\\(\\epsilon_{2,t}\\)は変数\\(Y_{1,t}\\)、\\(Y_{2,t}\\)の純粋なショックであり、経済学的に意味を持っています。そのため、構造VARモデルの誤差項は構造ショック（structural shock）とも呼ばれます。 \\[ \\begin{aligned} Y_{1,t} &amp;= a_{12,0}Y_{2,t} + a_{11,1}Y_{1,t-1} + a_{12,1}Y_{2,t-1} + \\epsilon_{1,t} \\\\ Y_{2,t} &amp;= a_{21,0}Y_{1,t} + a_{21,1}Y_{1,t-1} + a_{22,1}Y_{2,t-1} + \\epsilon_{2,t} \\end{aligned} \\] VARモデルを予測にのみ用いる場合は、誘導VARモデルを推定すればOKです。一方で、VARモデルを経済構造の分析に用いる場合、すなわち、各変数に生じたショックがモデルを通じて将来の変数にどのように影響するか知りたい場合は、構造VARモデルを推定する必要があります（エンダース（2019）P.299）。 しかし、ここで問題が生じます。誘導VARモデルはOLSで直接推定できますが、構造VARモデルは右辺にある同時点の内生変数の項と誤差項が相関しており、直接推定することができません。したがって、構造VARモデルを推定するためには、構造VARモデルを内生変数について解いて誘導VARモデルに変換し、誘導VARモデルの推定結果を用いて元の構造的VARモデルのパラメータを再現する必要があります。このように誘導VARモデルから構造VARモデルのパラメータを再現することを、識別（identification）と呼びます（エンダース（2019）P.276）。 例として、次の構造VARモデル、 \\[ \\begin{aligned} Y_{1,t} &amp;= a_{12,0}Y_{2,t} + a_{11,1}Y_{1,t-1} + a_{12,1}Y_{2,t-1} + \\epsilon_{1,t} \\\\ Y_{2,t} &amp;= a_{21,0}Y_{1,t} + a_{21,1}Y_{1,t-1} + a_{22,1}Y_{2,t-1} + \\epsilon_{2,t} \\end{aligned} \\] を\\(Y_{1,t}\\)、\\(Y_{2,t}\\)について解くと、 \\[ \\begin{aligned} Y_{1,t} &amp;= \\frac{(a_{11,1}+a_{12,0}a_{21,1})Y_{1,t-1} + (a_{12,1}+a_{12,0}a_{22,1})Y_{2,t-1} + \\epsilon_{1,t} + a_{12,0}\\epsilon_{2,t}}{1-a_{12,0}a_{21,0}} \\\\ Y_{2,t} &amp;= \\frac{(a_{21,1}+a_{21,0}a_{11,1})Y_{1,t-1} + (a_{22,1}+a_{21,0}a_{12,1})Y_{2,t-1} + a_{21,0}\\epsilon_{1,t} + \\epsilon_{2,t}}{1-a_{12,0}a_{21,0}} \\end{aligned} \\] の誘導VARモデルに変換できます。ここで、誘導VARモデルの誤差項\\(u_{1,t}\\)、\\(u_{2,t}\\)は、 \\[ u_{1, t} = \\frac{\\epsilon_{1,t} + a_{12,0}\\epsilon_{2,t}}{1-a_{12,0}a_{21,0}}, \\quad u_{2, t} = \\frac{a_{21,0}\\epsilon_{1,t} + \\epsilon_{2,t}}{1-a_{12,0}a_{21,0}} \\] であり、構造VARモデルの誤差項\\(\\epsilon_{1,t}\\)、\\(\\epsilon_{2,t}\\)の線型結合（加重平均）になっています。これが、誘導VARモデルの誤差項\\(u_{1,t}\\)、\\(u_{2,t}\\)が互いに相関している理由です。 変換した誘導VARモデルを、 \\[ \\begin{aligned} Y_{1,t} &amp;= b_{11,1}Y_{1,t-1} + b_{12,1}Y_{2,t-1} + u_{1,t} \\\\ Y_{2,t} &amp;= b_{21,1}Y_{1,t-1} + b_{22,1}Y_{2,t-1} + u_{2,t} \\end{aligned} \\] に書き直すと、誘導VARモデルで推定できるパラメータは、係数\\(b_{11,1}\\)、\\(b_{12,1}\\)、\\(b_{21,1}\\)、\\(b_{22,1}\\)、誤差項の分散\\(\\mathit{Var}(u_{1, t})\\)、\\(\\mathit{Var}(u_{2, t})\\)、誤差項の共分散\\(\\mathit{Cov}(u_{1,t},\\, u_{2,t})\\)の合計7個です。一方で、変換前の構造VARモデルの未知パラメータは、係数\\(a_{12,0}\\)、\\(a_{11,1}\\)、\\(a_{12,1}\\)、\\(a_{21,0}\\)、\\(a_{21,1}\\)、\\(a_{22,1}\\)、誤差項（構造ショック）の分散\\(\\sigma^2_{Y_1}\\)、\\(\\sigma^2_{Y_2}\\)の合計8個です（構造VARモデルの誤差項は互いに相関していないので共分散は常にゼロです）。誘導VARモデルで推定できるパラメータ数（7 個）は構造VARモデルの未知パラメータ数（8個）より少なく、この状態では誘導VARモデルから構造VARモデルを再現（識別）することはできません。したがって、誘導VARモデルから構造VARモデルを識別するには、構造VARモデルに何らかの制約を課す必要があります。 一般的に、\\(n\\)種類の変数があるVARモデルについて、推定した誘導VARモデルから構造VARモデルを識別するためには、構造モデルに\\(n(n-1)/2\\)個の制約を課す必要があります（エンダース（2019）P.301）。上記の例では変数が\\(Y_{1,t}\\)と\\(Y_{2,t}\\)の2種類のため、\\(n(n-1)/2=1\\)となり、制約を1つ課す必要があることが分かります。変数が2〜10種類の場合に必要になる制約の数は次のとおりです。 for (n in 2:10) { const &lt;- n * (n - 1) / 2 cat(str_c(&quot;変数：&quot;, stringr::str_pad(n, width = 2), &quot;種類 制約：&quot;, stringr::str_pad(const, width = 2), &quot;個\\n&quot;)) } ## 変数： 2種類 制約： 1個 ## 変数： 3種類 制約： 3個 ## 変数： 4種類 制約： 6個 ## 変数： 5種類 制約：10個 ## 変数： 6種類 制約：15個 ## 変数： 7種類 制約：21個 ## 変数： 8種類 制約：28個 ## 変数： 9種類 制約：36個 ## 変数：10種類 制約：45個 短期制約：再帰的制約 構造VARモデルの制約として最もよく用いられるのは、変数の順序に応じて機械的に制約を課す再帰的制約（recursive restriction）です（エンダース（2019）P.276、村尾（2019）P.197）。再帰的制約はコレスキー分解（Choleski decomposition）による識別とも呼ばれます。また、再帰的制約は変数の同時点間の関係に制約を課すことから、短期制約（short-run restriction）もしくは同時点制約とも呼ばれます（宮尾（2006）P.22、北岡 他（2013）P.70、西山 他（2019）P.612）。 次のような、未知パラメータが18個（係数15個、誤差項の分散3個）ある3変数1次ラグの構造VARを考えます。 \\[ \\begin{aligned} Y_{1,t} &amp;= a_{12,0}Y_{2,t} + a_{13,0}Y_{3,t} + a_{11,1}Y_{1,t-1} + a_{12,1}Y_{2,t-1} + a_{13,1}Y_{3,t-1} + \\epsilon_{1,t} \\\\ Y_{2,t} &amp;= a_{21,0}Y_{1,t} + a_{23,0}Y_{3,t} + a_{21,1}Y_{1,t-1} + a_{22,1}Y_{2,t-1} + a_{23,1}Y_{3,t-1} + \\epsilon_{2,t} \\\\ Y_{3,t} &amp;= a_{31,0}Y_{1,t} + a_{32,0}Y_{2,t} + a_{31,1}Y_{1,t-1} + a_{32,1}Y_{2,t-1} + a_{33,1}Y_{3,t-1} + \\epsilon_{3,t} \\end{aligned} \\] ここで、第1式の\\(Y_{2,t}\\)の係数\\(a_{12,0}\\)と\\(Y_{3,t}\\)の係数\\(a_{13,0}\\)、第2式の\\(Y_{3,t}\\)の係数\\(a_{23,0}\\)について、\\(a_{12,0} = 0\\)、\\(a_{13,0} = 0\\)、\\(a_{23,0} = 0\\)という3つの制約を置くと、構造VARモデルの未知パラメータは15個になります。 一方、この構造VARモデルを誘導VARモデル、 \\[ \\begin{aligned} Y_{1,t} &amp;= b_{11,1}Y_{1,t-1} + b_{12,1}Y_{2,t-1} + b_{13,1}Y_{3,t-1} + u_{1,t} \\\\ Y_{2,t} &amp;= b_{21,1}Y_{1,t-1} + b_{22,1}Y_{2,t-1} + b_{23,1}Y_{3,t-1} + u_{2,t} \\\\ Y_{3,t} &amp;= b_{31,1}Y_{1,t-1} + b_{32,1}Y_{2,t-1} + b_{33,1}Y_{3,t-1} + u_{3,t} \\end{aligned} \\] に変換すると、推定できるパラメータは15個（係数9個、誤差項の分散3個、誤差項の共分散3個）です。したがって、構造VARモデルに3つの制約を課すと、誘導VARモデルで推定した15個のパラメータから構造VARモデルの15個の未知パラメータが全て求められるようになります。このように、構造VARモデルを識別するために必要十分な数の制約を課すことを丁度識別（just identified）もしくは適度識別といいます（エンダース（2019）P.276、村尾（2019）P.200）。 ここで、構造VARモデルの係数に課した3つの制約について考えると、\\(a_{12,0} = 0\\)と\\(a_{13,0} = 0\\)の制約は、\\(Y_{1}\\)が同時点の\\(Y_{2}\\)と\\(Y_{3}\\)から影響を受けないと想定していることを意味します。また、\\(a_{23,0} = 0\\)は、\\(Y_{2}\\)が同時点の\\(Y_{3}\\)から影響を受けないと想定していることを意味します。一方、その他の係数にはゼロ制約を課していないため、\\(Y_{3}\\)は同時点の\\(Y_{1}\\)、\\(Y_{2}\\)から影響を受け、\\(Y_{2}\\)は同時点の\\(Y_{1}\\)から影響を受けることが許容されています。 すなわち、誘導VARモデルから構造VARモデルを識別するために再帰的制約を課すことは、「順序が先の変数（ここでは\\(Y_{1}\\)）ほど外生性が強く（他変数に影響するが自分は他変数の影響を受けない）、順序が後の変数（ここでは\\(Y_{3}\\)）ほど内生性が強い（他変数の影響を受けるが自分は他変数に影響しない）」と暗黙理に仮定していることを意味します。実際のデータがこの仮定通りであれば問題ありませんが、仮定と異なっていれば、構造VARモデルを誤って識別することになります。こうした再帰的制約における変数の順序の問題は、この後のインパルス応答分析において大きな課題になります。 短期制約：非再帰的制約 こうした機械的な再帰的制約に対し、変数の順序に依存しない柔軟な制約は非再帰的制約（nonrecursive restriction）と呼ばれます（宮尾（2006）P.23、エンダース（2019）P.302、村尾（2019）P.203）。非再帰的制約も変数の同時点間の関係に制約を課す点では再帰的制約と同じであり、短期制約（short-run restriction）の一種です。 再帰的制約で用いたものと同じ、3変数1次ラグの構造VARモデルを考えます。ただし、構造VARモデルの誤差項\\(\\epsilon_{i,t}\\)は\\(\\epsilon_{i,t} = \\sigma_i \\epsilon^{SD}_{i,t}\\)に変形します。\\(\\sigma_i\\)は誤差項\\(\\epsilon_{i,t}\\)の標準偏差、\\(\\epsilon^{SD}_{i,t}\\)は平均0、分散1になるよう標準化した標準化構造誤差項です。 この構造VARモデルの右辺にある\\(Y_{1,t}\\)、\\(Y_{2,t}\\)、\\(Y_{3,t}\\)の項をそれぞれ左辺に移項し、行列形式で表示すると、 \\[ \\begin{bmatrix} 1 &amp; -a_{12,0} &amp; -a_{13,0} \\\\ -a_{21,0} &amp; 1 &amp; -a_{23,0} \\\\ -a_{31,0} &amp; -a_{32,0} &amp; 1 \\end{bmatrix} \\begin{bmatrix} Y_{1,t} \\\\ Y_{2,t} \\\\ Y_{3,t} \\end{bmatrix} = \\begin{bmatrix} a_{11,1} &amp; a_{12,1} &amp; a_{13,1} \\\\ a_{21,1} &amp; a_{22,1} &amp; a_{23,1} \\\\ a_{31,1} &amp; a_{32,1} &amp; a_{33,1} \\end{bmatrix} \\begin{bmatrix} Y_{1,t-1} \\\\ Y_{2,t-1} \\\\ Y_{3,t-1} \\end{bmatrix} + \\begin{bmatrix} \\sigma_{1} &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_{2} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{3} \\end{bmatrix} \\begin{bmatrix} \\epsilon^{SD}_{1,t} \\\\ \\epsilon^{SD}_{2,t} \\\\ \\epsilon^{SD}_{3,t} \\end{bmatrix} \\] となり、 \\[ \\boldsymbol{A} = \\begin{bmatrix} 1 &amp; -a_{12,0} &amp; -a_{13,0} \\\\ -a_{21,0} &amp; 1 &amp; -a_{23,0} \\\\ -a_{31,0} &amp; -a_{32,0} &amp; 1 \\end{bmatrix} , \\quad \\boldsymbol{A}_1 = \\begin{bmatrix} a_{11,1} &amp; a_{12,1} &amp; a_{13,1} \\\\ a_{21,1} &amp; a_{22,1} &amp; a_{23,1} \\\\ a_{31,1} &amp; a_{32,1} &amp; a_{33,1} \\end{bmatrix} , \\quad \\boldsymbol{B} = \\begin{bmatrix} \\sigma_{1} &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_{2} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{3} \\end{bmatrix} \\] とすると、構造VARモデルは次のように表すことができます。\\(\\boldsymbol{Y}_t\\)と\\(\\boldsymbol{Y}_{t-1}\\)は3個の変数ベクトルです。 \\[ \\boldsymbol{A} \\boldsymbol{Y}_t = \\boldsymbol{A}_1 \\boldsymbol{Y}_{t-1} + \\boldsymbol{B} \\boldsymbol{\\epsilon}^{SD}_t \\] この構造VARモデルに対し、両辺に行列\\(\\boldsymbol{A}\\)の逆行列\\(\\boldsymbol{A}^{-1}\\)を掛けると、次のように誘導VARモデルに変換することができます。 \\[ \\boldsymbol{Y}_t = \\boldsymbol{A}^{-1} \\boldsymbol{A}_1 \\boldsymbol{Y}_{t-1} + \\boldsymbol{A}^{-1} \\boldsymbol{B} \\boldsymbol{\\epsilon}^{SD}_t \\] ここで、誘導誤差項ベクトル\\(\\boldsymbol{u}_t\\)は\\(\\boldsymbol{A}^{-1} \\boldsymbol{B} \\boldsymbol{\\epsilon}^{SD}_t\\)に相当します。したがって、識別すべき（標準化）構造誤差項ベクトル\\(\\boldsymbol{\\epsilon}^{SD}_t\\)は、誘導誤差項ベクトル\\(\\boldsymbol{u}_t\\)を使って\\(\\boldsymbol{\\epsilon}^{SD}\\_t = \\boldsymbol{B}^{-1} \\boldsymbol{A} \\boldsymbol{u}\\_t\\)と表すことができます。 いま、推定した誘導誤差項ベクトル\\(\\boldsymbol{u}_t\\)から（標準化）構造誤差項ベクトル\\(\\boldsymbol{\\epsilon}_t\\)を識別するには、3変数VARの場合\\(3(3-1)/2 = 3\\)個の制約が必要になります。 再帰的制約では\\(a_{12,0} = 0\\)、\\(a_{13,0} = 0\\)、\\(a_{23,0} = 0\\)の制約を置きました。これは次のように、行列\\(\\boldsymbol{A}\\)の右上三角部分を0と置いたことを意味します。 \\[ \\boldsymbol{A} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ -a_{21,0} &amp; 1 &amp; 0 \\\\ -a_{31,0} &amp; -a_{32,0} &amp; 1 \\end{bmatrix} \\] このとき、構造VARモデルにおける変数\\(Y_{1,t}\\)、\\(Y_{2,t}\\)、\\(Y_{3,t}\\)の同時点の関係は次のようになります。 \\[ \\begin{aligned} Y_{1,t} &amp;= \\epsilon_{1,t} \\\\ Y_{2,t} &amp;= a_{21,0} Y_{1,t} + \\epsilon_{2,t} \\\\ Y_{3,t} &amp;= a_{31,0} Y_{1,t} + a_{32,0} Y_{2,t} + \\epsilon_{3,t} \\end{aligned} \\] 一方、非再帰的制約では想定する経済構造に基づいて、対角要素以外の場所に柔軟に制約を設定します。例えば、 \\[ \\boldsymbol{A} = \\begin{bmatrix} 1 &amp; -a_{12,0} &amp; 0 \\\\ 0 &amp; 1 &amp; -a_{23,0} \\\\ -a_{31,0} &amp; 0 &amp; 1 \\end{bmatrix} \\] とすると、構造VARモデルにおける変数\\(Y_{1,t}\\)、\\(Y_{2,t}\\)、\\(Y_{3,t}\\)の同時点の関係は、 \\[ \\begin{aligned} Y_{1,t} &amp;= a_{12,0} Y_{2,t} + \\epsilon_{1,t} \\\\ Y_{2,t} &amp;= a_{23,0} Y_{3,t} + \\epsilon_{2,t} \\\\ Y_{3,t} &amp;= a_{31,0} Y_{1,t} + \\epsilon_{3,t} \\end{aligned} \\] となり、再帰的制約とは異なる構造を想定することができます。 非再帰的制約の一例に、Sims (1986)による6変量VARモデルがあります（詳細はエンダース（2019）P.313を参照してください。ただしエンダース（2019）では本書でいう行列\\(\\boldsymbol{A}\\)を行列\\(\\boldsymbol{B}\\)を記載している点に注意してください）。モデルには変数が、短期金利（\\(r\\)）、貨幣供給量M1（\\(m\\)）、実質GNP（\\(y\\)）、GNPデフレータ（\\(p\\)）、失業率（\\(u\\)）、実質企業固定投資（\\(i\\)）の順序で含まれています。Sims (1986)では行列\\(\\boldsymbol{A}\\)に対し、 \\[ \\boldsymbol{A} = \\begin{bmatrix} 1 &amp; -a_{12,0} &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ -a_{21,0} &amp; 1 &amp; -a_{23,0} &amp; -a_{24,0} &amp; 0 &amp; 0 \\\\ -a_{31,0} &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; -a_{36,0} \\\\ -a_{41,0} &amp; 0 &amp; -a_{43,0} &amp; 1 &amp; 0 &amp; -a_{46,0} \\\\ -a_{51,0} &amp; 0 &amp; -a_{53,0} &amp; -a_{54,0} &amp; 1 &amp; -a_{56,0} \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] の制約が課されています。これは、変数間の同時点の関係を、 \\[ \\begin{aligned} r_t &amp;= a_{12,0} m_t + \\epsilon_{r,t} \\\\ m_t &amp;= a_{21,0} r_t + a_{23,} y_t + a_{24,0} p_t + \\epsilon_{m,t} \\\\ y_t &amp;= a_{31,0} r_t + a_{36,0} i_t + \\epsilon_{y,t} \\\\ p_t &amp;= a_{41,0} r_t + a_{43,0} y_t + a_{46,0} i_t + \\epsilon_{p,t} \\\\ u_t &amp;= a_{51,0} r_t + a_{53,0} y_t + a_{54,0} p_t + a_{56,0} i_t + \\epsilon_{p,t} \\\\ i_t &amp;= \\epsilon_{i,t} \\end{aligned} \\] と想定していることを意味します。なお、6変数VARモデルで識別に必要な制約の数は\\(6(6-1)/2 = 15\\)個ですが、Sims (1986)は経済理論との整合性を重視し、\\(\\boldsymbol{A}\\)行列に17個の制約を課しています。このように、必要以上の制約を課すことを過剰識別（overidentified）といいます。 長期制約 長期制約（long-run restriction）は、構造ショックが特定の変数に及ぼす累積的な効果が、長期的にみてゼロになるという制約です（宮尾（2006）P.24、北岡 他（2019）P.75、西山 他（2019）P.621）。長期制約は、金融政策や財政政策が実質GDPに対する長期的な効果をもたない、といった経済理論を表現するための制約として用いられます。 長期制約では、VARモデルを過去の誤差項の和で表すVMA表現（vector moving average representation）を用います。VARモデルがARモデルの多変数版であるのと同様に、MAモデルの多変数に拡張したものがVMA表現です。 \\(n\\)個の変数をもつラグ次数\\(p\\)の構造VARモデル\\(\\mathit{VAR}(p)\\)は、右辺のラグ項に逐次代入を繰り返すことで、誤差項の無限大期までのラグで説明される\\(\\mathit{VMA}(\\infty)\\)モデル、 \\[ \\boldsymbol{Y}_t = \\boldsymbol{D}_0 \\boldsymbol{\\epsilon}_t + \\boldsymbol{D}_1 \\boldsymbol{\\epsilon}_{t-1} + \\boldsymbol{D}_2 \\boldsymbol{\\epsilon}_{t-2} + \\cdots \\] と表すことができます（宮尾（2006）P.19）。\\(\\boldsymbol{Y}_t\\)は\\(n\\)個の変数ベクトル、\\(\\boldsymbol{D}_j\\)は\\(n \\times n\\)の係数行列、\\(\\boldsymbol{\\epsilon}_{t-j}\\)は\\(n\\)個の構造誤差項ベクトルです。例えば2変数であれば次のようになります。 \\[ \\begin{bmatrix} Y_{1,t} \\\\ Y_{2,t} \\end{bmatrix} = \\begin{bmatrix} d_{11,0} &amp; d_{12,0} \\\\ d_{21,0} &amp; d_{22,0} \\end{bmatrix} \\begin{bmatrix} \\epsilon_{1,t} \\\\ \\epsilon_{2,t} \\end{bmatrix} + \\begin{bmatrix} d_{11,1} &amp; d_{12,1} \\\\ d_{21,1} &amp; d_{22,1} \\end{bmatrix} \\begin{bmatrix} \\epsilon_{1,t-1} \\\\ \\epsilon_{2,t-1} \\end{bmatrix} + \\begin{bmatrix} d_{11,2} &amp; d_{12,2} \\\\ d_{21,2} &amp; d_{22,2} \\end{bmatrix} \\begin{bmatrix} \\epsilon_{1,t-2} \\\\ \\epsilon_{2,t-2} \\end{bmatrix} + \\cdots \\] ここで、表記を簡単にするために、ラグオペレータ\\(L\\)を導入します。ラグオペレータ\\(L\\)は\\(LY_t = Y_{t-1}\\)、\\(L^pY_t = Y_{t-p}\\)のようにラグ次数を変数として表す演算子です。特に、ある変数\\(x\\)を\\(L\\)の関数をして表すとき、 \\[ \\begin{aligned} x(L) y_t &amp;= (x_0 + x_1 L + x_2 L^2 + \\cdots) y_t \\\\ &amp;= x_0 y_t + x_1 y_{t-1} + x_2 y_{t-2} + \\cdots \\end{aligned} \\] であることを意味します。このラグオペレータ\\(L\\)を用いると、上の2変数VMAモデルは次のように表すことができます。 \\[ \\begin{aligned} \\begin{bmatrix} Y_{1,t} \\\\ Y_{2,t} \\end{bmatrix} &amp;= \\begin{bmatrix} d_{11}(L) &amp; d_{12}(L) \\\\ d_{21}(L) &amp; d_{22}(L) \\end{bmatrix} \\begin{bmatrix} \\epsilon_{1,t} \\\\ \\epsilon_{2,t} \\end{bmatrix} \\\\ \\\\ \\boldsymbol{Y}_t &amp;= \\boldsymbol{D}(L) \\boldsymbol{\\epsilon}_t \\end{aligned} \\] このモデルにおいて、変数\\(Y_2\\)の構造誤差項\\(\\epsilon_2\\)に生じた構造ショックが、変数\\(Y_1\\)に対し長期的に影響を及ぼさないと想定します（例えば\\(Y_2\\)が金融政策、\\(Y_1\\)が実質GDP）。\\(\\epsilon_2\\)の\\(Y_1\\)に対する効果を表す係数\\(d_{12}(L)\\)に\\(L=1\\)を代入すると、 \\[ \\begin{aligned} d_{12}(1) &amp;= d_{12,0} + d_{12,1} + d_{12,2} + \\cdots \\\\ &amp;= \\sum^{\\infty}_{j=0}{d_{12,j}} \\end{aligned} \\] となり、\\(d_{12}(1)\\)は\\(\\epsilon_2\\)の\\(Y_1\\)に対する毎期の効果を長期にわたり全て合計したものに相当するため、係数行列\\(\\boldsymbol{D}(L)\\)を、 \\[ D(1) = \\begin{bmatrix} d_{11}(1) &amp; 0 \\\\ d_{21}(1) &amp; d_{22}(1) \\end{bmatrix} \\] とすると、想定した長期制約を課すことができます。 8.8.5 構造VECモデル VARモデルと同様に、ベクトル誤差修正（VEC）モデルでも、経済構造に基づく制約を課して誘導型モデルから構造型モデルを識別することができます。これを構造ベクトル誤差修正（Structural VEC、SVEC）モデルといいます。 VECモデルの節で導出したラグ次数1の誘導VECモデル\\(\\mathit{VEC}(1)\\)、 \\[ \\Delta \\boldsymbol{Y}_t = \\boldsymbol{\\alpha}\\boldsymbol{\\beta&#39;}\\boldsymbol{Y}_{t-1} + \\boldsymbol{\\Gamma}_{1}\\Delta\\boldsymbol{Y}_{t-1} + \\boldsymbol{u}_t \\] に対応する構造VECモデルは、 \\[ \\Delta \\boldsymbol{Y}_t = \\boldsymbol{\\alpha}\\boldsymbol{\\beta&#39;}\\boldsymbol{Y}_{t-1} + \\boldsymbol{\\Gamma}_{1}\\Delta\\boldsymbol{Y}_{t-1} + \\boldsymbol{B}\\boldsymbol{\\epsilon}^{SD}_t \\] です。\\(\\boldsymbol{u}_t\\)は要素数\\(n\\)の誘導誤差項ベクトル、\\(\\boldsymbol{\\epsilon}^{SD}_t\\)は要素数\\(n\\)の標準化構造誤差項ベクトル、\\(\\boldsymbol{B}\\)は対角要素が構造誤差項の標準偏差\\(\\sigma_i\\)、非対角要素が0の\\(n \\times n\\)の行列です。誘導VECモデルと構造VECモデルは同じものを表しているため、誘導誤差項ベクトルと標準化構造誤差項ベクトルの間には次の関係が成立します。 \\[ \\boldsymbol{u}_t = \\boldsymbol{B}\\boldsymbol{\\epsilon}^{SD}_t \\] ここで、\\(\\mathit{MA}(\\infty)\\)モデルを短期変動と長期変動に2分割するベバリッジ＝ネルソン分解表現（Beveridge-Nelson decomposition、B-N分解）を用いると、誘導VECモデルは、 \\[ \\boldsymbol{Y}_t = \\Xi \\boldsymbol{B} \\sum^{t}_{i=1}{\\epsilon^{SD}_i} + \\sum^{\\infty}_{j=1}{\\Xi^*_j \\boldsymbol{B} \\epsilon^{SD}_{t-j}} + \\boldsymbol{Y}^*_0 \\] と表すことができます。\\(\\Xi\\)は共和分関係に対し直交関係にある\\(n \\times n\\)の行列、\\(\\Xi^*\\)は共和分関係を表現する\\(n \\times n\\)の行列、\\(\\boldsymbol{Y}^*_0\\)は\\(\\boldsymbol{Y}_t\\)の初期値に依存する項です。 このBeveridge-Nelson分解表現の各項が意味するのは次のとおりです。 \\(\\Xi \\boldsymbol{B} \\sum^{t}_{i=1}{\\epsilon^{SD}_i}\\)：共和分関係にない\\(I(1)\\)過程の、現在（\\(t\\)期）までの変動の合計に相当し、\\(\\boldsymbol{Y}_t\\)の恒久的な動きを表します。この項を共通トレンド項（common trend terms）と呼日ます。構造ショック\\(\\epsilon^{SD}_t\\)の恒久的な波及効果は行列\\(\\Xi \\boldsymbol{B}\\)によって決まり、\\(\\Xi \\boldsymbol{B}\\)を長期インパクト行列と呼びます。 \\(\\sum^{\\infty}_{j=1}{\\Xi^*_j \\boldsymbol{B} \\epsilon^{SD}_{t-j}}\\)：共和分関係にある\\(I(0)\\)過程の変動に相当し、\\(\\boldsymbol{Y}_t\\)の過渡的な動きを表します。構造ショック\\(\\epsilon^{SD}_t\\)の過渡的な波及効果は行列\\(\\boldsymbol{B}\\)によって決まり、\\(\\boldsymbol{B}\\)を短期インパクト行列と呼びます。 構造VECモデルでは、長期インパクト行列\\(\\Xi \\boldsymbol{B}\\)に加える制約が長期的な制約、短期インパクト行列\\(\\boldsymbol{B}\\)に加える制約が短期的な制約になります。\\(n\\)種類の変数をもつ誘導VECモデルから構造VECモデルを識別するには、VARモデルと同様に、\\(n(n-1)/2\\)個の制約を課す必要があります。 構造VECモデルの制約 村尾（2019）は構造VECモデルに\\(n(n-1)/2\\)個の制約を課すための方法論を提案しています（P.212）。 共和分関係を表現する長期制約 共和分関係が\\(r\\)個の場合、\\(r\\)個の変数に関する長期的な均衡関係があると考えられることから、長期インパクト行列\\(\\Xi \\boldsymbol{B}\\)にゼロからなる列を最大\\(r\\)列だけ設けます。\\(r\\)列のゼロ制約は\\(r(n-r)\\)個の独立した制約になるため、残りの必要な制約は\\(n(n-1)/2 - r(n-r)\\)個です。 恒久的ショックを識別する長期制約 恒久的なショックを識別するために、少なくとも\\((n-r)\\{(n-r)-1\\}/2\\)個の制約を長期インパクト行列\\(\\Xi \\boldsymbol{B}\\)に加えます。 過渡的ショックを識別する短期制約 過渡的なショックを識別するため、少なくとも\\(r(r-1)/2\\)個の制約を短期インパクト行列\\(\\boldsymbol{B}\\)に加えます。 1〜3の制約の合計は\\(r(n-r) + (n-r)\\{(n-r)-1\\}/2 + r(r-1)/2 = n(n-1)/2\\)になり、適度識別に必要な制約の個数に一致します。 ここでは構造VECモデルにおける制約の例として、労働生産性（\\(Y_{1,t}\\)）、雇用者数の自然対数（\\(Y_{2,t}\\)）、失業率（\\(Y_{3,t}\\)）、実質賃金の自然対数（\\(Y_{4,t}\\)）、の4変数構造VECモデルを考えます。4個の内生変数は全て\\(I(1)\\)過程であり、共和分関係は1個であるとします。識別に必要な制約は\\(4(4-1)/2=6\\)個です。 まず、4変数間に存在する1個の共和分関係が、どの変数に関する均衡を表現しているか考えます。この例では、実質賃金の自然対数\\(Y_{4}\\)に関する均衡であり、したがって実質賃金方程式（第4方程式）の標準化構造誤差項\\(\\epsilon^{SD}_4\\)は恒久的インパクトをもっていないと想定されることから、長期インパクト行列\\(\\Xi \\boldsymbol{B}\\)の第4列の要素を全てゼロにします。これにより、\\(1(4-1) = 3\\)個の独立な制約を課したことになります。残りの必要な制約数は\\(6-3=3\\)個です。 \\[ \\Xi \\boldsymbol{B} = \\begin{bmatrix} * &amp; * &amp; * &amp; 0 \\\\ * &amp; * &amp; * &amp; 0 \\\\ * &amp; * &amp; * &amp; 0 \\\\ * &amp; * &amp; * &amp; 0 \\end{bmatrix} \\] 次に、恒久的なショックを識別するために、長期インパクト行列\\(\\Xi \\boldsymbol{B}\\)へ制約を追加します。ここでは、労働生産性\\(Y_1\\)が労働生産性の構造ショック\\(\\epsilon^{SD}_1\\)からのみ恒久的インパクトを受け、他の変数の構造ショック（\\(\\epsilon^{SD}_2\\)、\\(\\epsilon^{SD}_3\\)、\\(\\epsilon^{SD}_4\\)）からは恒久的インパクトを受けないと想定します。この想定は、長期インパクト行列\\(\\Xi \\boldsymbol{B}\\)の\\((1,2)\\)、\\((1,3)\\)、\\((1,4)\\)要素をゼロにすることで表現できます。これによる追加的な制約数は\\(2\\)個であり、残りの必要な制約数は\\(1\\)個です。 \\[ \\Xi \\boldsymbol{B} = \\begin{bmatrix} * &amp; 0 &amp; 0 &amp; 0 \\\\ * &amp; * &amp; * &amp; 0 \\\\ * &amp; * &amp; * &amp; 0 \\\\ * &amp; * &amp; * &amp; 0 \\end{bmatrix} \\] 最後に、過渡的ショックを識別するために、短期インパクト行列\\(\\boldsymbol{B}\\)へ制約を追加します。ここでは、実質賃金に関する硬直性の理論に基づき、実質賃金\\(Y_4\\)は同時点における雇用者数の構造ショック\\(\\epsilon^{SD}_2\\)から影響を受けない（雇用者数の増減は同時点の実質賃金に影響しない）と想定します。この想定は、短期インパクト行列\\(\\boldsymbol{B}\\)の\\((2,4)\\)要素をゼロにすることで表現できます。これによる追加的な制約数は\\(1\\)個であり、残りの必要な制約数は\\(0\\)個になります（適度識別）。 \\[ \\boldsymbol{B} = \\begin{bmatrix} * &amp; * &amp; * &amp; * \\\\ * &amp; * &amp; * &amp; * \\\\ * &amp; * &amp; * &amp; * \\\\ * &amp; 0 &amp; * &amp; * \\end{bmatrix} \\] 8.9 VARモデルに関連する分析 8.9.1 グレンジャー因果性 グレンジャー因果性（Granger causality）は、予測精度の改善という観点から、時系列データの因果関係を評価する手法です（西山 他（2019）P.584）。 グレンジャー因果性では、「原因は結果に先立って起こる」、「原因には結果に関するユニークな情報が含まれている」という2つのアイデアをもとにして、背景の経済理論を必要とせずに時系列データのみから因果関係の有無を判断します（村尾（2019）P.369）。 具体例として、ある事象の原因（cause）を表すと考えられる変数\\(X_t\\)、結果を表すと考えられる変数\\(Y_t\\)、に関する次のモデルを考えます。 \\[ \\begin{aligned} Y_t &amp;= \\phi_1Y_{t-1} \\quad \\quad \\quad \\quad \\; + u_{1,t} \\\\ Y_t &amp;= \\phi_1Y_{t-1} + \\phi_2X_{t-1} + u_{2,t} \\end{aligned} \\] ここで、\\(u_1\\)は過去の結果変数\\(Y_t\\)だけを使って\\(Y_t\\)を予測したときの残差、\\(u_2\\)は過去の原因変数\\(X_t\\)も使って\\(Y_t\\)を予測したときの残差です。この2つの予測誤差の残差平方和（Mean Squared Error、MSE）を比較し、原因変数\\(X_t\\)を予測に使うことで予測誤差が有意に小さくなったかを検定するのが、グレンジャー因果性検定です。 グレンジャー因果性検定では、原因変数\\(X_t\\)と結果変数\\(Y_t\\)を含むVARモデルを構築し、「結果変数の式における原因変数の係数がゼロ」との帰無仮説を検定する方法で行います。したがって、帰無仮説が棄却できれば原因変数\\(X_t\\)は結果変数\\(Y_t\\)に対しグレンジャー因果性をもつと判断されます。一方、帰無仮説が棄却できない場合はグレンジャー因果性をもつとは判断できず、結果変数\\(Y_t\\)は原因変数\\(X_t\\)に対し外生的であることが示唆されます。 グレンジャー瞬間的因果性 グレンジャー瞬間的因果性（instantaneous Granger causality）は、グレンジャー因果性の発展形です。モデルに変数\\(X_t\\)の予測値\\(X_{t+1}\\)を追加した場合に、変数\\(Y_{t}\\)の予測値\\(Y_{t+1}\\)の予測誤差（MSE）が小さくなると、変数\\(X\\)から変数\\(Y\\)へのグレンジャー瞬間的因果性があるといいます。 グレンジャー瞬間的因果性は、同時点における変数の影響を検定したものです。2変数のVARモデルにおいて、グレンジャー瞬間的因果性がないことの必要十分条件は、同時点における残差同士の共分散が0であることが知られています（馬場（2018）P.157）。したがって、2つの変数の間にグレンジャー瞬間的因果性があれば、VARモデルの2つの式の残差同士に関連があるといえます。 グレンジャー瞬間的因果性検定もグレンジャー因果性検定と同様に、「変数\\(X\\)から変数\\(Y\\)へのグレンジャー瞬間的因果性がない」との帰無仮説を検定します。帰無仮説が棄却できれば、変数\\(X\\)は変数\\(Y\\)に対しグレンジャー瞬間的因果性をもつと判断されます。 ブロック外生性 グレンジャー因果性は、VARモデルにおける変数選択の指針として使用することができます。 いま、2つの変数\\(X_t\\)と\\(Y_t\\)からなるVARモデルを構築し、そこに変数\\(Z_t\\)を新たに加えるか検討しているとします。グレンジャー因果性検定を行い、\\(Z_t\\)が\\(X_t\\)か\\(Y_t\\)の少なくともどちらか一方にグレンジャー因果性をもつと判断できれば、\\(Z_t\\)はこのVARモデルに加えるべきです（エンダース（2019）P.293）。 一方、\\(Z_t\\)が\\(X_t\\)と\\(Y_t\\)のどちらに対してもグレンジャー因果性をもつと判断できない場合（厳密には、\\(X_t\\)と\\(Y_t\\)の式で\\(Z_t\\)のすべてのラグの係数が0になる場合）、\\(Z_t\\)はブロック外生的であると言います。こうしたケースでは、\\(Z_t\\)は将来の\\(X_t\\)と\\(Y_t\\)に影響を与えないため、\\(Z_t\\)はこのVARモデルに加えるべきではありません。 このように、VARモデルにおいて、ある変数が他のどの変数に対してもグレンジャー因果性をもっていないことをブロック外生性（block-exogeneity）と言います。 グレンジャー因果性の注意点 なお、グレンジャー因果性を用いて分析する際は、次の点に注意してください。 グレンジャー因果性検定が使用できるのは、定常な時系列データのみです（馬場（2018）P.147）。 原因と結果の変数両方に影響する第3の変数がある場合は、原因・結果変数のグレンジャー因果性が有意であったとしても、真の因果関係はありません。例えば、原因：津波警報と結果：津波（真の原因は地震）、原因：雷光と結果：雷鳴（真の原因は落雷現象）などです（村尾（2019）P.372）。 VARモデルに他の変数を追加すると、グレンジャー因果性検定の結果が変わります。特に、結果の変数と相関の強い第3の変数をモデルに追加すると、原因の変数のグレンジャー因果性は低下します（村尾（2019）P.372）。 グレンジャー因果性検定の結果をそのまま政策判断に用いるべきではありません。政策手段である原因の変数が内生変数の場合（例えば貨幣供給量や名目金利など）、原因の変数を予測する回帰式の誤差項には政策変化以外の要因が含まれており、純粋な政策ショックの効果を捉えることができません。政策ショックの影響を分析するには、構造VARモデルに基づくインパルス応答を用いる必要があります（西山 他（2019）P.590）。 8.9.2 インパルス応答 インパルス応答（impulse response）は、VARモデルのある変数に急な変動（ショック）が発生した場合に、他の変数へ時間を通じてどのような影響が生じるかを定量的に評価する手法です。 インパルス応答では、「構造VARモデル」に記載したように、誘導VARモデルから構造VARモデルをいかに識別するかが重要になります。インパルス応答を引き起こすショックは変数の独立な構造ショック（構造VARモデルの誤差項）\\(\\epsilon_{t}\\)ですが、構造ショック\\(\\epsilon_{t}\\)は直接推定できず、誘導VARモデルの誤差項\\(u_{t}\\)の推定値を使って再現（識別）する必要があるためです。識別の際に、どのような制約を構造VARモデルに課すかでインパルス応答の結果が変わるため、制約が経済構造の実態を反映したものか十分検討する必要があります。 3つの変数\\(Y_{1,t}\\)、\\(Y_{2,t}\\)、\\(Y_{3,t}\\)からなるラグ次数1の構造VARモデルを考えます。構造VARモデルは誘導VARモデルと異なり、各式の右辺に左辺と同時点の項が含まれており、経済構造に基づいて変数の同時点間の相互関係を考慮する形状になっています。 \\[ \\begin{aligned} Y_{1,t} &amp;= a_{10} + a_{12,0}Y_{2,t} + a_{13,0}Y_{3,t} + a_{11,1}Y_{1,t-1} + a_{12,1}Y_{2,t-1} + a_{13,1}Y_{3,t-1} + \\epsilon_{1,t} \\\\ Y_{2,t} &amp;= a_{20} + a_{21,0}Y_{1,t} + a_{23,0}Y_{3,t} + a_{21,1}Y_{1,t-1} + a_{22,1}Y_{2,t-1} + a_{23,1}Y_{3,t-1} + \\epsilon_{2,t} \\\\ Y_{3,t} &amp;= a_{30} + a_{31,0}Y_{1,t} + a_{32,0}Y_{2,t} + a_{31,1}Y_{1,t-1} + a_{32,1}Y_{2,t-1} + a_{33,1}Y_{3,t-1} + \\epsilon_{3,t} \\end{aligned} \\] 右辺にある同時点の項を左辺に移項し、行列表示すると、この構造VARモデルは次のように表すことができます。なお、構造誤差項\\(\\epsilon_{i,t}\\)は、構造誤差項の標準偏差\\(\\sigma_i\\)と、平均0、分散1になるよう標準化した標準化構造誤差項\\(\\epsilon^{SD}_{i,t}\\)に分解しています。 \\[ \\begin{aligned} \\begin{bmatrix} 1 &amp; -a_{12,0} &amp; -a_{13,0} \\\\ -a_{21,0} &amp; 1 &amp; -a_{23,0} \\\\ -a_{31,0} &amp; -a_{32,0} &amp; 1 \\end{bmatrix} \\begin{bmatrix} Y_{1,t} \\\\ Y_{2,t} \\\\ Y_{3,t} \\end{bmatrix} &amp;= \\begin{bmatrix} a_{10} \\\\ a_{20} \\\\ a_{30} \\end{bmatrix} + \\begin{bmatrix} a_{11,1} &amp; a_{12,1} &amp; a_{13,1} \\\\ a_{21,1} &amp; a_{22,1} &amp; a_{23,1} \\\\ a_{31,1} &amp; a_{32,1} &amp; a_{33,1} \\end{bmatrix} \\begin{bmatrix} Y_{1,t-1} \\\\ Y_{2,t-1} \\\\ Y_{3,t-1} \\end{bmatrix} + \\begin{bmatrix} \\sigma_{1} &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_{2} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{3} \\end{bmatrix} \\begin{bmatrix} \\epsilon^{SD}_{1,t} \\\\ \\epsilon^{SD}_{2,t} \\\\ \\epsilon^{SD}_{3,t} \\end{bmatrix} \\\\ \\\\ \\boldsymbol{A} \\boldsymbol{Y}_t &amp;= \\boldsymbol{A}_0 + \\boldsymbol{A}_1 \\boldsymbol{Y}_{t-1} + \\boldsymbol{B} \\boldsymbol{\\epsilon}^{SD}_t \\end{aligned} \\] この構造VARモデルの両辺に、\\(\\boldsymbol{A}\\)の逆行列\\(\\boldsymbol{A}^{-1}\\)を乗じると、推計可能な誘導VARモデルに変換することができます。 \\[ \\boldsymbol{Y}_t = \\boldsymbol{A}^{-1} \\boldsymbol{A}_0 + \\boldsymbol{A}^{-1} \\boldsymbol{A}_1 \\boldsymbol{Y}_{t-1} + \\boldsymbol{A}^{-1} \\boldsymbol{B} \\boldsymbol{\\epsilon}^{SD}_t \\] 一方、対応するラグ次数1の誘導VARモデルを、 \\[ \\begin{aligned} Y_{1,t} &amp;= b_{10} + b_{11,1}Y_{1,t-1} + b_{12,1}Y_{2,t-1} + b_{13,1}Y_{3,t-1} + u_{1,t} \\\\ Y_{2,t} &amp;= b_{20} + b_{21,1}Y_{1,t-1} + b_{22,1}Y_{2,t-1} + b_{23,1}Y_{3,t-1} + u_{2,t} \\\\ Y_{3,t} &amp;= b_{30} + b_{31,1}Y_{1,t-1} + b_{32,1}Y_{2,t-1} + b_{33,1}Y_{3,t-1} + u_{3,t} \\\\ \\\\ \\begin{bmatrix} Y_{1,t} \\\\ Y_{2,t} \\\\ Y_{3,t} \\end{bmatrix} &amp;= \\begin{bmatrix} b_{10} \\\\ b_{20} \\\\ b_{30} \\end{bmatrix} + \\begin{bmatrix} b_{11,1} &amp; b_{12,1} &amp; b_{13,1} \\\\ b_{21,1} &amp; b_{22,1} &amp; b_{23,1} \\\\ b_{31,1} &amp; b_{32,1} &amp; b_{33,1} \\end{bmatrix} \\begin{bmatrix} Y_{1,t-1} \\\\ Y_{2,t-1} \\\\ Y_{3,t-1} \\end{bmatrix} + \\begin{bmatrix} u_{1,t} \\\\ u_{2,t} \\\\ u_{3,t} \\end{bmatrix} \\\\ \\\\ \\boldsymbol{Y}_t &amp;= \\boldsymbol{B}_0 + \\boldsymbol{B}_1 \\boldsymbol{Y}_{t-1} + \\boldsymbol{u}_t \\end{aligned} \\] とすると、各項は次のように対応します。 \\[ \\boldsymbol{A}^{-1} \\boldsymbol{A}_0= \\boldsymbol{B}_0, \\quad \\boldsymbol{A}^{-1} \\boldsymbol{A}_1 = \\boldsymbol{B}_1, \\quad \\boldsymbol{A}^{-1} \\boldsymbol{B} \\boldsymbol{\\epsilon}^{SD}_t = \\boldsymbol{u}_t \\] ラグ次数1の誘導VARモデル\\(\\boldsymbol{Y}_t = \\boldsymbol{B}_0 + \\boldsymbol{B}_1 \\boldsymbol{Y}_{t-1} + \\boldsymbol{u}_t\\)は、右辺の\\(\\boldsymbol{Y}_{t-1}\\)に1期前の式を逐次代入していくと、次のように\\(\\boldsymbol{Y}_t\\)を過去の誘導誤差項の和で表すVMA表現に変換できます（エンダース（2019）P.269、P.278、村尾（2019）P.251）。ここで、\\(\\overline{\\boldsymbol{Y}}\\)は\\((\\boldsymbol{I} + \\boldsymbol{A}_1 + \\boldsymbol{A}^2_1 + \\cdots + \\boldsymbol{A}^n_1)\\boldsymbol{A}_0\\)の極限であり、要素数3の定数ベクトルになります。 \\[ \\boldsymbol{Y}_t = \\overline{\\boldsymbol{Y}} + \\sum^{\\infty}_{s=0}{(\\boldsymbol{B}^s_1 \\boldsymbol{u}_{t-s})} \\] この式に\\(\\boldsymbol{u}_t = \\boldsymbol{A}^{-1} \\boldsymbol{B} \\boldsymbol{\\epsilon}^{SD}_t\\)を代入すると、 \\[ \\begin{aligned} \\boldsymbol{Y}_t &amp;= \\overline{\\boldsymbol{Y}} + \\sum^{\\infty}_{s=0}{(\\boldsymbol{B}^s_1 \\boldsymbol{A}^{-1} \\boldsymbol{B} \\boldsymbol{\\epsilon}^{SD}_{t-s})} \\\\ \\\\ \\begin{bmatrix} Y_{1,t} \\\\ Y_{2,t} \\\\ Y_{3,t} \\end{bmatrix} &amp;= \\begin{bmatrix} \\overline{Y_1} \\\\ \\overline{Y_2} \\\\ \\overline{Y_3} \\end{bmatrix} + \\sum^{\\infty}_{s=0} { \\begin{bmatrix} b_{11,1} &amp; b_{12,1} &amp; b_{13,1} \\\\ b_{21,1} &amp; b_{22,1} &amp; b_{23,1} \\\\ b_{31,1} &amp; b_{32,1} &amp; b_{33,1} \\end{bmatrix}^s \\begin{bmatrix} 1 &amp; -a_{12,0} &amp; -a_{13,0} \\\\ -a_{21,0} &amp; 1 &amp; -a_{23,0} \\\\ -a_{31,0} &amp; -a_{32,0} &amp; 1 \\end{bmatrix}^{-1} \\begin{bmatrix} \\sigma_{1} &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_{2} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{3} \\end{bmatrix} \\begin{bmatrix} \\epsilon^{SD}_{1,t-s} \\\\ \\epsilon^{SD}_{2,t-s} \\\\ \\epsilon^{SD}_{3,t-s} \\end{bmatrix} } \\end{aligned} \\] となります。この\\(3 \\times 3\\)の係数行列\\(\\boldsymbol{B}^j_1 \\boldsymbol{A}^{-1} \\boldsymbol{B}\\)を、 \\[ \\begin{aligned} \\boldsymbol{IRF}_s &amp;= \\boldsymbol{B}^s_1 \\boldsymbol{A}^{-1} \\boldsymbol{B} \\\\ \\begin{bmatrix} \\mathit{IRF}_{11}(s) &amp; \\mathit{IRF}_{12}(s) &amp; \\mathit{IRF}_{13}(s) \\\\ \\mathit{IRF}_{21}(s) &amp; \\mathit{IRF}_{22}(s) &amp; \\mathit{IRF}_{23}(s) \\\\ \\mathit{IRF}_{31}(s) &amp; \\mathit{IRF}_{32}(s) &amp; \\mathit{IRF}_{33}(s) \\end{bmatrix} &amp;= \\begin{bmatrix} b_{11,1} &amp; b_{12,1} &amp; b_{13,1} \\\\ b_{21,1} &amp; b_{22,1} &amp; b_{23,1} \\\\ b_{31,1} &amp; b_{32,1} &amp; b_{33,1} \\end{bmatrix}^s \\begin{bmatrix} 1 &amp; -a_{12,0} &amp; -a_{13,0} \\\\ -a_{21,0} &amp; 1 &amp; -a_{23,0} \\\\ -a_{31,0} &amp; -a_{32,0} &amp; 1 \\end{bmatrix}^{-1} \\begin{bmatrix} \\sigma_{1} &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_{2} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{3} \\end{bmatrix} \\end{aligned} \\] と表すと、\\(\\mathit{IRF}_{ij}(s)\\)は、\\(t\\)期の\\(s\\)期前（\\(t-s\\)期）に発生した変数\\(Y_{j,t-s}\\)の（標準化）構造ショック\\(\\epsilon^{SD}_{j,t-s}\\)が、\\(t\\)期の変数\\(Y_{i,t}\\)に及ぼす影響を表します。 このように、\\(t-s\\)期に変数\\(Y_{j,t-s}\\)に発生した構造ショック\\(\\epsilon_{j,t-s}\\)が\\(t\\)期の変数\\(Y_{i,t}\\)に及ぼす影響を、\\(Y_{i,t}\\)の\\(Y_{j,t-s}\\)に対するインパルス応答と呼び、インパルス応答の大きさを表す関数\\(\\mathit{IRF}_{ij}(s)\\)をインパルス応答関数（impulse response function、IRF）と呼びます。インパルス応答関数\\(\\mathit{IRF}_{ij}(s)\\)は衝撃乗数とも呼ばれます（エンダース（2019）P.279）。また、インパルス応答関数を束ねた行列\\(\\boldsymbol{IRF}_s\\)の中の\\(\\boldsymbol{B^s_1}\\)はイノベーション係数行列と呼ばれることがあります（村尾（2019）P.253）。 なお、上記のインパルス応答関数\\(\\mathit{IRF}_{ij}(s)\\)やイノベーション係数行列\\(\\boldsymbol{B^s_1}\\)は、ラグ次数1次のVARモデル\\(\\mathit{VAR}(1)\\)の場合の特殊な列です（そのため、上記のイノベーション係数行列は、誘導VARモデルのラグ次数1次の係数行列\\(\\boldsymbol{B_1}\\)のみで構成されています）。ラグ次数を\\(p\\)に拡張したインパルス応答関数については、村尾（2019）P.251を参照してください。 さて、ここで問題になるのは、\\(\\mathit{IRF}_{ij}(s)\\)の一部である（構造VARモデルに由来する）行列\\(\\boldsymbol{A}\\)に含まれるパラメータ（\\(a_{12,0}\\)、\\(a_{13,0}\\)、\\(a_{21,0}\\)、\\(a_{23,0}\\)、\\(a_{31,0}\\)、\\(a_{32,0}\\)）が未知であることです。 そこで、これらの構造VARモデルの未知パラメータを、誘導VARモデルの推定結果から求める必要があります。しかし、いま考えている3変数1次ラグの構造VARモデルには合計21個（係数18個、構造誤差項の分散3個）の未知パラメータがある一方、対応する誘導VARモデルには合計18個（係数12個、誘導誤差項の分散・共分散各3個）のパラメータしかなく、そのままでは構造VARモデルの未知パラメータを求めることができません。推定した誘導VARモデルのパラメータから構造VARモデルの未知パラメータを求め（構造VARモデルを識別し）、インパルス応答関数を導出するためには、構造VARモデルのパラメータに少なくとも\\(n(n-1)/2\\)（\\(n\\)は変数の種類）個の制約を課す必要があります。したがって、3変数VARモデルであれば、必要な制約は\\(3(3-1)/2 = 3\\)個です。 構造VARモデルにどのような制約を課すかによって、インパルス応答は次のような種類に分けることができます。 非直交化インパルス応答 最も単純な制約は、行列\\(\\boldsymbol{A}\\)の非対角要素である未知パラメータ（\\(a_{12,0}\\)、\\(a_{13,0}\\)、\\(a_{21,0}\\)、\\(a_{23,0}\\)、\\(a_{31,0}\\)、\\(a_{32,0}\\)）をすべて0にすることです。これは、行列\\(\\boldsymbol{A}\\)を単位行列\\(\\boldsymbol{I}\\)（対角要素が1、非対角要素が0）に置き換えることに相当します。 行列\\(\\boldsymbol{A}\\)の6個の未知パラメータは、構造VARモデルにおいて変数\\(Y_{1,t}\\)、\\(Y_{2,t}\\)、\\(Y_{3,t}\\)の同時点間の関係を表す係数です。したがって、この制約は変数に同時点間の関係がない経済構造を想定していることを意味します。このように同時点無相関の制約を課したインパルス応答は、非直交化インパルス応答（non-orthogonal impulse response）と呼ばれます。EViewsではImpulse DefinitionのDecomposition MethodでResidualを選択すると、同時点無相関の制約を課すことができます。 同時点無相関の制約は非常に強い仮定であり、適用できるモデルが限定されます。例えばGDPと政府支出の変数でVARモデルを構築する場合、政府支出に生じたショック（拡張的財政政策など）は同じ期のGDPに影響するため、同時点無相関の制約を課すことは適切ではありません。 なお、この制約では構造VARモデルの右辺にある同時点の項が消えるため、構造VARモデルは誘導VARモデルに一致し、推定した誘導誤差項\\(u_{i,t}\\)をそのまま構造誤差項\\(\\epsilon_{i,t}\\)として使用することができます。 行列\\(\\boldsymbol{A}\\)の非対角要素を0とおくと、インパルス応答関数\\(\\mathit{IRF}_{ij}(s)\\)は次のように表すことができます。 \\[ \\begin{aligned} \\boldsymbol{IRF}_s &amp;= \\boldsymbol{B}^s_1 \\boldsymbol{I}^{-1} \\boldsymbol{B} \\\\ \\begin{bmatrix} \\mathit{IRF}_{11}(s) &amp; \\mathit{IRF}_{12}(s) &amp; \\mathit{IRF}_{13}(s) \\\\ \\mathit{IRF}_{21}(s) &amp; \\mathit{IRF}_{22}(s) &amp; \\mathit{IRF}_{23}(s) \\\\ \\mathit{IRF}_{31}(s) &amp; \\mathit{IRF}_{32}(s) &amp; \\mathit{IRF}_{33}(s) \\end{bmatrix} &amp;= \\begin{bmatrix} b_{11,1} &amp; b_{12,1} &amp; b_{13,1} \\\\ b_{21,1} &amp; b_{22,1} &amp; b_{23,1} \\\\ b_{31,1} &amp; b_{32,1} &amp; b_{33,1} \\end{bmatrix}^s \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}^{-1} \\begin{bmatrix} \\sigma_{1} &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_{2} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{3} \\end{bmatrix} \\\\ \\\\ &amp;= \\begin{bmatrix} b_{11,1} &amp; b_{12,1} &amp; b_{13,1} \\\\ b_{21,1} &amp; b_{22,1} &amp; b_{23,1} \\\\ b_{31,1} &amp; b_{32,1} &amp; b_{33,1} \\end{bmatrix}^s \\begin{bmatrix} \\sigma_{1} &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_{2} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{3} \\end{bmatrix} \\end{aligned} \\] ここから、例えば、\\(Y_{1,t}\\)の\\(Y_{1,t}\\)に対するインパルス応答（\\(\\epsilon^{SD}_{1,t}\\)の変化が\\(Y_{1_t}\\)に及ぼす影響）は\\(\\mathit{IRF}_{11}(0) = \\sigma_1\\)と計算できます（なお、\\(s=0\\)のとき\\(\\boldsymbol{B}^s_1 = \\boldsymbol{I}\\)になります）。一方、\\(Y_{1,t}\\)の\\(Y_{2,t}\\)に対するインパルス応答（\\(\\epsilon^{SD}_{2,t}\\)の変化が\\(Y_{1_t}\\)に及ぼす影響）は\\(\\mathit{IRF}_{12}(0) = 0\\)であり、他の変数に対する同時点のインパルス応答は必ず0になることが確認できます。 直交化インパルス応答 次に、非直交化インパルス応答で想定した「同時点無相関の制約」より柔軟な制約として、「構造VAR」の「短期制約：再帰的制約」（コレスキー分解による識別）と同じ制約を課したインパルス応答を求めます。再帰的制約を課したインパルス応答は、直交化インパルス応答（orthogonal impulse response）と呼ばれます。通常、インパルス応答と言えばこの直交化インパルス応答を指します。EViewsではImpulse DefinitionのDecomposition MethodでCholeskyを選択すると、再帰的制約を課した直交化インパルス応答を求めることができます。 再帰的制約では次のように、行列\\(\\boldsymbol{A}\\)の右上三角部分を0と置く制約を課します。 \\[ \\begin{aligned} \\boldsymbol{IRF}_s &amp;= \\boldsymbol{B}^s_1 \\boldsymbol{A}^{-1} \\boldsymbol{B} \\\\ \\begin{bmatrix} \\mathit{IRF}_{11}(s) &amp; \\mathit{IRF}_{12}(s) &amp; \\mathit{IRF}_{13}(s) \\\\ \\mathit{IRF}_{21}(s) &amp; \\mathit{IRF}_{22}(s) &amp; \\mathit{IRF}_{23}(s) \\\\ \\mathit{IRF}_{31}(s) &amp; \\mathit{IRF}_{32}(s) &amp; \\mathit{IRF}_{33}(s) \\end{bmatrix} &amp;= \\begin{bmatrix} b_{11,1} &amp; b_{12,1} &amp; b_{13,1} \\\\ b_{21,1} &amp; b_{22,1} &amp; b_{23,1} \\\\ b_{31,1} &amp; b_{32,1} &amp; b_{33,1} \\end{bmatrix}^s \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ -a_{21,0} &amp; 1 &amp; 0 \\\\ -a_{31,0} &amp; -a_{32,0} &amp; 1 \\end{bmatrix}^{-1} \\begin{bmatrix} \\sigma_{1} &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_{2} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{3} \\end{bmatrix} \\end{aligned} \\] すると、同時点間（\\(s=0\\)）のインパルス応答関数は、\\(\\boldsymbol{A}\\)の逆行列の計算結果、 \\[ \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ -a_{21,0} &amp; 1 &amp; 0 \\\\ -a_{31,0} &amp; -a_{32,0} &amp; 1 \\end{bmatrix}^{-1} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ a_{21,0} &amp; 1 &amp; 0 \\\\ a_{31,0} + a_{21,0}a_{32,0} &amp; a_{32,0} &amp; 1 \\end{bmatrix} \\] を使うと、それぞれ次のように求められます。なお、\\(s=0\\)のとき\\(\\boldsymbol{B}^s_1 = \\boldsymbol{I}\\)になります。 \\[ \\begin{aligned} \\mathit{IRF}_{11}(0) &amp;= \\sigma_1 \\\\ \\mathit{IRF}_{12}(0) &amp;= 0 \\\\ \\mathit{IRF}_{13}(0) &amp;= 0 \\\\ \\\\ \\mathit{IRF}_{21}(0) &amp;= a_{21,0} \\sigma_1 \\\\ \\mathit{IRF}_{22}(0) &amp;= \\sigma_2 \\\\ \\mathit{IRF}_{23}(0) &amp;= 0 \\\\ \\\\ \\mathit{IRF}_{31}(0) &amp;= (a_{31,0} + a_{21,0}a_{32,0}) \\sigma_1 \\\\ \\mathit{IRF}_{32}(0) &amp;= a_{32,0} \\sigma_2 \\\\ \\mathit{IRF}_{33}(0) &amp;= \\sigma_3 \\end{aligned} \\] ここから分かるように、再帰的制約を課した直交化インパルス応答は、次のように変数の順序によってインパルス応答関数の形状が異なる非対称性があります。 \\(Y_{1,t}\\)は同時点の\\(Y_{2,t}\\)、\\(Y_{3,t}\\)から影響を受けない（\\(\\mathit{IRF}_{12}(0)=0\\)、\\(\\mathit{IRF}_{13}(0)=0\\)）。 \\(Y_{2,t}\\)は同時点の\\(Y_{1,t}\\)から影響を受けるが（\\(\\mathit{IRF}_{21}(0) = a_{21,0} \\sigma_1\\)）、\\(Y_{3,t}\\)から影響を受けない（\\(\\mathit{IRF}_{13}(0)=0\\)）。 \\(Y_{3,t}\\)は同時点の\\(Y_{1,t}\\)、\\(Y_{2,t}\\)から影響を受ける（\\(\\mathit{IRF}_{31}(0) = (a_{31,0} + a_{21,0}a_{32,0}) \\sigma_1\\)、\\(\\mathit{IRF}_{32}(0) = a_{32,0} \\sigma_2\\)）。 これは、「順序が先の変数（ここでは\\(Y_{1}\\)）ほど外生性が強く（他変数に影響するが自分は他変数の影響を受けない）、順序が後の変数（ここでは\\(Y_{3}\\)）ほど内生性が強い（他変数の影響を受けるが自分は他変数に影響しない）」と暗黙理に仮定していることを意味します。実際のデータがこの仮定通りであれば問題ありませんが、仮定と異なっていれば、インパルス応答を誤って導出することになります。 直交化インパルス応答における変数の順序付けの影響を確認には、変数の順序を変えて複数のインパルス応答を計算し、結果を比較します。インパルス応答の形状が著しく異なっている場合は、どの順序が経済構造に照らし合わせて適切か判断する必要があります。また、より柔軟な制約として、同時点無相関の制約や再帰的制約以外の制約を構造VARに課したり、変数の順序に左右されない一般化インパルス応答の活用を検討します。 構造VARに基づくインパルス応答 同時点無相関の制約を課した非直交化インパルス応答や、変数の順序に基づき機械的に同時点間の関係を決める直交化インパルス応答が、経済理論に照らし合わせて不自然な結果になる場合は、「構造VARモデル」の「短期制約：非再帰的制約」のように経済理論に基づいた柔軟な制約を課す必要があります。 非再帰的制約の具体例は、「構造VARモデル」の「短期制約：非再帰的制約」を参照してください。制約を考える際は、変数の種類\\(n\\)に対して、少なくとも\\(n(n-1)/2\\)個の制約を\\(\\boldsymbol{A}\\)行列に置く必要があることに注意します。 一般化インパルス応答 ここまで紹介した非直交化インパルス応答、直交化インパルス応答、構造VARモデルに基づくインパルス応答は、全て構造VARモデルの同時点の関係を表す係数（行列\\(\\boldsymbol{A}\\)）に特定の制約を課して誘導VARモデルから構造VARモデルを識別し、インパルス応答を計算する仕組みでした。 一般化インパルス応答（generalized impulse response）は、それらとは全く異なり、構造VARモデルに特定の制約を課さずにインパルス応答を計算する手法です（村尾（2019）P.265）。 任意の変数\\(Y_{*,t}\\)の\\(Y_{j,t-s}\\)に対するインパルス応答関数を\\(\\mathit{IRF}_{*j}(s)\\)とすると、これまで紹介したインパルス応答関数\\(\\mathit{IRF}^O_{*j}(s)\\)は次のように表すことができます（ラグ次数1次のVARモデル\\(VAR(1)\\)の場合）。なお、\\(\\boldsymbol{e}_j\\)は変数\\(Y_j\\)の標準化構造誤差項\\(\\epsilon^{SD}_j\\)の単位ショックを格納した列ベクトルであり、例えば3変数VARモデルにおける\\(j=2\\)の単位ショックベクトルは\\(\\boldsymbol{e}_2=(0,1,0)&#39;\\)です（村尾（2019）P.261）。 \\[ \\mathit{IRF}^O_{*j}(s) = \\boldsymbol{B}^s_1 \\boldsymbol{A}^{-1} \\boldsymbol{B} \\boldsymbol{e}_j \\] 一方、ラグ次数1次のVARモデル\\(VAR(1)\\)の場合の一般化インパルス応答関数\\(\\mathit{IRF}^G_{*j}(s)\\)は、\\(\\boldsymbol{A}^{-1} \\boldsymbol{B}\\)を\\(\\boldsymbol{\\Sigma}_u / \\sqrt{\\sigma_{jj}}\\)で置き換えた形になっています（村尾（2019）P.266）。\\(\\boldsymbol{\\Sigma}_u\\)は誘導誤差項\\(\\boldsymbol{u}_t\\)の分散共分散行列、\\(\\sqrt{\\sigma_{jj}}\\)は変数\\(Y_{j,t}\\)の誘導誤差項\\(u_{j,t}\\)の標準偏差（分散\\(\\sigma_{jj}\\)の平方根）です。 \\[ \\mathit{IRF}^G_{*j}(s) = \\boldsymbol{B}^s_1 \\frac{\\boldsymbol{\\Sigma}_u}{\\sqrt{\\sigma_{jj}}} \\boldsymbol{e}_j \\] 誘導誤差項の分散共分散行列\\(\\boldsymbol{\\Sigma}_u\\)（3変数の場合）は次の通り、対角要素に誘導誤差項\\(u_{i,t}\\)の分散\\(\\sigma_{ii}\\)、非対角要素に誘導誤差項\\(u_{i,t}\\)と\\(u_{j,t}\\)の共分散\\(\\sigma_{ij}\\)が並ぶ行列です。なお、分母である誘導誤差項の標準偏差\\(\\sqrt{\\sigma_{jj}}\\)は、ショック\\(\\boldsymbol{\\Sigma}_u \\boldsymbol{e}_j\\)の大きさを標準化する役割があります。 \\[ \\boldsymbol{\\Sigma}_u = \\begin{bmatrix} \\sigma_{11} &amp; \\sigma_{12} &amp; \\sigma_{13} \\\\ \\sigma_{21} &amp; \\sigma_{22} &amp; \\sigma_{23} \\\\ \\sigma_{31} &amp; \\sigma_{32} &amp; \\sigma_{33} \\end{bmatrix} \\] 要するに一般化インパルス応答は、構造VARモデルに制約を置いて変数の同時点間の関係を（恣意的に）特定化する代わりに、データから推定できる誘導誤差項の同時点間の関係（誘導誤差項の分散共分散行列）をそのまま変数の同時点間の関係として使ってしまおうというアイデアです。 一般化インパルス応答を用いる際には、次の点に注意してください。 一般化インパルス応答は誤差項の正規分布を仮定しているため、誤差項の正規性の検定により、正規分布の仮定が妥当であるか確認する必要があります。 一般化インパルス応答は、直交インパルス応答と異なり、インパルス応答関数の形状が内生変数を並べる順序に依存しません。 第1変数の誤差項にショックを与える場合、一般化インパルス応答と直交化インパルス応答の結果は一致します。 第1変数以外の誤差項にショックを与える場合、一般化インパルス応答と直交化インパルス応答の結果は一致しません。 誘導誤差項の分散共分散行列\\(\\boldsymbol{\\Sigma}_u\\)が対角行列の場合（すなわち変数が同時点無相関の場合）、一般化インパルス応答と直交化インパルス応答の結果は一致します。 8.9.3 予測誤差の分散分解 予測誤差の分散分解（forecast error variance decomposition, FEVD）は、ある変数の変動に対して他の変数がどの程度影響しているかを定量的に分析する手法です（西山 他（2019）P.614、エンダース（2019）P.286、村尾（2019）P.339）。インパルス応答と、予測誤差の分散分解は、どちらも経済変数の総合依存関係を調べるための重要な分析手法であり、両者は合わせてイノベーション会計（innovation accounting）と呼ばれます（エンダース（2019）P.288）。 具体的には、\\(t+1\\)期における変数の実現値を\\(Y_{t+1}\\)、\\(t\\)時点の情報をもとに\\(t+1\\)期の値を予測した値を\\(Y_{t+1|t}\\)とし、その差である予測誤差を\\(Y_{t+1} - Y_{t+1|t}\\)としたとき、予測誤差の分散である平均2乗予測誤差（Mean Squared Forecast Error、MSFE）\\(E[(Y_{t+1} - Y_{t+1|t})^2]\\)に対して、各変数の独立ショックの寄与度を計算します。この寄与度を分散寄与度（Relative Variance Contribution、RVC）と呼びます。 例えば3変数\\(Y_{1,t}\\)、\\(Y_{2,t}\\)、\\(Y_{3,t}\\)のVARモデルにおいて、\\(t-s\\)期に変数\\(Y_{j,t-s}\\)に発生した独立ショック\\(\\epsilon_{j,t-s}\\)が\\(t\\)期の変数\\(Y_{i,t}\\)に及ぼす影響を、\\(Y_{i,t}\\)の\\(Y_{j,t-s}\\)に対するインパルス応答と呼び、関数\\(\\mathit{IRF}_{ij}(s)\\)で表すとします。 ここで、\\(h\\)期先の変数\\(Y_{1,t+h}\\)のMSFEに占める\\(\\epsilon_{2,t}\\)の分散寄与度（RVC）は、 \\[ \\frac{\\sum^{h-1}_{s=0}{\\mathit{IRF}_{12}(s)^2}}{\\sum^{h-1}_{s=0}{\\mathit{IRF}{11}(s)^2} + \\sum^{h-1}_{s=0}{\\mathit{IRF}_{12}(s)^2} + \\sum^{h-1}_{s=0}{\\mathit{IRF}_{13}(s)^2}} \\] で計算できます。 予測誤差の分散分解（FEVD）を行う上では、次の点に注意してください。 予測誤差の分散分解はインパルス応答関数を用いて計算されるため、インパルス応答と同様に構造誤差項の識別が問題になります。構造VARモデルに誤った制約を課すと、そこから得られる分散分解の結果も現実の経済構造を反映しないものになります。 予測誤差の分散分解では、多くの場合、予測期間が短い場合は自己変数のショックが予測誤差分散の大半を占め、予測期間が長くなるに連れてその他の変数のショックの影響が大きくなっていきます。これは、変数間の同時点の相関関係が大きくないことに起因しています。 8.10 VARモデルの分析フロー VARモデルは、次のような一連のフローに基づいて分析を実施します。 1. 探索的データ分析 データの折れ線グラフや自己相関・偏自己相関プロットを作成し、季節性やトレンドといったデータの特徴を把握します。 2. 変数選択 経済理論やグレンジャー因果性検定をもとに、VARモデルに含める変数を選択します。 既存のモデルに新たな変数を加える場合は、グレンジャー因果性検定を実施し、新たな変数が少なくとも一つの既存変数に対しグレンジャー因果性をもてば（「因果性がない」との帰無仮説を棄却できれば）、変数をモデルに追加します（エンダース（2019）P.293）。 ただし、グレンジャー因果性検定の結果はラグ次数の選択に大きく依存します。したがって、グレンジャー因果性検定を使用した変数選択は、次の「ラグ次数の選択」と不可分であり、両者を同時並行で実施するのが良いでしょう。 Rではvarsパッケージのcausality()関数でグレンジャー因果性検定を実施します。 3. ラグ次数の選択 自己回帰（AR）過程のラグ次数\\(p\\)を選択します。選択したラグ次数\\(p\\)は、この後に行う単位根検定、共和分検定、モデル推定で使用します。 まず、妥当であると考えられる最長のラグ次数\\(p\\)を設定します。例えば四半期データ分析において「モデルの動学的な特性を捉えるには3年程度の長さが必要である」と判断される場合、最長のラグは12になります（エンダース（2019）P.289）。 次に、情報量規準を用いて実際にモデルで用いるラグ次数\\(p\\)を決定します。ラグ次数を選択するための情報量規準には、赤池情報量規準（Akaike Information Criterion、AIC）、赤池最終予測誤差（Akaike’s Final Prediction Error、FPE）、ハナン＝クイン情報量規準（Hannan-Quinn Information Criterion、HQ）、シュワルツのベイズ情報量規準（Schwarz Bayesian Information Criterion、SC）があります。これら3つの情報量規準が選ぶラグ次数には、\\(\\mathit{SC} \\leq \\mathit{HQ} \\leq \\mathit{AIC} \\fallingdotseq \\mathit{FPE}\\)の関係があります。時系列データのサンプル数が大きい場合はSCを、小さい場合はAICかFPEを、どちらか迷う場合はHQを選択してください（村尾（2019）P.117）。 ラグ次数\\(p\\)が小さすぎると、モデルの特定化を誤ってしまう可能性が高まる点に注意してください。ラグ次数の選択で迷う場面があれば、長めのラグ次数にしておくことが無難です（エンダース（2019）P289、P.291）。 RではvarsパッケージのVARselect()関数を用いてVARモデルを暫定的に推定し、あらかじめ決めておいた情報量規準に基づいてラグ次数を選択します。 4. 単位根検定 VAR分析に用いる時系列データそれぞれに対してADF検定を行い、データの定常性を確認します。原系列のままで定常なレベル定常過程（\\(I(0)\\)）か、単位根をもつ階差定常過程（和分過程、\\(I(1)\\)）かを判断します。また、定数項やトレンド項の確定項の有無もあわせて確認します。 単位根検定の結果に基づき、適用するモデルが変わります。 全ての変数が\\(I(0)\\)のケース： レベルVARモデルを適用（フロー7へ） 1つの変数が\\(I(1)\\)： 階差VARモデルを適用（フロー7へ） 2つ以上の変数が\\(I(1)\\)： 共和分検定を行い、\\(I(1)\\)変数間の共和分関係の有無を確認（フロー5へ） Rでは本書で自作したADF検定フロー関数のadf_test_flow()関数を用いて単位根検定を行います。 5. 共和分検定 単位根検定で2つ以上の\\(I(1)\\)変数があると判断した場合は、共和分検定を行い、\\(I(1)\\)変数間に共和分関係があるか判断します。 共和分検定の結果に基づき、適用するモデルが変わります。 共和分関係がないケース： 階差VARモデルを適用（フロー7へ） 共和分関係があるケース： VECモデルを適用（フロー7へ） Rでは本書で自作したヨハンセン検定フロー関数のjohansen_test_flow()関数を用いて共和分検定を行います。 6. データ分割 VARモデルを将来予測に用いる場合は、予測精度を評価するため、データを訓練データとテストデータに分割します。 Rでは、ts形式のデータを分割するにはstats::window()関数を使用してstart引数とend引数に時点を指定します。tibble形式などのデータフレーム形式の場合は、dplyr::slice()関数で行インデックスを指定するか、データフレームの列に日付などの時点情報の列が含まれる場合はdplyr::filter()関数で行をフィルタします。詳細は、第7章の「ts形式データ」、第3章の「行のスライスとサンプリング」と「行のフィルタ」を参照してください。 7. 誘導VAR・VECモデルの推定 ここまでに実施した一連の診断的検定（diagnostic test）に基づき、誘導形のレベルVARモデル、階差VARモデル、VECモデルを推定します。 誘導形のレベルVARモデルと階差VARモデルは、varsパッケージのVAR()関数で推定します。階差VARモデルの場合は事前に変数を階差変換しておきます。 一方、誘導形のVECモデルは、urcaパッケージのcajorls()関数で推定します。具体的には、ヨハンセン検定に用いるurcaパッケージのca.jo()関数で共和分ランクを決定したうえで、ca.jo()関数で出力したオブジェクトをurca::cajorls()関数の入力として用い、VECモデルを推定します。 8. 構造VAR・VECモデルの識別 インパルス応答や予測誤差の分散分解を行うには、推定した誘導VAR・VECモデルから構造VAR・VECモデルを識別する必要があります。 構造VARモデルは、varsパッケージのSVAR()関数で推定します。構造VECモデルは同じくvarsパッケージのSVEC()関数で推定することができます。 9. 誤差項の検定 誘導形のVAR・VECモデルが推定できたら、誤差項の推定量である残差系列を用いて、誤差項に系列相関がないか、分散が均一か、正規分布に従っているかを検定します。これらの誤差項の仮定が満たされない場合は、変数の選択やラグ次数の選択を再度行い、モデル推定をやり直す必要があります。 VARモデルの場合は、vars::VAR()関数で出力したオブジェクトを、それぞれの検定用関数に入力します。VECモデルの場合は、urca::ca.jo()関数で出力したアウトプットをvars::vec2var()関数でVARモデルのオブジェクトに変換し、検定用の関数に入力します。 系列無相関 系列無相関の主な検定には、次の4種類があります。 ボックス＝ピアース検定（Box-Pierce test）、別名：ポルトマント検定（Portmanteau test） リュン＝ボックス検定（Ljung-Box test）、別名：修正ポルトマント検定（Adjusted Portmanteau test） ブロイシュ＝ゴッドフレイLM検定（Breusch-Godfrey LM test） エドガートン＝シュクル検定（Edgerton-Shukur test） ブロイシュ＝ゴッドフレイLM検定（もしくは、その小標本版のエドガートン＝シュクル検定）が主流です（村尾（2019）P.178）。4つの検定すべてにおいて、帰無仮説は「系列無相関」であり、Rではvarsパッケージのserial.test()関数で4つの検定を実行できます。 均一分散 均一分散の主な検定には、次の3種類があります。 ブロイシュ＝ペーガン検定（Breusch-Pagan test） ホワイト検定（White test） ARCH-LM検定 村尾（2019）P.183では、実用性の観点からARCH-LM検定を用いるべきとしています。3つの検定すべてにおいて帰無仮説は「均一分散」です。Rではvarsパッケージのarch.test()関数でARCH-LM検定を実行することができます。 正規分布 正規分布の検定には、ジャック＝ベラ検定（Jarque-Bera test）を用いるのが一般的です（村尾（2019）P.184）。ジャック＝ベラ検定の帰無仮説は「正規分布」であり、Rではvarsパッケージのnormality.test()関数で実行できます。 10. モデル推定後の分析 推定したモデルを用いて、インパルス応答、予測誤差の分散分解、グレンジャー因果性検定といった追加的な分析を行います。インパルス応答にはvarsパッケージのirf()関数、予測誤差の分散分解にはvarsパッケージのfevd()関数、グレンジャー因果性検定にはvarsパッケージのcausality()関数を用います。 vars::irf()関数とvars::fevd()関数の入力として用いることができるオブジェクトは、誘導VARモデルvars::VAR()、構造VARモデルvars::SVAR()、構造VECモデルvars::SVEC()の出力のみです。urcaパッケージのcajorls()関数で推定した誘導VECモデルのオブジェクトは、そのままではvars::irf()関数やvars::fevd()関数の入力としては使用できないため、あらかじめvarsパッケージのvec2var()関数でオブジェクトを変換しておきます。 なお、vars::causality()関数の入力として用いることができるオブジェクトは、誘導VARモデルvars::VAR()の出力のみです。 11. 予測 推定したモデルを用いて、先行きの予測値を作成します。予測値の作成には、varsパッケージのpredict()関数を用います。 予測値は、分割しておいたテストデータの実績値と比較して精度を評価します。評価指標にはRMSE（Root Mean Square Error）などが用いられます。加えて、過去の平均値や前期の値を予測値として用いるナイーブ予測（VARなどの複雑なモデルを使わない予測）の精度を上回ることを確認します。 8.11 VARモデルの実例 VARモデルに関する様々なパターンの分析フローをRで再現した実例です。 8.11.1 実例：探索的データ分析 varsパッケージのCanadaデータセットに含まれる雇用者数（e、自然対数×100）、実質労働生産性（prod、自然対数×100）、実質賃金（rw、自然対数×100）、失業率（U、％）の4変数に対し、探索的データ分析を行います。データ頻度は四半期、期間は1980年1-3月期〜2000年10-12月期です。 まず、data()関数でCanadaデータセットを呼び出し、forecastパッケージのggtsdisplay()関数で折れ線グラフと自己相関・偏自己相関プロットを作成します。 Canadaデータセットに格納されているデータはts形式データのため、そのままggtsdisplay()関数を適用します。なお、tibble形式などのデータフレーム形式のデータもggtsdisplay()関数の入力として用いることができますが、その場合はggtsdisplay()関数で出力した図表の横軸が日付ではなくインデックスになります。 # Canadaデータセットを呼び出し data(Canada) # 変数をプロット forecast::ggtsdisplay(Canada[, &quot;e&quot;], main = &quot;雇用者数 e&quot;) forecast::ggtsdisplay(Canada[, &quot;prod&quot;], main = &quot;実質労働生産性 prod&quot;) forecast::ggtsdisplay(Canada[, &quot;rw&quot;], main = &quot;実質賃金 rw&quot;) forecast::ggtsdisplay(Canada[, &quot;U&quot;], main = &quot;失業率 U&quot;) VARモデルでは変数間の関係性も重要なので、GGallyパッケージのggpairs()関数でペアプロットを作成します。ggpairs()関数で作成したペアプロットは、対角部分が各変数の折れ線グラフ、上三角部分が2変数間の相関係数、下三角部分が2変数の散布図になります。 ここでは、レベルVARモデル、階差VARモデル双方に対応できるよう、変数のレベル系列と階差系列それぞれについてペアプロットを作成します。階差系列を作成するにはdiff()関数を使用し、階差次数をlag引数に指定します。 # レベル系列のペアプロット Canada %&gt;% tibble::as_tibble() %&gt;% GGally::ggpairs(progress = FALSE) # 階差系列のペアプロット Canada %&gt;% diff(lag = 1) %&gt;% tibble::as_tibble() %&gt;% GGally::ggpairs(progress = FALSE) # dplyrを使って階差系列のペアプロットを作成する場合 # Canada %&gt;% # tibble::as_tibble() %&gt;% # dplyr::mutate(across(.cols = everything(), .fns = ~ {. - dplyr::lag(.)})) %&gt;% # tidyr::drop_na() %&gt;% # GGally::ggpairs(progress = FALSE) 8.11.2 実例：変数選択（グレンジャー因果性検定） 村尾（2019）P.379の「15.7 Rによるグレンジャー因果性検定」を再現します。varsパッケージのCanadaデータセットに対しグレンジャー因果性検定を行って、VARモデルに含める変数を選択します。 雇用者数（e、自然対数×100）、実質労働生産性（prod、自然対数×100）、実質賃金（rw、自然対数×100）、失業率（U、％）の4変数について、原因変数を1つずつ選び、他の3変数に対するグレンジャー因果性があるかを確かめます。これら4変数はマクロ経済の時系列変数であり、何も単位根を持っている可能性があるため、レベル系列と階差系列の2パターンでグレンジャー因果性検定を行います。 Rのグレンジャー因果性検定は、まずvarsパッケージのVAR()関数で検定対象になる変数を含むVARモデルを暫定的に推定し、次にcausality()関数へVARモデルを入力して検定を行います。なお、ここではVARモデルのラグ次数pを暫定的に2としておきます。 Canadaデータセットの4変数について、ラグ次数2でレベル系列のグレンジャー因果性検定を次のように実施したところ、全ての検定において有意水準5％で帰無仮説が棄却されました。したがって、どの変数も他の3変数に対しグレンジャー因果性を持っていると判断できます。 # Canadaデータセットを呼び出し data(Canada) # Canadaデータセットのレベル系列のVARモデルを暫定的に推定 model_var_canada &lt;- vars::VAR(Canada, # VARモデルを構築するデータセット p = 2, # ラグ次数 type = &quot;const&quot;, # 確定項（&quot;const&quot;, &quot;trend&quot;, &quot;both&quot;, &quot;none&quot;） exogen = NULL # 外生変数 ) # forループで全ての変数に対しグレンジャー因果性検定を実施 for (x in colnames(Canada)) { # グレンジャー因果性検定 granger_canada &lt;- vars::causality(model_var_canada, # VARモデルの推計結果 cause = x, # 原因変数（文字列で指定。ベクトルにして2種類以上の原因変数を指定することも可能） vcov. = NULL, # 共和分行列を指定（デフォルトはNULL） boot = FALSE, # 棄却点をブートストラップ法で計算する場合はTRUE（デフォルトはFALSE） boot.runs = 1000 # ブートストラップ法の繰り返し数 ) # グレンジャー因果性検定の結果を出力 # $Instantにするとグレンジャー瞬間的因果性検定の結果が出力される print(granger_canada$Granger) } ## ## Granger causality H0: e do not Granger-cause prod rw U ## ## data: VAR object model_var_canada ## F-Test = 6.2768, df1 = 6, df2 = 292, p-value = 3.206e-06 ## ## ## Granger causality H0: prod do not Granger-cause e rw U ## ## data: VAR object model_var_canada ## F-Test = 2.7811, df1 = 6, df2 = 292, p-value = 0.01205 ## ## ## Granger causality H0: rw do not Granger-cause e prod U ## ## data: VAR object model_var_canada ## F-Test = 2.594, df1 = 6, df2 = 292, p-value = 0.01828 ## ## ## Granger causality H0: U do not Granger-cause e prod rw ## ## data: VAR object model_var_canada ## F-Test = 2.8116, df1 = 6, df2 = 292, p-value = 0.01126 次に、Canadaデータセットの1階階差系列に対し、同様にグレンジャー因果性検定を行います。その結果、原因変数が失業率Uの時だけ有意水準5％で帰無仮説を棄却できませんでした。したがって1階階差系列では、雇用者数e、実質労働生産性prod、実質賃金rwがそれぞれ他の3変数に対しグレンジャー因果性を持つと判断できる一方、失業率Uは他の3変数に対しグレンジャー因果性を持つと判断することはできません。 階差VARモデルを構築する場合に、雇用者数e、実質労働生産性prod、実質賃金rwの予測が重要であり、失業率Uに関心がないのであれば、失業率Uをモデルから除くことを検討すべきです。ここでは失業率Uの予測にも関心があることから、失業率UもVARモデルに含めます。 # Canadaデータセットの1階階差系列のVARモデルを暫定的に推定 model_var_canada_diff &lt;- vars::VAR(diff(Canada, lag = 1), # VARモデルを構築するデータセット p = 2, # ラグ次数 type = &quot;const&quot;, # 確定項（&quot;const&quot;, &quot;trend&quot;, &quot;both&quot;, &quot;none&quot;） exogen = NULL # 外生変数 ) # forループで全ての変数に対しグレンジャー因果性検定を実施 for (x in colnames(Canada)) { # グレンジャー因果性検定 granger_canada &lt;- vars::causality(model_var_canada_diff, # VARモデルの推計結果 cause = x, # 原因変数（文字列で指定。ベクトルにして2種類以上の原因変数を指定することも可能） vcov. = NULL, # 共和分行列を指定（デフォルトはNULL） boot = FALSE, # 棄却点をブートストラップ法で計算する場合はTRUE（デフォルトはFALSE） boot.runs = 1000 # ブートストラップ法の繰り返し数 ) # グレンジャー因果性検定の結果を出力 # $Instantにするとグレンジャー瞬間的因果性検定の結果が出力される print(granger_canada$Granger) } ## ## Granger causality H0: e do not Granger-cause prod rw U ## ## data: VAR object model_var_canada_diff ## F-Test = 5.6853, df1 = 6, df2 = 288, p-value = 1.332e-05 ## ## ## Granger causality H0: prod do not Granger-cause e rw U ## ## data: VAR object model_var_canada_diff ## F-Test = 3.3915, df1 = 6, df2 = 288, p-value = 0.003008 ## ## ## Granger causality H0: rw do not Granger-cause e prod U ## ## data: VAR object model_var_canada_diff ## F-Test = 2.9202, df1 = 6, df2 = 288, p-value = 0.00883 ## ## ## Granger causality H0: U do not Granger-cause e prod rw ## ## data: VAR object model_var_canada_diff ## F-Test = 1.5734, df1 = 6, df2 = 288, p-value = 0.1547 なお、ラグ次数を変化させてグレンジャー因果性検定の結果がどう変わるかをforループで確認することができます。次の例は、Canadaデータセットの1階階差系列に対し、VARモデル推定時のラグ次数を1〜12で変化させた結果を出力するコードです。 その結果を見ると、ラグ次数が2〜3のときは失業率Uを除く3変数がグレンジャー因果性を持ちますが、ラグ次数が4〜5次ではGranegr因果性を持つ変数が雇用者数eのみになり、6次以上では全ての変数がグレンジャー因果性を持たないとの結果になりました。この傾向はCanadaデータセットのレベル系列を対象にする場合でも大きく変わらず、ラグ次数が高次になるとグレンジャー因果性が消失することが確認できます。 # グレンジャー因果性検定の結果を格納するtibbleを作成 granger_canada_diff &lt;- tibble::tibble() %&gt;% dplyr::mutate(lag = NA_real_, variable = NA_character_, pvalue = NA_real_, sig = NA_character_ ) # ラグ次数の候補 lags &lt;- 1:12 # ラグ次数のforループ for (l in lags) { # VARモデル推定 model_temp &lt;- vars::VAR(diff(Canada, lag = 1), # VARモデルを構築するデータセット p = l, # ラグ次数 type = &quot;const&quot;, # 確定項（&quot;const&quot;, &quot;trend&quot;, &quot;both&quot;, &quot;none&quot;） exogen = NULL # 外生変数 ) # 変数のforループ for (x in colnames(Canada)) { # グレンジャー因果性検定 granger_temp &lt;- vars::causality(model_temp, # VARモデルの推計結果 cause = x, # 原因変数（文字列で指定。ベクトルにして2種類以上の原因変数を指定することも可能） vcov. = NULL, # 共和分行列を指定（デフォルトはNULL） boot = FALSE, # 棄却点をブートストラップ法で計算する場合はTRUE（デフォルトはFALSE） boot.runs = 1000 # ブートストラップ法の繰り返し数 ) # グレンジャー因果性検定の結果をtibbleに追加 granger_canada_diff %&lt;&gt;% tibble::add_row(lag = l, variable = x, pvalue = granger_temp$Granger$p.value %&gt;% as.numeric(), sig = case_when(granger_temp$Granger$p.value &lt; 0.05 ~ &quot;＊&quot;) ) } } # tibbleを横型に変換して出力 granger_canada_diff %&gt;% tidyr::pivot_wider(id_cols = &quot;lag&quot;, names_from = &quot;variable&quot;, names_sep = &quot;_&quot;, values_from = c(&quot;pvalue&quot;, &quot;sig&quot;)) %&gt;% print() ## # A tibble: 12 × 9 ## lag pvalue_e pvalue_prod pvalue_rw pvalue_U sig_e sig_prod sig_rw sig_U ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 0.0000159 0.00781 0.381 0.0540 ＊ ＊ &lt;NA&gt; &lt;NA&gt; ## 2 2 0.0000133 0.00301 0.00883 0.155 ＊ ＊ ＊ &lt;NA&gt; ## 3 3 0.000154 0.0301 0.0133 0.152 ＊ ＊ ＊ &lt;NA&gt; ## 4 4 0.00205 0.0742 0.0854 0.0736 ＊ &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 5 0.0163 0.0588 0.0611 0.0762 ＊ &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 6 0.121 0.235 0.181 0.200 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 7 7 0.269 0.422 0.681 0.390 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 8 8 0.473 0.439 0.904 0.336 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 9 9 0.574 0.419 0.986 0.216 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 10 10 0.723 0.565 0.912 0.280 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 11 11 0.889 0.745 0.969 0.295 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 12 12 0.786 0.825 0.846 0.307 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 8.11.3 実例：ラグ次数選択 村尾（2019）P.118の「5.3 Rによるラグ字数の選択」を再現します。対象はvarsパッケージのCanadaデータセットに含まれる雇用者数（e、自然対数×100）、実質労働生産性（prod、自然対数×100）、実質賃金（rw、自然対数×100）、失業率（U、％）の4変数です。 まず、Canadaデータセットのレベル系列に対してvarsパッケージのVARselect()関数を適用し、情報量規準に基づくラグ次数を計算します。VARselect()関数のtype引数には、モデルに含む確定項を指定します。確定項は、const：定数項のみ、trend：タイムトレンド項のみ、both：定数項とタイムトレンド項の両方、none：確定項なし、の4種類から選択します。 Canadaデータセットのレベル系列に対しVARselect()関数を適用すると、次の結果が得られます。情報量規準により最適なラグ次数が1〜3と異なりますが、ここでは中間的なHQ（Hannan-Quinn Information Criterion）の「2」を暫定的に選択します。最終的なラグ次数の決定は、VAR・VECモデルの推定後に誤差項の仮定に関する検定を行い、選択したラグ次数のもとで誤差項が系列無相関、均一分散、正規分布の仮定を満たしているかを踏まえて判断します。 # Canadaデータセットを呼び出し data(Canada) varselect_canada &lt;- vars::VARselect(y = Canada, # 内生変数を含むデータセット type = &quot;both&quot;, # モデルに含む確定項（&quot;const&quot;, &quot;trend&quot;, &quot;both&quot;, &quot;none&quot;） lag.max = 8 # 検査するラグ次数の最大値 ) print(varselect_canada) ## $selection ## AIC(n) HQ(n) SC(n) FPE(n) ## 3 2 1 3 ## ## $criteria ## 1 2 3 4 5 ## AIC(n) -6.272579064 -6.636669705 -6.771176872 -6.634609210 -6.398132246 ## HQ(n) -5.978429449 -6.146420347 -6.084827770 -5.752160366 -5.319583658 ## SC(n) -5.536558009 -5.409967947 -5.053794411 -4.426546046 -3.699388378 ## FPE(n) 0.001889842 0.001319462 0.001166019 0.001363175 0.001782055 ## 6 7 8 ## AIC(n) -6.307704843 -6.070727259 -6.06159685 ## HQ(n) -5.033056512 -4.599979185 -4.39474903 ## SC(n) -3.118280272 -2.390621985 -1.89081087 ## FPE(n) 0.002044202 0.002768551 0.00306012 また、1次の階差系列についても同様に最適なラグ次数を確認すると、1〜2次となります。ここでも同様にHQの「1」を暫定的に採用します。 varselect_canada &lt;- vars::VARselect(y = diff(Canada, lag = 1), type = &quot;both&quot;, lag.max = 8 ) print(varselect_canada) ## $selection ## AIC(n) HQ(n) SC(n) FPE(n) ## 2 1 1 2 ## ## $criteria ## 1 2 3 4 5 ## AIC(n) -6.27372428 -6.356374491 -6.175322581 -6.076315440 -5.956187509 ## HQ(n) -5.97761306 -5.862855794 -5.484396406 -5.187981786 -4.870446377 ## SC(n) -5.53212808 -5.120380830 -4.444931456 -3.851526851 -3.237001456 ## FPE(n) 0.00188778 0.001746773 0.002117316 0.002386034 0.002780352 ## 6 7 8 ## AIC(n) -5.825348974 -5.83585581 -5.800448783 ## HQ(n) -4.542200363 -4.35529972 -4.122485215 ## SC(n) -2.611765456 -2.12787483 -1.598070337 ## FPE(n) 0.003327675 0.00352927 0.004021854 8.11.4 実例：ADF検定 定義した自作関数adf_test_flow()を用い、村尾（2019）「6.7 拡張ディッキー＝フラー検定の例」（P.141）及び、「6.10 Rによる拡張ディッキー＝フラー検定」（P.147）の例を再現します。村尾（2019）6.7、6.10では、varsパッケージのCanadaデータセットのうち実質労働生産性（prod、自然対数×100）だけがADF検定の対象になっていますが、ここでは雇用者数（e、自然対数×100）、実質賃金（rw、自然対数×100）、失業率（U、％）に対してもADF検定を行います。 自作関数adf_test_flow()で4変数に対しADF検定を行った結果、次の結果が得られました。なお、ラグ次数選択基準はAIC（Akaike Information Criterion）とし、最大ラグ次数は「実例：ラグ次数選択」で選択した2に設定しています。 雇用者数e：1次の階差系列に対するtrend第2検定で「単位根あり」の帰無仮説を棄却。I(1)過程と判断。 実質労働生産性prod：1次の階差系列に対するtrend第2検定で「単位根あり」の帰無仮説を棄却。I(1)過程と判断。 実質賃金rw：レベル系列に対するtrend第2検定で「単位根あり」の帰無仮説を棄却。I(0)過程と判断。 失業率U：1次の階差系列に対するtrend第2検定で「単位根あり」の帰無仮説を棄却。I(1)過程と判断。 4変数のうち3変数が非定常なI(1)過程、1変数が定常なI(0)過程と判断できました。I(1)過程の変数が2つ以上あるため、分析フローに基けば、この後に共和分検定を実施して、I(1)変数間に共和分の関係があるか確認するのが適切です。 # Canadaデータセットを呼び出し data(Canada) # Canadaの変数名に対するforループ for (y in colnames(Canada)) { # yにadf_test_flow()関数を適用 test_adf &lt;- adf_test_flow(y = Canada[, y], # 検定対象の時系列データ lag_criterion = &quot;AIC&quot;, # ラグ次数選択基準 lags_max = 2, # AICで自動選択する最大ラグ次数 sig_level = 0.05 # 有意水準 ) # 結果をコンソールに出力 print(y) test_adf %&gt;% dplyr::select(`フロー`, `階差次数`, `判断`, `判断経路`) %&gt;% print() } ## [1] &quot;e&quot; ## # A tibble: 14 × 4 ## フロー 階差次数 判断 判断経路 ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 0 単位根の判断保留：フロー2へ &quot;＊&quot; ## 2 2 0 トレンド項なし：フロー4へ &quot;＊&quot; ## 3 3 0 単位根なし：I(0)過程 &quot;&quot; ## 4 4 0 単位根の判断保留：フロー5へ &quot;＊&quot; ## 5 5 0 定数項なし：フロー7へ &quot;＊&quot; ## 6 6 0 階差変換し再検定 &quot;&quot; ## 7 7 0 階差変換し再検定 &quot;＊&quot; ## 8 8 1 単位根なし：I(1)過程 &quot;&quot; ## 9 9 1 トレンド項あり：フロー10へ &quot;&quot; ## 10 10 1 単位根なし：I(1)過程 &quot;＊&quot; ## 11 11 1 単位根なし：I(1)過程 &quot;&quot; ## 12 12 1 定数項あり：フロー13へ &quot;&quot; ## 13 13 1 単位根なし：I(1)過程 &quot;&quot; ## 14 14 1 単位根なし：I(1)過程 &quot;&quot; ## [1] &quot;prod&quot; ## # A tibble: 14 × 4 ## フロー 階差次数 判断 判断経路 ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 0 単位根の判断保留：フロー2へ &quot;＊&quot; ## 2 2 0 トレンド項なし：フロー4へ &quot;＊&quot; ## 3 3 0 単位根なし：I(0)過程 &quot;&quot; ## 4 4 0 単位根の判断保留：フロー5へ &quot;＊&quot; ## 5 5 0 定数項なし：フロー7へ &quot;＊&quot; ## 6 6 0 階差変換し再検定 &quot;&quot; ## 7 7 0 階差変換し再検定 &quot;＊&quot; ## 8 8 1 単位根なし：I(1)過程 &quot;&quot; ## 9 9 1 トレンド項あり：フロー10へ &quot;&quot; ## 10 10 1 単位根なし：I(1)過程 &quot;＊&quot; ## 11 11 1 単位根なし：I(1)過程 &quot;&quot; ## 12 12 1 定数項あり：フロー13へ &quot;&quot; ## 13 13 1 単位根なし：I(1)過程 &quot;&quot; ## 14 14 1 単位根なし：I(1)過程 &quot;&quot; ## [1] &quot;rw&quot; ## # A tibble: 14 × 4 ## フロー 階差次数 判断 判断経路 ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 0 単位根の判断保留：フロー2へ &quot;＊&quot; ## 2 2 0 トレンド項あり：フロー3へ &quot;＊&quot; ## 3 3 0 単位根なし：I(0)過程 &quot;＊&quot; ## 4 4 0 単位根なし：I(0)過程 &quot;&quot; ## 5 5 0 定数項あり：フロー6へ &quot;&quot; ## 6 6 0 単位根なし：I(0)過程 &quot;&quot; ## 7 7 0 階差変換し再検定 &quot;&quot; ## 8 8 1 単位根なし：I(1)過程 &quot;&quot; ## 9 9 1 トレンド項あり：フロー10へ &quot;&quot; ## 10 10 1 単位根なし：I(1)過程 &quot;&quot; ## 11 11 1 単位根なし：I(1)過程 &quot;&quot; ## 12 12 1 定数項あり：フロー13へ &quot;&quot; ## 13 13 1 単位根なし：I(1)過程 &quot;&quot; ## 14 14 1 単位根なし：I(1)過程 &quot;&quot; ## [1] &quot;U&quot; ## # A tibble: 14 × 4 ## フロー 階差次数 判断 判断経路 ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 0 単位根の判断保留：フロー2へ &quot;＊&quot; ## 2 2 0 トレンド項なし：フロー4へ &quot;＊&quot; ## 3 3 0 単位根なし：I(0)過程 &quot;&quot; ## 4 4 0 単位根の判断保留：フロー5へ &quot;＊&quot; ## 5 5 0 定数項なし：フロー7へ &quot;＊&quot; ## 6 6 0 単位根なし：I(0)過程 &quot;&quot; ## 7 7 0 階差変換し再検定 &quot;＊&quot; ## 8 8 1 単位根なし：I(1)過程 &quot;&quot; ## 9 9 1 トレンド項あり：フロー10へ &quot;&quot; ## 10 10 1 単位根なし：I(1)過程 &quot;＊&quot; ## 11 11 1 単位根なし：I(1)過程 &quot;&quot; ## 12 12 1 定数項あり：フロー13へ &quot;&quot; ## 13 13 1 単位根なし：I(1)過程 &quot;&quot; ## 14 14 1 単位根なし：I(1)過程 &quot;&quot; 8.11.5 実例：ヨハンセン検定 定義した自作関数johansen_test_flow()を用い、村尾（2019）「7.4 ヨハンセン検定の例」（P.158）及び、「7.7 Rによるヨハンセン検定」（P.166）の例を再現します。 村尾（2019）7.4及び7.7では、varsパッケージのCanadaデータセットに含まれるカナダの実質労働生産性（prod、自然対数×100）、雇用者数（e、自然対数×100）、失業率（U、％）、実質賃金（rw、自然対数×100）の4変数（サンプル数84）に対し、ヨハンセン検定を適用しています。 村尾（2019）P.158には、「検定対象の変数は、単位根検定（ADF検定）において、それぞれ\\(I(1)\\)過程と判断した4個の変数である」との記載がありますが、ADF検定フロー自作関数のadf_test_flow()でADF検定を実施したところ、有意水準5％ではCanadaデータセットの実質賃金rwは\\(I(1)\\)過程ではなく\\(I(0)\\)過程と判断できます。すると4変数のうちI(1)過程の変数は3変数であり、共和分関係の数は最大で2になります。 johansen_test_flow()自作関数における事前推定VARモデルのオプションでは、ラグ次数選択基準のlag_criterionにHannan-Quinn情報量規準を意味する\"HQ\"を、確定項のver_typeにトレンド項と定数項の両方を意味する\"both\"を指定します。また、ヨハンセン検定のオプションでは、誤差修正スペックを意味するjohansen_specに「一時的」を意味する\"transitory\"を指定します。なお、村尾（2019）7.4及び7.7では検定の優位水準を10％としていますが、ここでは5％を指定します。 johansen_test_flow()自作関数を実行した結果、trendモデルとnoneモデルでは共和分関係の数が1、constモデルでは2と判断されました（ラグ次数は何も2次）。なお、トレース検定と最大固有値検定の結果はどのモデルにおいても一致しています。村尾（2019）7.4及び7.7ではtrendモデルを採用し、共和分関係の数を1と判断しています。 # Canadaデータセットを呼び出し data(Canada) # Canadaにヨハンセン検定フローを適用 jo_canada &lt;- johansen_test_flow(data = Canada, # ヨハンセン検定を行う時系列データセット lag_criterion = &quot;HQ&quot;, # 事前推定VARモデルでラグ次数を決定する情報量規準 lags_max = 10, # 最大ラグ次数 var_type = &quot;both&quot;, # 事前推定VARモデルに含む確定項 johansen_spec = &quot;transitory&quot;, # 共和分関係の均衡からの乖離が一時的・長期的か（&quot;transitory&quot;, &quot;longrun&quot;） sig_level = 0.05 # 有意水準 ) # 結果をコンソールに表示 jo_canada$trend jo_canada$const jo_canada$none 8.11.6 実例：VARモデルの推定 村尾（2019）P.218の「9.8 RによるVARモデルの推定」を再現します。使用するデータは、varsパッケージのCanadaデータセットです。データ頻度は四半期、期間は1980年1-3月期〜2000年10-12月期です。格納されているデータは、雇用者数（e、自然対数×100）、実質労働生産性（prod、自然対数×100）、実質賃金（rw、自然対数×100）、失業率（U、％）です。これら4変数を用いて、レベルVARモデル、階差VARモデル、外生変数を用いたVARモデルを構築します。 レベルVARモデル ADF検定とヨハンセン検定の結果に基けば、Canadaデータセットに対してはVECモデルを適用すべきですが、ここでは例としてレベルVARモデルを推計します。変数の順番は、実質労働生産性prod、雇用者数e、失業率U、実質賃金rwとします。 VARモデルの推定は、varsパッケージのVAR()関数で行います。 VAR()関数で出力したモデルオブジェクトに対しsummary()関数を適用すると、各内生変数の推定結果が出力されます。出力結果の最後には残差の共分散行列と相関係数行列も表示されています。 また、VAR()関数で出力したモデルオブジェクトに対しplot()関数を適用すると、各内生変数の推定結果の折れ線グラフ、自己相関プロット（ACF）、偏自己相関プロット（PACF）のグラフがPlots画面に出力されます。グラフは内生変数の順番に出力され、Enterキーを押すと次の内生変数に進みます。 # Canadaデータセットを呼び出し data(Canada) # レベルVARモデルの推定 model_var_level &lt;- vars::VAR(y = Canada[, c(&quot;prod&quot;, &quot;e&quot;, &quot;U&quot;, &quot;rw&quot;)], # 内生変数を含むデータセット p = 2, # ラグ次数 type = &quot;trend&quot;, # 確定項（&quot;both&quot;, &quot;const&quot;, &quot;trend&quot;, &quot;none&quot;） exogen = NULL # 外生変数 ) # 推定結果を出力 summary(model_var_level) ## ## VAR Estimation Results: ## ========================= ## Endogenous variables: prod, e, U, rw ## Deterministic variables: trend ## Sample size: 82 ## Log Likelihood: -177.03 ## Roots of the characteristic polynomial: ## 1 0.8828 0.8728 0.8728 0.7164 0.2545 0.1573 0.1573 ## Call: ## vars::VAR(y = Canada[, c(&quot;prod&quot;, &quot;e&quot;, &quot;U&quot;, &quot;rw&quot;)], p = 2, type = &quot;trend&quot;, ## exogen = NULL) ## ## ## Estimation results for equation prod: ## ===================================== ## prod = prod.l1 + e.l1 + U.l1 + rw.l1 + prod.l2 + e.l2 + U.l2 + rw.l2 + trend ## ## Estimate Std. Error t value Pr(&gt;|t|) ## prod.l1 1.09870 0.10804 10.169 1.24e-15 *** ## e.l1 -0.14824 0.26017 -0.570 0.57058 ## U.l1 -0.57940 0.33582 -1.725 0.08870 . ## rw.l1 0.01523 0.09693 0.157 0.87562 ## prod.l2 -0.21979 0.11443 -1.921 0.05868 . ## e.l2 0.23074 0.25527 0.904 0.36901 ## U.l2 0.88798 0.33311 2.666 0.00945 ** ## rw.l2 -0.09289 0.09215 -1.008 0.31674 ## trend 0.06759 0.02310 2.926 0.00458 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.6288 on 73 degrees of freedom ## Multiple R-Squared: 1, Adjusted R-squared: 1 ## F-statistic: 3.834e+06 on 9 and 73 DF, p-value: &lt; 2.2e-16 ## ## ## Estimation results for equation e: ## ================================== ## e = prod.l1 + e.l1 + U.l1 + rw.l1 + prod.l2 + e.l2 + U.l2 + rw.l2 + trend ## ## Estimate Std. Error t value Pr(&gt;|t|) ## prod.l1 0.16456 0.06431 2.559 0.0126 * ## e.l1 1.63082 0.15486 10.531 2.69e-16 *** ## U.l1 0.13232 0.19989 0.662 0.5101 ## rw.l1 -0.05765 0.05770 -0.999 0.3210 ## prod.l2 -0.12339 0.06812 -1.811 0.0742 . ## e.l2 -0.64150 0.15194 -4.222 6.89e-05 *** ## U.l2 -0.04002 0.19828 -0.202 0.8406 ## rw.l2 0.03935 0.05485 0.717 0.4754 ## trend 0.01533 0.01375 1.115 0.2686 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.3743 on 73 degrees of freedom ## Multiple R-Squared: 1, Adjusted R-squared: 1 ## F-statistic: 5.803e+07 on 9 and 73 DF, p-value: &lt; 2.2e-16 ## ## ## Estimation results for equation U: ## ================================== ## U = prod.l1 + e.l1 + U.l1 + rw.l1 + prod.l2 + e.l2 + U.l2 + rw.l2 + trend ## ## Estimate Std. Error t value Pr(&gt;|t|) ## prod.l1 -0.079404 0.051432 -1.544 0.127 ## e.l1 -0.570210 0.123849 -4.604 1.71e-05 *** ## U.l1 0.770015 0.159858 4.817 7.69e-06 *** ## rw.l1 0.008927 0.046142 0.193 0.847 ## prod.l2 0.074044 0.054473 1.359 0.178 ## e.l2 0.569518 0.121514 4.687 1.26e-05 *** ## U.l2 0.124884 0.158570 0.788 0.434 ## rw.l2 0.001470 0.043865 0.034 0.973 ## trend -0.012464 0.010997 -1.133 0.261 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.2993 on 73 degrees of freedom ## Multiple R-Squared: 0.9991, Adjusted R-squared: 0.999 ## F-statistic: 9163 on 9 and 73 DF, p-value: &lt; 2.2e-16 ## ## ## Estimation results for equation rw: ## =================================== ## rw = prod.l1 + e.l1 + U.l1 + rw.l1 + prod.l2 + e.l2 + U.l2 + rw.l2 + trend ## ## Estimate Std. Error t value Pr(&gt;|t|) ## prod.l1 -0.12702 0.13138 -0.967 0.3368 ## e.l1 -0.23961 0.31636 -0.757 0.4512 ## U.l1 0.03722 0.40834 0.091 0.9276 ## rw.l1 0.85682 0.11786 7.270 3.28e-10 *** ## prod.l2 -0.03001 0.13915 -0.216 0.8298 ## e.l2 0.35274 0.31039 1.136 0.2595 ## U.l2 -0.09255 0.40505 -0.228 0.8199 ## rw.l2 0.04513 0.11205 0.403 0.6883 ## trend 0.04948 0.02809 1.762 0.0823 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.7646 on 73 degrees of freedom ## Multiple R-Squared: 1, Adjusted R-squared: 1 ## F-statistic: 3.053e+06 on 9 and 73 DF, p-value: &lt; 2.2e-16 ## ## ## ## Covariance matrix of residuals: ## prod e U rw ## prod 0.395423 -0.004776 0.008032 0.03387 ## e -0.004776 0.140101 -0.079011 -0.04717 ## U 0.008032 -0.079011 0.089603 0.03761 ## rw 0.033867 -0.047169 0.037607 0.58464 ## ## Correlation matrix of residuals: ## prod e U rw ## prod 1.00000 -0.02029 0.04267 0.07044 ## e -0.02029 1.00000 -0.70519 -0.16481 ## U 0.04267 -0.70519 1.00000 0.16431 ## rw 0.07044 -0.16481 0.16431 1.00000 階差VARモデル ADF検定とヨハンセン検定の結果に基けば、Canadaデータセットに対してはVECモデルを適用すべきですが、ここでは例として階差VARモデルを推計します。変数の順番は、実質労働生産性prod、雇用者数e、失業率U、実質賃金rwとします。 # Canadaデータセットを呼び出し data(Canada) # 階差VARモデルの推定 model_var_diff &lt;- vars::VAR(y = diff(Canada[, c(&quot;prod&quot;, &quot;e&quot;, &quot;U&quot;, &quot;rw&quot;)], lag = 1), # 内生変数を含むデータセット p = 1, # ラグ次数 type = &quot;const&quot;, # 確定項（&quot;both&quot;, &quot;const&quot;, &quot;trend&quot;, &quot;none&quot;） exogen = NULL # 外生変数 ) # 推定結果を出力 summary(model_var_diff) ## ## VAR Estimation Results: ## ========================= ## Endogenous variables: prod, e, U, rw ## Deterministic variables: const ## Sample size: 82 ## Log Likelihood: -213.785 ## Roots of the characteristic polynomial: ## 0.7431 0.401 0.1095 0.06195 ## Call: ## vars::VAR(y = diff(Canada[, c(&quot;prod&quot;, &quot;e&quot;, &quot;U&quot;, &quot;rw&quot;)], lag = 1), ## p = 1, type = &quot;const&quot;, exogen = NULL) ## ## ## Estimation results for equation prod: ## ===================================== ## prod = prod.l1 + e.l1 + U.l1 + rw.l1 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## prod.l1 0.24427 0.10722 2.278 0.0255 * ## e.l1 -0.42335 0.24837 -1.705 0.0923 . ## U.l1 -0.85238 0.34801 -2.449 0.0166 * ## rw.l1 -0.02006 0.08007 -0.251 0.8029 ## const 0.29283 0.14203 2.062 0.0426 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.6735 on 77 degrees of freedom ## Multiple R-Squared: 0.1729, Adjusted R-squared: 0.1299 ## F-statistic: 4.023 on 4 and 77 DF, p-value: 0.005139 ## ## ## Estimation results for equation e: ## ================================== ## e = prod.l1 + e.l1 + U.l1 + rw.l1 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## prod.l1 0.18428 0.06176 2.984 0.00381 ** ## e.l1 0.74083 0.14305 5.179 1.74e-06 *** ## U.l1 0.14031 0.20045 0.700 0.48603 ## rw.l1 -0.05207 0.04612 -1.129 0.26239 ## const 0.13182 0.08180 1.611 0.11119 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.3879 on 77 degrees of freedom ## Multiple R-Squared: 0.5854, Adjusted R-squared: 0.5639 ## F-statistic: 27.18 on 4 and 77 DF, p-value: 4.457e-14 ## ## ## Estimation results for equation U: ## ================================== ## U = prod.l1 + e.l1 + U.l1 + rw.l1 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## prod.l1 -0.13648 0.04988 -2.736 0.00772 ** ## e.l1 -0.50251 0.11555 -4.349 4.14e-05 *** ## U.l1 -0.12721 0.16191 -0.786 0.43447 ## rw.l1 0.06515 0.03725 1.749 0.08432 . ## const 0.13588 0.06608 2.056 0.04313 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.3134 on 77 degrees of freedom ## Multiple R-Squared: 0.5109, Adjusted R-squared: 0.4855 ## F-statistic: 20.11 on 4 and 77 DF, p-value: 2.27e-11 ## ## ## Estimation results for equation rw: ## =================================== ## rw = prod.l1 + e.l1 + U.l1 + rw.l1 + const ## ## Estimate Std. Error t value Pr(&gt;|t|) ## prod.l1 -0.3065 0.1479 -2.072 0.04159 * ## e.l1 0.1440 0.3426 0.420 0.67546 ## U.l1 0.4594 0.4801 0.957 0.34158 ## rw.l1 0.3337 0.1105 3.021 0.00342 ** ## const 0.6481 0.1959 3.308 0.00143 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.9291 on 77 degrees of freedom ## Multiple R-Squared: 0.2478, Adjusted R-squared: 0.2088 ## F-statistic: 6.343 on 4 and 77 DF, p-value: 0.0001823 ## ## ## ## Covariance matrix of residuals: ## prod e U rw ## prod 0.4536329 0.007223 -0.01728 0.0001991 ## e 0.0072231 0.150491 -0.08396 -0.0465106 ## U -0.0172828 -0.083958 0.09819 0.0760282 ## rw 0.0001991 -0.046511 0.07603 0.8631851 ## ## Correlation matrix of residuals: ## prod e U rw ## prod 1.0000000 0.02765 -0.08189 0.0003181 ## e 0.0276451 1.00000 -0.69068 -0.1290461 ## U -0.0818896 -0.69068 1.00000 0.2611502 ## rw 0.0003181 -0.12905 0.26115 1.0000000 外生変数がある階差VARモデル Canadaデータセットのうち、雇用者数e、失業率U、実質賃金rwを内生変数、実質労働生産性prodを外生変数として、階差VARモデルを推定します。 # Canadaデータセットを呼び出し data(Canada) # 内生変数 data_endo &lt;- diff(Canada[, c(&quot;e&quot;, &quot;U&quot;, &quot;rw&quot;)], lag = 1) # 外生変数 data_exo &lt;- diff(Canada[, &quot;prod&quot;], lag = 1) # レベルVARモデルの推定 model_var_diff_exo &lt;- vars::VAR(y = data_endo, # 内生変数を含むデータセット p = 2, # ラグ次数 type = &quot;const&quot;, # 確定項（&quot;both&quot;, &quot;const&quot;, &quot;trend&quot;, &quot;none&quot;） exogen = data_exo # 外生変数 ) # 推定結果を出力 summary(model_var_diff_exo) ## ## VAR Estimation Results: ## ========================= ## Endogenous variables: e, U, rw ## Deterministic variables: const ## Sample size: 81 ## Log Likelihood: -121.213 ## Roots of the characteristic polynomial: ## 0.6608 0.6608 0.4838 0.4838 0.1943 0.1943 ## Call: ## vars::VAR(y = data_endo, p = 2, type = &quot;const&quot;, exogen = data_exo) ## ## ## Estimation results for equation e: ## ================================== ## e = e.l1 + U.l1 + rw.l1 + e.l2 + U.l2 + rw.l2 + const + exo1 ## ## Estimate Std. Error t value Pr(&gt;|t|) ## e.l1 0.9417544 0.1584011 5.945 8.73e-08 *** ## U.l1 0.0363265 0.2134018 0.170 0.86530 ## rw.l1 -0.0395863 0.0497552 -0.796 0.42883 ## e.l2 -0.4772416 0.1731212 -2.757 0.00737 ** ## U.l2 -0.2276251 0.2056673 -1.107 0.27203 ## rw.l2 -0.0495961 0.0503728 -0.985 0.32808 ## const 0.2976861 0.1027403 2.897 0.00496 ** ## exo1 0.0008943 0.0685323 0.013 0.98962 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.3893 on 73 degrees of freedom ## Multiple R-Squared: 0.604, Adjusted R-squared: 0.5661 ## F-statistic: 15.91 on 7 and 73 DF, p-value: 1.662e-12 ## ## ## Estimation results for equation U: ## ================================== ## U = e.l1 + U.l1 + rw.l1 + e.l2 + U.l2 + rw.l2 + const + exo1 ## ## Estimate Std. Error t value Pr(&gt;|t|) ## e.l1 -0.60910 0.12635 -4.821 7.58e-06 *** ## U.l1 -0.13478 0.17023 -0.792 0.4311 ## rw.l1 0.05039 0.03969 1.270 0.2082 ## e.l2 0.09755 0.13810 0.706 0.4822 ## U.l2 -0.11506 0.16406 -0.701 0.4853 ## rw.l2 0.10516 0.04018 2.617 0.0108 * ## const 0.03407 0.08195 0.416 0.6788 ## exo1 -0.03389 0.05467 -0.620 0.5372 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.3105 on 73 degrees of freedom ## Multiple R-Squared: 0.5433, Adjusted R-squared: 0.4995 ## F-statistic: 12.4 on 7 and 73 DF, p-value: 2.368e-10 ## ## ## Estimation results for equation rw: ## =================================== ## rw = e.l1 + U.l1 + rw.l1 + e.l2 + U.l2 + rw.l2 + const + exo1 ## ## Estimate Std. Error t value Pr(&gt;|t|) ## e.l1 -0.22779 0.37705 -0.604 0.5476 ## U.l1 0.71662 0.50797 1.411 0.1626 ## rw.l1 0.28900 0.11843 2.440 0.0171 * ## e.l2 0.84637 0.41209 2.054 0.0436 * ## U.l2 0.24604 0.48956 0.503 0.6168 ## rw.l2 0.13062 0.11990 1.089 0.2796 ## const 0.30734 0.24456 1.257 0.2129 ## exo1 0.08683 0.16313 0.532 0.5961 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.9266 on 73 degrees of freedom ## Multiple R-Squared: 0.2742, Adjusted R-squared: 0.2046 ## F-statistic: 3.94 on 7 and 73 DF, p-value: 0.001065 ## ## ## ## Covariance matrix of residuals: ## e U rw ## e 0.15153 -0.08689 -0.03912 ## U -0.08689 0.09642 0.08105 ## rw -0.03912 0.08105 0.85855 ## ## Correlation matrix of residuals: ## e U rw ## e 1.0000 -0.7189 -0.1085 ## U -0.7189 1.0000 0.2817 ## rw -0.1085 0.2817 1.0000 8.11.7 実例：VECモデルの推定 村尾（2019）P.221の「9.9 RによるVECモデルの推定」を再現します。使用するデータは、varsパッケージのCanadaデータセットです。データ頻度は四半期、期間は1980年1-3月期〜2000年10-12月期です。格納されているデータは、雇用者数（e、自然対数×100）、実質労働生産性（prod、自然対数×100）、実質賃金（rw、自然対数×100）、失業率（U、％）です。これら4変数を用いてVECモデルを推定します。 「実例：ADF検定（村尾（2019）6.7）」では、Canadaデータセットの4変数のうち、雇用者数e、実質労働生産性prod、失業率UがI(1)過程、実質賃金rwがI(0)過程と判断しました。その後、「実例：ヨハンセン検定（村尾（2019）7.4、7.7）」では、Trendモデルを指定したトレース検定、最大固有値検定の両方で、これら4変数の間に1つの共和分関係があると判断しました。したがって、VARモデルの分析フローを踏まえると、Canadaデータセットの4変数にはVECモデルが適していると考えられます。 RによるVECモデルの推定は2段階で行います。まず、urcaパッケージのca.jo()関数で、ヨハンセン検定による共和分ランクの決定を行います。次に、ca.jo()関数で出力したオブジェクトをurca::cajorls()関数に入力し、VECモデルを推定します。 ここでは先に実施したヨハンセン検定の結果を踏まえ、type引数にトレース検定を意味するtraceを、ecdet引数にTrendモデルを意味するtrendを、spec引数には均衡からの乖離が一時的とするtransitoryを指定します。 # Canadaデータセットを呼び出し data(Canada) # VECモデル推定の第１段階（ヨハンセン検定） model_vec &lt;- urca::ca.jo(x = Canada[, c(&quot;prod&quot;, &quot;e&quot;, &quot;U&quot;, &quot;rw&quot;)], # 内生変数を含むデータセット type = &quot;trace&quot;, # 検定タイプ（&quot;eigen&quot;, &quot;trace&quot;） ecdet = &quot;trend&quot;, # 確定項（&quot;none&quot;, &quot;const&quot;, &quot;trend&quot;） K = 3, # ラグ次数 spec = &quot;transitory&quot;, # 共和分関係の均衡からの乖離が一時的・長期的か（&quot;transitory&quot;, &quot;longrun&quot;） season = NULL, # 季節ダミー（四半期の場合は4） dumvar = NULL # 引数xに指定したデータセットにダミー変数が含まれている場合はTRUEにする ) summary(model_vec) ## ## ###################### ## # Johansen-Procedure # ## ###################### ## ## Test type: trace statistic , with linear trend in cointegration ## ## Eigenvalues (lambda): ## [1] 4.505013e-01 1.962777e-01 1.676668e-01 4.647108e-02 2.894486e-17 ## ## Values of teststatistic and critical values of test: ## ## test 10pct 5pct 1pct ## r &lt;= 3 | 3.85 10.49 12.25 16.26 ## r &lt;= 2 | 18.72 22.76 25.32 30.45 ## r &lt;= 1 | 36.42 39.06 42.44 48.45 ## r = 0 | 84.92 59.14 62.99 70.05 ## ## Eigenvectors, normalised to first column: ## (These are the cointegration relations) ## ## prod.l1 e.l1 U.l1 rw.l1 trend.l1 ## prod.l1 1.00000000 1.0000000 1.0000000 1.0000000 1.0000000 ## e.l1 -0.02385142 1.2946681 -2.8831559 4.2418087 -8.2903941 ## U.l1 3.16874550 3.4036732 -7.4261513 6.8413561 -12.5578436 ## rw.l1 1.83528156 -0.3330945 1.3978788 -0.1393999 2.4466500 ## trend.l1 -1.30156097 -0.2302803 -0.5093218 -1.5925918 0.2831079 ## ## Weights W: ## (This is the loading matrix) ## ## prod.l1 e.l1 U.l1 rw.l1 trend.l1 ## prod.d -0.006535281 -0.02763446 -0.070975297 -0.014754352 -8.205627e-12 ## e.d -0.008503348 0.11414011 -0.008156659 0.003988051 -5.756735e-12 ## U.d -0.004718574 -0.06154306 0.020719431 -0.006557248 3.631716e-12 ## rw.d -0.046213351 -0.14579644 -0.016945105 0.011896044 -4.801434e-12 # VECモデル推定の第２段階（VECモデル推定） model_vec &lt;- urca::cajorls(z = model_vec, # 第１段階の結果 r = 1, # 共和分関係の数（共和分ランク） reg.number = NULL # 推定すべき方程式の本数（デフォルトはNULL） ) # VECモデルの推定結果を出力 # rlm：係数推定値 # beta：共和分ベクトルの係数推定値（第1変数の係数が1になるよう標準化） print(model_vec) ## $rlm ## ## Call: ## lm(formula = substitute(form1), data = data.mat) ## ## Coefficients: ## prod.d e.d U.d rw.d ## ect1 -0.006535 -0.008503 -0.004719 -0.046213 ## constant 8.274808 10.331308 5.687832 55.469125 ## prod.dl1 0.234441 0.200953 -0.138916 -0.074493 ## e.dl1 -0.246544 0.821558 -0.646846 -0.634084 ## U.dl1 -0.979868 0.003379 -0.191125 0.063137 ## rw.dl1 0.004707 -0.078491 0.017263 -0.012082 ## prod.dl2 -0.029520 0.048273 -0.002909 -0.251940 ## e.dl2 -0.580473 -0.459693 -0.019741 0.081197 ## U.dl2 -0.128101 -0.103415 -0.262685 -0.230009 ## rw.dl2 -0.190264 -0.095835 0.080354 -0.157388 ## ## ## $beta ## ect1 ## prod.l1 1.00000000 ## e.l1 -0.02385142 ## U.l1 3.16874550 ## rw.l1 1.83528156 ## trend.l1 -1.30156097 8.11.8 実例：構造VARモデルの推定 村尾（2019）P.223の「9.10 Rによる構造VARモデルの推定」を再現します。使用するデータは、varsパッケージのCanadaデータセットです。データ頻度は四半期、期間は1980年1-3月期〜2000年10-12月期です。格納されているデータは、雇用者数（e、自然対数×100）、実質労働生産性（prod、自然対数×100）、実質賃金（rw、自然対数×100）、失業率（U、％）です。これら4変数を用いて構造VARモデルを推定します。 Rによる構造VARモデルの推定は、2段階で行います。まず、varsパッケージのVAR()関数で準備的な推定を行い、次に、VAR()関数で出力したオブジェクトをSVAR()関数に入力し、構造VARモデルを推定します。SVAR()関数のAmat、Bmat引数にはそれぞれ、変数\\(\\boldsymbol{Y}_t\\)の同時点間の関係を表す行列\\(\\boldsymbol{A}\\)と、構造誤差項の標準偏差を表す行列\\(\\boldsymbol{B}\\)を指定します。 \\[ \\boldsymbol{A} \\boldsymbol{Y}_t = \\boldsymbol{A}_1 \\boldsymbol{Y}_{t-1} + \\boldsymbol{B} \\boldsymbol{\\epsilon}^{SD}_t \\] ここではCanadaデータセットの4変数の順序を、実質労働生産性（prod、自然対数×100）、雇用者数（e、自然対数×100）、失業率（U、％）、実質賃金（rw、自然対数×100）とし、行列\\(\\boldsymbol{A}\\)を次のように指定します。 \\[ \\boldsymbol{A} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ * &amp; 1 &amp; 0 &amp; 0 \\\\ * &amp; * &amp; 1 &amp; 0 \\\\ * &amp; 0 &amp; * &amp; 1 \\\\ \\end{bmatrix} \\] ＊は当該要素に制約を課さないことを意味し、Rで行列\\(\\boldsymbol{A}\\)を作成する際はNAを入れます。構造VARモデルの識別に必要な制約の数は、変数の個数を\\(n\\)として\\(n(n-1)/2\\)であり、すなわちこのモデルでは\\(4(4-1)/2 = 6\\)個になります。上記の行列\\(\\boldsymbol{A}\\)にはゼロ制約が7個あるため、識別の条件を満たしています。 なお、仮に上記の行列\\(\\boldsymbol{A}\\)を、 \\[ \\boldsymbol{A} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ a_{21} &amp; 1 &amp; 0 &amp; 0 \\\\ a_{31} &amp; a_{32} &amp; 1 &amp; 0 \\\\ a_{41} &amp; 0 &amp; a_{43} &amp; 1 \\\\ \\end{bmatrix} \\] を表すと、変数間の同時点の関係を、次のように想定していることを意味します。ただしこれは一例であって、こうした同時点間の制約の置き方がCanadaデータセットに対してベストな制約であることを意味するものではありません。 \\[ \\begin{aligned} prod_t &amp;= \\epsilon_{prod,t} \\\\ e_t &amp;= a_{21} prod_t + \\epsilon_{e,t} \\\\ U_t &amp;= a_{31} prod_t + a_{32} e_t + \\epsilon_{U,t} \\\\ rw_t &amp;= a_{41} prod_t + a_{43} U_t + \\epsilon_{rw,t} \\\\ \\end{aligned} \\] また、行列\\(\\boldsymbol{B}\\)は次のように指定します。＊には各変数の構造誤差項の標準偏差が入ります。Rで行列\\(\\boldsymbol{B}\\)を作成する際は制約を課さないことを示すNAを入力します。 \\[ \\boldsymbol{B} = \\begin{bmatrix} * &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; * &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; * &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; * \\\\ \\end{bmatrix} \\] # Canadaデータセットを呼び出し data(Canada) # 構造VARモデル推定の第１段階（VARモデル推定） model_svar &lt;- vars::VAR(y = Canada[, c(&quot;prod&quot;, &quot;e&quot;, &quot;U&quot;, &quot;rw&quot;)], # 内生変数を含むデータセット p = 3, # ラグ次数 type = &quot;both&quot;, # 確定項（&quot;both&quot;, &quot;const&quot;, &quot;trend&quot;, &quot;none&quot;） exogen = NULL # 外生変数 ) summary(model_svar) ## ## VAR Estimation Results: ## ========================= ## Endogenous variables: prod, e, U, rw ## Deterministic variables: both ## Sample size: 81 ## Log Likelihood: -143.629 ## Roots of the characteristic polynomial: ## 0.9576 0.9576 0.8187 0.8187 0.6087 0.6087 0.5687 0.5687 0.5422 0.5422 0.2911 0.26 ## Call: ## vars::VAR(y = Canada[, c(&quot;prod&quot;, &quot;e&quot;, &quot;U&quot;, &quot;rw&quot;)], p = 3, type = &quot;both&quot;, ## exogen = NULL) ## ## ## Estimation results for equation prod: ## ===================================== ## prod = prod.l1 + e.l1 + U.l1 + rw.l1 + prod.l2 + e.l2 + U.l2 + rw.l2 + prod.l3 + e.l3 + U.l3 + rw.l3 + const + trend ## ## Estimate Std. Error t value Pr(&gt;|t|) ## prod.l1 1.08139 0.11837 9.136 2.19e-13 *** ## e.l1 -0.19623 0.27967 -0.702 0.4853 ## U.l1 -0.75410 0.36739 -2.053 0.0440 * ## rw.l1 -0.01995 0.09922 -0.201 0.8413 ## prod.l2 -0.18079 0.17447 -1.036 0.3038 ## e.l2 -0.15481 0.43518 -0.356 0.7231 ## U.l2 0.74329 0.45500 1.634 0.1070 ## rw.l2 -0.20124 0.12870 -1.564 0.1226 ## prod.l3 -0.02050 0.12100 -0.169 0.8660 ## e.l3 0.45747 0.30770 1.487 0.1418 ## U.l3 0.32218 0.38113 0.845 0.4009 ## rw.l3 0.12124 0.09990 1.214 0.2291 ## const -13.21924 135.17985 -0.098 0.9224 ## trend 0.07452 0.03035 2.455 0.0167 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.6287 on 67 degrees of freedom ## Multiple R-Squared: 0.9816, Adjusted R-squared: 0.9781 ## F-statistic: 275.5 on 13 and 67 DF, p-value: &lt; 2.2e-16 ## ## ## Estimation results for equation e: ## ================================== ## e = prod.l1 + e.l1 + U.l1 + rw.l1 + prod.l2 + e.l2 + U.l2 + rw.l2 + prod.l3 + e.l3 + U.l3 + rw.l3 + const + trend ## ## Estimate Std. Error t value Pr(&gt;|t|) ## prod.l1 0.18518 0.06393 2.897 0.005087 ** ## e.l1 1.76383 0.15104 11.678 &lt; 2e-16 *** ## U.l1 0.12194 0.19841 0.615 0.540906 ## rw.l1 -0.07242 0.05359 -1.352 0.181069 ## prod.l2 -0.10936 0.09423 -1.161 0.249915 ## e.l2 -1.19012 0.23502 -5.064 3.44e-06 *** ## U.l2 -0.03229 0.24573 -0.131 0.895837 ## rw.l2 -0.02488 0.06951 -0.358 0.721456 ## prod.l3 0.02565 0.06535 0.392 0.695963 ## e.l3 0.61471 0.16617 3.699 0.000438 *** ## U.l3 0.35976 0.20583 1.748 0.085078 . ## rw.l3 0.03172 0.05395 0.588 0.558480 ## const -193.37044 73.00464 -2.649 0.010066 * ## trend -0.01741 0.01639 -1.062 0.291941 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.3395 on 67 degrees of freedom ## Multiple R-Squared: 0.9988, Adjusted R-squared: 0.9985 ## F-statistic: 4212 on 13 and 67 DF, p-value: &lt; 2.2e-16 ## ## ## Estimation results for equation U: ## ================================== ## U = prod.l1 + e.l1 + U.l1 + rw.l1 + prod.l2 + e.l2 + U.l2 + rw.l2 + prod.l3 + e.l3 + U.l3 + rw.l3 + const + trend ## ## Estimate Std. Error t value Pr(&gt;|t|) ## prod.l1 -0.115838 0.052588 -2.203 0.031056 * ## e.l1 -0.630594 0.124249 -5.075 3.29e-06 *** ## U.l1 0.633746 0.163223 3.883 0.000239 *** ## rw.l1 0.002742 0.044083 0.062 0.950579 ## prod.l2 0.092189 0.077514 1.189 0.238512 ## e.l2 0.525390 0.193337 2.717 0.008364 ** ## U.l2 -0.102428 0.202146 -0.507 0.614027 ## rw.l2 0.070509 0.057178 1.233 0.221829 ## prod.l3 -0.028450 0.053757 -0.529 0.598388 ## e.l3 -0.061913 0.136701 -0.453 0.652079 ## U.l3 0.045532 0.169328 0.269 0.788836 ## rw.l3 -0.031535 0.044382 -0.711 0.479841 ## const 163.889696 60.056811 2.729 0.008108 ** ## trend 0.020204 0.013486 1.498 0.138790 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.2793 on 67 degrees of freedom ## Multiple R-Squared: 0.9745, Adjusted R-squared: 0.9695 ## F-statistic: 196.8 on 13 and 67 DF, p-value: &lt; 2.2e-16 ## ## ## Estimation results for equation rw: ## =================================== ## rw = prod.l1 + e.l1 + U.l1 + rw.l1 + prod.l2 + e.l2 + U.l2 + rw.l2 + prod.l3 + e.l3 + U.l3 + rw.l3 + const + trend ## ## Estimate Std. Error t value Pr(&gt;|t|) ## prod.l1 -0.13954 0.13859 -1.007 0.3176 ## e.l1 -0.52468 0.32745 -1.602 0.1138 ## U.l1 -0.10820 0.43016 -0.252 0.8022 ## rw.l1 0.86031 0.11618 7.405 2.84e-10 *** ## prod.l2 -0.19911 0.20428 -0.975 0.3332 ## e.l2 0.69674 0.50953 1.367 0.1761 ## U.l2 -0.38989 0.53275 -0.732 0.4668 ## rw.l2 -0.14327 0.15069 -0.951 0.3451 ## prod.l3 0.14160 0.14167 0.999 0.3212 ## e.l3 -0.26039 0.36027 -0.723 0.4723 ## U.l3 0.06263 0.44625 0.140 0.8888 ## rw.l3 0.22136 0.11697 1.893 0.0627 . ## const 192.77721 158.27618 1.218 0.2275 ## trend 0.08341 0.03554 2.347 0.0219 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Residual standard error: 0.7361 on 67 degrees of freedom ## Multiple R-Squared: 0.999, Adjusted R-squared: 0.9988 ## F-statistic: 5157 on 13 and 67 DF, p-value: &lt; 2.2e-16 ## ## ## ## Covariance matrix of residuals: ## prod e U rw ## prod 0.395271 -0.02377 0.005524 0.01693 ## e -0.023775 0.11528 -0.069133 -0.02806 ## U 0.005524 -0.06913 0.078018 0.02636 ## rw 0.016932 -0.02806 0.026359 0.54188 ## ## Correlation matrix of residuals: ## prod e U rw ## prod 1.00000 -0.1114 0.03146 0.03659 ## e -0.11137 1.0000 -0.72896 -0.11225 ## U 0.03146 -0.7290 1.00000 0.12820 ## rw 0.03659 -0.1122 0.12820 1.00000 # 行列A mat_a &lt;- diag(4) mat_a[2, 1] &lt;- NA mat_a[3, 1:2] &lt;- NA mat_a[4, c(1, 3)] &lt;- NA print(mat_a) ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] NA 1 0 0 ## [3,] NA NA 1 0 ## [4,] NA 0 NA 1 # 行列B mat_b &lt;- diag(4) diag(mat_b) &lt;- NA print(mat_b) ## [,1] [,2] [,3] [,4] ## [1,] NA 0 0 0 ## [2,] 0 NA 0 0 ## [3,] 0 0 NA 0 ## [4,] 0 0 0 NA # 構造VARモデル推定の第２段階（構造VARモデル推定） model_svar &lt;- vars::SVAR(x = model_svar, # 推定対象のオブジェクト（VAR()関数の出力） estmethod = &quot;scoring&quot;, # 最尤法の推定オプション（&quot;scoring&quot;、&quot;direct&quot;） Amat = mat_a, # 変数の同時点間の関係を表す行列Aを指定 Bmat = mat_b, # 構造誤差項の標準偏差を表す行列Bを指定 max.iter = 100, # 最尤法の計算を繰り返す最大回数 maxls = 1000, # conv.crit = 1.0e-8 # 推定正確性（収束）の基準 ) print(model_svar) ## ## SVAR Estimation Results: ## ======================== ## ## ## Estimated A matrix: ## prod e U rw ## prod 1.00000 0.0000 0.0000 0 ## e 0.06015 1.0000 0.0000 0 ## U 0.02237 0.6043 1.0000 0 ## rw -0.03815 0.0000 -0.3352 1 ## ## Estimated B matrix: ## prod e U rw ## prod 0.6287 0.0000 0.0000 0.0000 ## e 0.0000 0.3374 0.0000 0.0000 ## U 0.0000 0.0000 0.1907 0.0000 ## rw 0.0000 0.0000 0.0000 0.7297 8.11.9 実例：構造VECモデルの推定 村尾（2019）P.227の「9.11 Rによる構造VECモデルの推定」を再現します。使用するデータは、varsパッケージのCanadaデータセットです。データ頻度は四半期、期間は1980年1-3月期〜2000年10-12月期です。格納されているデータは、雇用者数（e、自然対数×100）、実質労働生産性（prod、自然対数×100）、実質賃金（rw、自然対数×100）、失業率（U、％）です。これら4変数を用いて構造VECモデルを推定します。 Rによる構造VECモデルの推定は、2段階で行います。まず、urcaパッケージのca.jo()関数で、ヨハンセン検定による共和分ランクの決定を行います。次に、ca.jo()関数で出力したオブジェクトをvarsパッケージのSVEC()関数に入力し、構造VECモデルを推定します。SVEC()関数のLR、SR引数にはそれぞれ、長期インパクト行列\\(\\Xi \\boldsymbol{B}\\)と、短期インパクト行列\\(\\boldsymbol{B}\\)を指定します。 \\[ \\boldsymbol{Y}_t = \\Xi \\boldsymbol{B} \\sum^{t}_{i=1}{\\epsilon^{SD}_i} + \\sum^{\\infty}_{j=1}{\\Xi^*_j \\boldsymbol{B} \\epsilon^{SD}_{t-j}} + \\boldsymbol{Y}^*_0 \\] ここではCanadaデータセットの4変数の順序を、実質労働生産性（prod、自然対数×100）、雇用者数（e、自然対数×100）、失業率（U、％）、実質賃金（rw、自然対数×100）とし、長期インパクト行列\\(\\Xi \\boldsymbol{B}\\)と短期インパクト行列\\(\\boldsymbol{B}\\)を次のように指定します。なお、＊は当該要素に制約を課さないことを意味し、Rで行列を作成する際はNAを入れます。 \\[ \\Xi \\boldsymbol{B} = \\begin{bmatrix} * &amp; 0 &amp; 0 &amp; 0 \\\\ * &amp; * &amp; * &amp; 0 \\\\ * &amp; * &amp; * &amp; 0 \\\\ * &amp; * &amp; * &amp; * \\\\ \\end{bmatrix} , \\quad \\boldsymbol{B} = \\begin{bmatrix} * &amp; * &amp; * &amp; * \\\\ * &amp; * &amp; * &amp; * \\\\ * &amp; * &amp; * &amp; * \\\\ * &amp; 0 &amp; * &amp; * \\\\ \\end{bmatrix} \\] まず、urca::ca.jo()関数で共和分ランクを決定します。ここでは先に実施したヨハンセン検定の結果を踏まえ、type引数にトレース検定を意味するtraceを、ecdet引数にTrendモデルを意味するtrendを、spec引数には均衡からの乖離が一時的とするtransitoryを指定します。 # Canadaデータセットを呼び出し data(Canada) # 構造VECモデル推定の第１段階（ヨハンセン検定） model_svec &lt;- urca::ca.jo(x = Canada[, c(&quot;prod&quot;, &quot;e&quot;, &quot;U&quot;, &quot;rw&quot;)], # 内生変数を含むデータセット type = &quot;trace&quot;, # 検定タイプ（&quot;eigen&quot;, &quot;trace&quot;） ecdet = &quot;trend&quot;, # 確定項（&quot;none&quot;, &quot;const&quot;, &quot;trend&quot;） K = 3, # ラグ次数 spec = &quot;transitory&quot;, # 共和分関係の均衡からの乖離が一時的・長期的か（&quot;transitory&quot;, &quot;longrun&quot;） season = NULL, # 季節ダミー（四半期の場合は4） dumvar = NULL # 引数xに指定したデータセットにダミー変数が含まれている場合はTRUEにする ) summary(model_svec) ## ## ###################### ## # Johansen-Procedure # ## ###################### ## ## Test type: trace statistic , with linear trend in cointegration ## ## Eigenvalues (lambda): ## [1] 4.505013e-01 1.962777e-01 1.676668e-01 4.647108e-02 2.894486e-17 ## ## Values of teststatistic and critical values of test: ## ## test 10pct 5pct 1pct ## r &lt;= 3 | 3.85 10.49 12.25 16.26 ## r &lt;= 2 | 18.72 22.76 25.32 30.45 ## r &lt;= 1 | 36.42 39.06 42.44 48.45 ## r = 0 | 84.92 59.14 62.99 70.05 ## ## Eigenvectors, normalised to first column: ## (These are the cointegration relations) ## ## prod.l1 e.l1 U.l1 rw.l1 trend.l1 ## prod.l1 1.00000000 1.0000000 1.0000000 1.0000000 1.0000000 ## e.l1 -0.02385142 1.2946681 -2.8831559 4.2418087 -8.2903941 ## U.l1 3.16874550 3.4036732 -7.4261513 6.8413561 -12.5578436 ## rw.l1 1.83528156 -0.3330945 1.3978788 -0.1393999 2.4466500 ## trend.l1 -1.30156097 -0.2302803 -0.5093218 -1.5925918 0.2831079 ## ## Weights W: ## (This is the loading matrix) ## ## prod.l1 e.l1 U.l1 rw.l1 trend.l1 ## prod.d -0.006535281 -0.02763446 -0.070975297 -0.014754352 -8.205627e-12 ## e.d -0.008503348 0.11414011 -0.008156659 0.003988051 -5.756735e-12 ## U.d -0.004718574 -0.06154306 0.020719431 -0.006557248 3.631716e-12 ## rw.d -0.046213351 -0.14579644 -0.016945105 0.011896044 -4.801434e-12 # 長期インパクト行列 mat_lr &lt;- matrix(NA, nrow = 4, ncol = 4) mat_lr[1:4, 4] &lt;- 0 mat_lr[1, 2:4] &lt;- 0 print(mat_lr) ## [,1] [,2] [,3] [,4] ## [1,] NA 0 0 0 ## [2,] NA NA NA 0 ## [3,] NA NA NA 0 ## [4,] NA NA NA 0 # 短期インパクト行列 mat_sr &lt;- matrix(NA, nrow = 4, ncol = 4) mat_sr[4, 2] &lt;- 0 print(mat_sr) ## [,1] [,2] [,3] [,4] ## [1,] NA NA NA NA ## [2,] NA NA NA NA ## [3,] NA NA NA NA ## [4,] NA 0 NA NA # 構造VACモデル推定の第２段階（構造VACモデル推定） model_svec &lt;- vars::SVEC(x = model_svec, # 推定対象のオブジェクト（ca.jo()関数の出力） LR = mat_lr, # 長期的な制約を加える行列 SR = mat_sr, # 短期的な制約を加える行列 r = 1, # 共和分関係の数 lrtest = FALSE, # 過剰識別のための尤度比検定の必要性（適度識別の場合はNULL、過剰識別の場合はTRUEを指定） boot = TRUE, # ブートストラップ法により推定量の標準誤差を計算するか runs = 100 # ブートストラップ方による計算の繰り返し数 ) summary(model_svec) ## ## SVEC Estimation Results: ## ======================== ## ## Call: ## vars::SVEC(x = model_svec, LR = mat_lr, SR = mat_sr, r = 1, lrtest = FALSE, ## boot = TRUE, runs = 100) ## ## Type: B-model ## Sample size: 81 ## Log Likelihood: -161.838 ## Number of iterations: 15 ## ## Estimated contemporaneous impact matrix: ## prod e U rw ## prod 0.58402 0.07434 -0.152578 0.06900 ## e -0.12029 0.26144 -0.155096 0.08978 ## U 0.02526 -0.26720 0.005488 0.04982 ## rw 0.11170 0.00000 0.483771 0.48791 ## ## Estimated standard errors for impact matrix: ## prod e U rw ## prod 0.10678 0.11723 0.23968 0.07066 ## e 0.07052 0.05533 0.18008 0.04340 ## U 0.05557 0.05296 0.05899 0.02921 ## rw 0.15727 0.00000 0.66049 0.08427 ## ## Estimated long run impact matrix: ## prod e U rw ## prod 0.7910 0.0000 0.0000 0 ## e 0.2024 0.5769 -0.4923 0 ## U -0.1592 -0.3409 0.1408 0 ## rw -0.1535 0.5961 -0.2495 0 ## ## Estimated standard errors for long-run matrix: ## prod e U rw ## prod 0.1541 0.00000 0.0000 0 ## e 0.2812 0.15162 0.5922 0 ## U 0.1303 0.08053 0.1540 0 ## rw 0.2071 0.14084 0.2732 0 ## ## Covariance matrix of reduced form residuals (*100): ## prod e U rw ## prod 37.4642 -2.096 -0.2512 2.509 ## e -2.0960 11.494 -6.9273 -4.467 ## U -0.2512 -6.927 7.4544 2.978 ## rw 2.5087 -4.467 2.9783 48.457 8.11.10 実例：誤差項の検定 村尾（2019）の「8.6 Rによる系列無相関の検定」（P.185）、「8.7 Rによる均一分散の検定」（P.187）、「8.8 Rによる正規分布の検定」（P.189）を再現します。まず、varsパッケージのCanadaデータセットを使ってレベルVARモデルを推定し、その推定結果に対して誤差項の検定を行います。 系列無相関の検定 varsパッケージのserial.test()関数で、ボックス＝ピアース検定（ポルトマント検定）、リュン＝ボックス検定（修正ポルトマント検定）、ブロイシュ＝ゴッドフレイLM検定、エドガートン＝シュクル検定を実行します。 どの検定を行うかは、serial.test()関数のtype引数に指定します。 4つの検定を実施した結果、ボックス＝ピアース検定（ポルトマント検定）とリュン＝ボックス検定（修正ポルトマント検定）では帰無仮説「系列無相関」を棄却できませんが、一方で、ブロイシュ＝ゴッドフレイLM検定とエドガートン＝シュクル検定では帰無仮説「系列無相関」が棄却されました。 ここでは、ブロイシュ＝ゴッドフレイLM検定とエドガートン＝シュクル検定の結果を重視し、誤差項に系列相関の観点で問題がある、と判断します。 # Canadaデータセットを呼び出し data(Canada) # レベルVARモデルを推定 model_var &lt;- vars::VAR(Canada, # 内生変数を含むデータセット p = 3, # ラグ次数 type = &quot;both&quot; # 確定項（&quot;both&quot;, &quot;const&quot;, &quot;trend&quot;, &quot;none&quot;） ) # ボックス＝ピアース検定（ポルトマント検定） # 帰無仮説：系列無相関 test_serial_ptasy &lt;- vars::serial.test(model_var, # 検定対象のモデルオブジェクト lags.pt = 16, # ポルトマント検定のラグ次数 type = &quot;PT.asymptotic&quot; # 検定タイプ（&quot;PT.asymptotic&quot;、&quot;PT.adjusted&quot;、&quot;BG&quot;、&quot;ES&quot;） ) print(test_serial_ptasy) ## ## Portmanteau Test (asymptotic) ## ## data: Residuals of VAR object model_var ## Chi-squared = 173.97, df = 208, p-value = 0.9587 ## $serial ## ## Portmanteau Test (asymptotic) ## ## data: Residuals of VAR object model_var ## Chi-squared = 173.97, df = 208, p-value = 0.9587 # リュン＝ボックス検定（修正ポルトマント検定） # 帰無仮説：系列無相関 test_serial_ptadj &lt;- vars::serial.test(model_var, # 検定対象のモデルオブジェクト lags.pt = 16, # ポルトマント検定のラグ次数 type = &quot;PT.adjusted&quot; # 検定タイプ（&quot;PT.asymptotic&quot;、&quot;PT.adjusted&quot;、&quot;BG&quot;、&quot;ES&quot;） ) print(test_serial_ptadj) ## ## Portmanteau Test (adjusted) ## ## data: Residuals of VAR object model_var ## Chi-squared = 198.04, df = 208, p-value = 0.6785 ## $serial ## ## Portmanteau Test (adjusted) ## ## data: Residuals of VAR object model_var ## Chi-squared = 198.04, df = 208, p-value = 0.6785 # ブロイシュ＝ゴッドフレイ検定 # 帰無仮説：系列無相関 test_serial_bg &lt;- vars::serial.test(model_var, # 検定対象のモデルオブジェクト lags.bg = 16, # ブロイシュ＝ゴッドフレイ検定のラグ次数 type = &quot;BG&quot; # 検定タイプ（&quot;PT.asymptotic&quot;、&quot;PT.adjusted&quot;、&quot;BG&quot;、&quot;ES&quot;） ) print(test_serial_bg) ## ## Breusch-Godfrey LM test ## ## data: Residuals of VAR object model_var ## Chi-squared = 315.2, df = 256, p-value = 0.006811 ## $serial ## ## Breusch-Godfrey LM test ## ## data: Residuals of VAR object model_var ## Chi-squared = 315.2, df = 256, p-value = 0.006811 # エドガートン＝シュクル検定 # 帰無仮説：系列無相関 test_serial_es &lt;- vars::serial.test(model_var, # 検定対象のモデルオブジェクト lags.bg = 16, # ブロイシュ＝ゴッドフレイ検定のラグ次数 type = &quot;ES&quot; # 検定タイプ（&quot;PT.asymptotic&quot;、&quot;PT.adjusted&quot;、&quot;BG&quot;、&quot;ES&quot;） ) print(test_serial_es) ## ## Edgerton-Shukur F test ## ## data: Residuals of VAR object model_var ## F statistic = Inf, df1 = 256, df2 = 2, p-value &lt; 2.2e-16 ## $serial ## ## Edgerton-Shukur F test ## ## data: Residuals of VAR object model_var ## F statistic = Inf, df1 = 256, df2 = 2, p-value &lt; 2.2e-16 均一分散の検定 varsパッケージのarch.test()関数で、ARCH-LM検定を実行します。 ARCH-LM検定を実行した結果、帰無仮説「均一分散」を棄却できません。したがって、分散均一性の観点では誤差項に問題はない、と判断します。 # Canadaデータセットを呼び出し data(Canada) # レベルVARモデルを推定 model_var &lt;- vars::VAR(Canada, # 内生変数を含むデータセット p = 3, # ラグ次数 type = &quot;both&quot; # 確定項（&quot;both&quot;, &quot;const&quot;, &quot;trend&quot;, &quot;none&quot;） ) # ARCH-LM検定 # 帰無仮説：均一分散 test_arch &lt;- vars::arch.test(model_var, # 検定対象のモデルオブジェクト lags.multi = 5 # 他変量ARCH検定のラグ次数 ) print(test_arch) ## ## ARCH (multivariate) ## ## data: Residuals of VAR object model_var ## Chi-squared = 512.04, df = 500, p-value = 0.3451 ## $arch.mul ## ## ARCH (multivariate) ## ## data: Residuals of VAR object model_var ## Chi-squared = 512.04, df = 500, p-value = 0.3451 正規分布の検定 varsパッケージのnormality.test()関数で、ジャック＝ベラ検定を実行します。 ジャック＝ベラ検定を実行した結果、帰無仮説「正規分布」を棄却できません。したがって、正規性の観点では誤差項に問題はない、と判断します。 # Canadaデータセットを呼び出し data(Canada) # レベルVARモデルを推定 model_var &lt;- vars::VAR(Canada, # 内生変数を含むデータセット p = 3, # ラグ次数 type = &quot;both&quot; # 確定項（&quot;both&quot;, &quot;const&quot;, &quot;trend&quot;, &quot;none&quot;） ) # ジャック＝ベラ検定 # 帰無仮説：正規分布 test_jb &lt;- vars::normality.test(model_var, # 検定対象のモデルオブジェクト ) print(test_jb) ## $JB ## ## JB-Test (multivariate) ## ## data: Residuals of VAR object model_var ## Chi-squared = 9.6532, df = 8, p-value = 0.2902 ## ## ## $Skewness ## ## Skewness only (multivariate) ## ## data: Residuals of VAR object model_var ## Chi-squared = 4.4454, df = 4, p-value = 0.3491 ## ## ## $Kurtosis ## ## Kurtosis only (multivariate) ## ## data: Residuals of VAR object model_var ## Chi-squared = 5.2078, df = 4, p-value = 0.2666 ## $jb.mul ## $jb.mul$JB ## ## JB-Test (multivariate) ## ## data: Residuals of VAR object model_var ## Chi-squared = 9.6532, df = 8, p-value = 0.2902 ## ## ## $jb.mul$Skewness ## ## Skewness only (multivariate) ## ## data: Residuals of VAR object model_var ## Chi-squared = 4.4454, df = 4, p-value = 0.3491 ## ## ## $jb.mul$Kurtosis ## ## Kurtosis only (multivariate) ## ## data: Residuals of VAR object model_var ## Chi-squared = 5.2078, df = 4, p-value = 0.2666 8.11.11 実例：インパルス応答 馬場（2018）の第3部2章「VARモデル」の「2-10 Rによるインパルス応答関数」を再現します。 使用するデータセットは、fppパッケージのusconsumptionデータセットです。データ頻度は四半期、期間は1970年1-3月期〜2010年10-12月期です。格納されているデータは、実質個人消費支出（consumption、前期比、％）、実質可処分所得（income、前期比、％）です。 usconsumptionデータセットの消費・所得データはどちらも前期比変化率のため、ADF検定を行うと単位根をもたないI(0)過程であることが確認できます（ここではADF検定は省略します）。 また、VARモデル推定の準備としてvars::VARselect()関数でtype = \"const\"として定数項のあるモデルでラグ次数の選択を行うと、AICで5次のラグ次数が選択されます。そこで、ここではラグ次数が5次で定数項があるVARモデルを推定し、その結果をもとに、varsパッケージのirf()関数でインパルス応答（直交化インパルス応答）を求めます。なお、変数は実質個人消費支出、実質可処分所得の順番です。 例として、消費・所得それぞれに発生したショックにより、消費と所得にどのような影響が現れるかをインパルス応答関数で計算します。結果を図示すると、消費のショックは0期目（ショックと同時点）から消費と所得の両方に影響し、その後1年程度で減衰していくことが確認できます。一方、所得は変数の順序を後にしているため、所得のショックは0期目の消費には全く影響しません。その後も、所得のショックは消費にはあまり影響しないことが確認できます。 # usconsumptionデータを格納 data_usconsumption &lt;- fpp::usconsumption # レベルVARモデルの推定（変数は消費、所得の順番） model_var_usconsumption &lt;- vars::VAR(y = data_usconsumption[, c(&quot;consumption&quot;, &quot;income&quot;)], # 内生変数を含むデータセット p = 5, # ラグ次数 type = &quot;const&quot;, # 確定項（&quot;both&quot;, &quot;const&quot;, &quot;trend&quot;, &quot;none&quot;） exogen = NULL # 外生変数 ) # 消費のショックに対するインパルス応答 irf_consumption &lt;- vars::irf(x = model_var_usconsumption, # インパルス応答を計算するモデルオブジェクト（VAR、SVAR、SVEC、vec2varの出力） impulse = &quot;consumption&quot;, # ショックを発生させる変数（デフォルトは全ての変数） response = c(&quot;consumption&quot;, &quot;income&quot;), # ショックを受ける変数（デフォルトは全ての変数） n.ahead = 12, # 何期先までインパルス応答を計算するか ortho = TRUE, # 直交化インパルス応答の場合はTRUE（VAR()関数の出力を用いる場合のみ） boot = TRUE # ブートストラップ法による信頼区間を求める場合はTRUE ) plot(irf_consumption) # 所得のショックに対するインパルス応答 irf_income &lt;- vars::irf(x = model_var_usconsumption, # インパルス応答を計算するモデルオブジェクト impulse = &quot;income&quot;, # ショックを発生させる変数（デフォルトは全ての変数） response = c(&quot;consumption&quot;, &quot;income&quot;), # ショックを受ける変数（デフォルトは全ての変数） n.ahead = 12, # 何期先までインパルス応答を計算するか ortho = TRUE, # 直交化インパルス応答の場合はTRUE（VAR()関数の出力を用いる場合のみ） boot = TRUE # ブートストラップ法による信頼区間を求める場合はTRUE ) plot(irf_income) VARモデルを推定する際に変数の順序を実質可処分所得、実質個人消費支出の順番に逆転させると、インパルス応答（直交化インパルス応答）の結果が変化します。この場合は、消費のショックが0期目の所得に影響しなくなる一方、所得のショックが0期目の消費に影響するようになります。 # usconsumptionデータを格納 data_usconsumption &lt;- fpp::usconsumption # レベルVARモデルの推定（変数は所得、消費の順番） model_var_usconsumption &lt;- vars::VAR(y = data_usconsumption[, c(&quot;income&quot;, &quot;consumption&quot;)], # 内生変数を含むデータセット p = 5, # ラグ次数 type = &quot;const&quot;, # 確定項（&quot;both&quot;, &quot;const&quot;, &quot;trend&quot;, &quot;none&quot;） exogen = NULL # 外生変数 ) # 消費のショックに対するインパルス応答 irf_consumption &lt;- vars::irf(x = model_var_usconsumption, # インパルス応答を計算するモデルオブジェクト（VAR、SVAR、SVEC、vec2varの出力） impulse = &quot;consumption&quot;, # ショックを発生させる変数（デフォルトは全ての変数） response = c(&quot;consumption&quot;, &quot;income&quot;), # ショックを受ける変数（デフォルトは全ての変数） n.ahead = 12, # 何期先までインパルス応答を計算するか ortho = TRUE, # 直交化インパルス応答の場合はTRUE（VAR()関数の出力を用いる場合のみ） boot = TRUE # ブートストラップ法による信頼区間を求める場合はTRUE ) plot(irf_consumption) # 所得のショックに対するインパルス応答 irf_income &lt;- vars::irf(x = model_var_usconsumption, # インパルス応答を計算するモデルオブジェクト impulse = &quot;income&quot;, # ショックを発生させる変数（デフォルトは全ての変数） response = c(&quot;consumption&quot;, &quot;income&quot;), # ショックを受ける変数（デフォルトは全ての変数） n.ahead = 12, # 何期先までインパルス応答を計算するか ortho = TRUE, # 直交化インパルス応答の場合はTRUE（VAR()関数の出力を用いる場合のみ） boot = TRUE # ブートストラップ法による信頼区間を求める場合はTRUE ) plot(irf_income) 8.11.12 実例：予測誤差の分散分解 馬場（2018）の第3部2章「VARモデル」の「2-10 Rによるインパルス応答関数」の後半に記載されている、予測誤差の分散分解（forecast error variance decomposition, FEVD）を再現します。使用するデータセットは、「2-10 Rによるインパルス応答関数」と同じfppパッケージのusconsumptionデータセットです。分散分解はvarsパッケージのfevd()関数で求めます。 まず、変数の順序を消費、所得としてVARモデルを推定し、その推定結果をもとに分散分解を求めます。分散分解の結果を図示すると、消費は所得の影響をほとんど受けていませんが、所得は1〜5期にかけて消費の影響が25％程度まで徐々に拡大していくことが確認できます。 # usconsumptionデータを格納 data_usconsumption &lt;- fpp::usconsumption # レベルVARモデルの推定（変数は消費、所得の順番） model_var_usconsumption &lt;- vars::VAR(y = data_usconsumption[, c(&quot;consumption&quot;, &quot;income&quot;)], # 内生変数を含むデータセット p = 5, # ラグ次数 type = &quot;const&quot;, # 確定項（&quot;both&quot;, &quot;const&quot;, &quot;trend&quot;, &quot;none&quot;） exogen = NULL # 外生変数 ) # 分散分解 fevd_usconsumption &lt;- vars::fevd(x = model_var_usconsumption, # 分散分解を計算するモデルオブジェクト（VAR、SVAR、SVEC、vec2varの出力） n.ahead = 12, # 何期先まで分散分解を計算するか ) plot(fevd_usconsumption) 次に、VARモデルの推定における変数の順序を所得、消費に変更し、分散分解を求め直します。すると、消費に対して所得が10％程度の影響度をもつようになりました。このように、分散分解もインパルス応答（直交化インパルス応答）と同様に変数の順序によって結果が変わりうる点に注意が必要です。 # usconsumptionデータを格納 data_usconsumption &lt;- fpp::usconsumption # レベルVARモデルの推定（変数は所得、消費の順番） model_var_usconsumption &lt;- vars::VAR(y = data_usconsumption[, c(&quot;income&quot;, &quot;consumption&quot;)], # 内生変数を含むデータセット p = 5, # ラグ次数 type = &quot;const&quot;, # 確定項（&quot;both&quot;, &quot;const&quot;, &quot;trend&quot;, &quot;none&quot;） exogen = NULL # 外生変数 ) # 分散分解 fevd_usconsumption &lt;- vars::fevd(x = model_var_usconsumption, # 分散分解を計算するモデルオブジェクト（VAR、SVAR、SVEC、vec2varの出力） n.ahead = 12, # 何期先まで分散分解を計算するか ) plot(fevd_usconsumption) 8.11.13 実例：グレンジャー因果性検定 馬場（2018）の第3部2章「VARモデル」の「2-9 RによるGranger因果性検定」を再現します。使用するデータセットは、「2-10 Rによるインパルス応答関数」と同じfppパッケージのusconsumptionデータセットです。分散分解はvarsパッケージのcausality()関数で求めます。 あらかじめ、変数の順序を消費、所得としてVARモデルを推定します。 # usconsumptionデータを格納 data_usconsumption &lt;- fpp::usconsumption # レベルVARモデルの推定（変数は消費、所得の順番） model_var_usconsumption &lt;- vars::VAR(y = data_usconsumption[, c(&quot;consumption&quot;, &quot;income&quot;)], # 内生変数を含むデータセット p = 5, # ラグ次数 type = &quot;const&quot;, # 確定項（&quot;both&quot;, &quot;const&quot;, &quot;trend&quot;, &quot;none&quot;） exogen = NULL # 外生変数 ) VARモデルの推定結果を使い、まず所得から消費へのグレンジャー因果を検証します。原因変数が所得なので、vars::causality()関数のcause引数に\"income\"を指定します。vars::causality()関数の出力のうち、Grangerがグレンジャー因果性検定の結果、Instantがグレンジャー瞬間的因果性検定の結果を示しています。 グレンジャー因果性検定ではp値が0.212と5％有意水準を上回り、「グレンジャー因果性がない」との帰無仮説を棄却できないため、所得から消費へのグレンジャー因果があるとは判断できません。 一方、グレンジャー瞬間的因果性検定ではp値が5％有意水準より小さく、「グレンジャー瞬間的因果性がない」との帰無仮説を棄却するため、所得から消費へのグレンジャー瞬間的因果があると判断します。これは、所得が同時点の消費に影響し、VARモデルにおける所得の式の残差と消費の式の残差に関連性があることを意味しています。 # 所得から消費へのグレンジャー因果 vars::causality(model_var_usconsumption, # VARモデルの推計結果 cause = &quot;income&quot;, # 原因変数（文字列で指定。ベクトルにして2種類以上の原因変数を指定することも可能） vcov. = NULL, # 共和分行列を指定（デフォルトはNULL） boot = FALSE, # 棄却点をブートストラップ法で計算する場合はTRUE（デフォルトはFALSE） boot.runs = 1000 # ブートストラップ法の繰り返し数 ) ## $Granger ## ## Granger causality H0: income do not Granger-cause consumption ## ## data: VAR object model_var_usconsumption ## F-Test = 1.4337, df1 = 5, df2 = 296, p-value = 0.212 ## ## ## $Instant ## ## H0: No instantaneous causality between: income and consumption ## ## data: VAR object model_var_usconsumption ## Chi-squared = 15.492, df = 1, p-value = 8.285e-05 次に、消費から所得へのグレンジャー因果を検証します。グレンジャー因果性検定、グレンジャー瞬間的因果性検定ともにp値が有意水準5％より小さく、消費から所得へのグレンジャー因果性、グレンジャー瞬間的因果性があると判断します。 # 消費から所得へのグレンジャー因果 vars::causality(model_var_usconsumption, # VARモデルの推計結果 cause = &quot;consumption&quot;, # 原因変数（文字列で指定。ベクトルにして2種類以上の原因変数を指定することも可能） vcov. = NULL, # 共和分行列を指定（デフォルトはNULL） boot = FALSE, # 棄却点をブートストラップ法で計算する場合はTRUE（デフォルトはFALSE） boot.runs = 1000 # ブートストラップ法の繰り返し数 ) ## $Granger ## ## Granger causality H0: consumption do not Granger-cause income ## ## data: VAR object model_var_usconsumption ## F-Test = 10.575, df1 = 5, df2 = 296, p-value = 2.334e-09 ## ## ## $Instant ## ## H0: No instantaneous causality between: consumption and income ## ## data: VAR object model_var_usconsumption ## Chi-squared = 15.492, df = 1, p-value = 8.285e-05 8.11.14 実例：予測 馬場（2018）の第3部2章「VARモデル」の「2-8 VARモデルによる予測」をもとに、VARモデルによる予測精度を評価します。 使用するデータセットは、「2-10 Rによるインパルス応答関数」と同じfppパッケージのusconsumptionデータセットです。データ頻度は四半期、期間は1970年1-3月期〜2010年10-12月期です。格納されているデータは、実質個人消費支出（consumption、前期比、％）、実質可処分所得（income、前期比、％）です。usconsumptionデータセットの消費・所得データはどちらも前期比変化率のため、ADF検定を行うと単位根をもたないI(0)過程であることが確認できます（ここではADF検定は省略します）。また、VARモデル推定の準備としてvars::VARselect()関数でtype = \"const\"として定数項のあるモデルでラグ次数の選択を行うと、AICで5次のラグ次数が選択されます。 まず、予測精度を評価するため、データを訓練データとテストデータに分割します。ここでは、1970年1-3月期〜2010年10-12月期の41年間のうち、最後5年間（2006年1-3月期以降）をテストデータとしてホールドアウトしておきます。 # usconsumptionデータを格納 data_usconsumption &lt;- fpp::usconsumption # 訓練データ（2005年10-12月期以前） data_train &lt;- stats::window(data_usconsumption, end = c(2005, 4)) tail(data_train) ## consumption income ## 2004 Q3 0.8703787 0.6660822 ## 2004 Q4 1.0739515 1.3889037 ## 2005 Q1 0.7939389 -1.2255228 ## 2005 Q2 0.9847789 0.7016735 ## 2005 Q3 0.7562780 0.5947781 ## 2005 Q4 0.2481979 0.5453283 # テストデータ（2006年1-3月期以降） data_test &lt;- stats::window(data_usconsumption, start = c(2006, 1)) head(data_test) ## consumption income ## 2006 Q1 1.0190271 1.8556786 ## 2006 Q2 0.6004822 0.8823418 ## 2006 Q3 0.5980000 0.4791447 ## 2006 Q4 0.9258411 1.3023574 ## 2007 Q1 0.5542412 0.4513898 ## 2007 Q2 0.3825796 0.1493349 次に、訓練データでVARモデルのラグ次数を選択します。ここでは、確定項は定数項のみを意味するconstとしておきます。全てのデータを用いてラグ次数を選択したときと同様に、AICで最適なラグ次数が「5」になることを踏まえ、ラグ次数は「5」を選択します。 varselect_train &lt;- vars::VARselect(y = data_train, # 内生変数を含むデータセット type = &quot;const&quot;, # モデルに含む確定項（&quot;const&quot;, &quot;trend&quot;, &quot;both&quot;, &quot;none&quot;） lag.max = 10 # 検査するラグ次数の最大値 ) print(varselect_train) ## $selection ## AIC(n) HQ(n) SC(n) FPE(n) ## 5 1 1 5 ## ## $criteria ## 1 2 3 4 5 6 ## AIC(n) -1.2852084 -1.2458673 -1.2632626 -1.2554251 -1.2877577 -1.2388627 ## HQ(n) -1.2324805 -1.1579875 -1.1402309 -1.0972415 -1.0944222 -1.0103754 ## SC(n) -1.1554544 -1.0296106 -0.9605032 -0.8661630 -0.8119929 -0.6765953 ## FPE(n) 0.2765971 0.2877112 0.2827839 0.2850701 0.2760931 0.2900685 ## 7 8 9 10 ## AIC(n) -1.1953796 -1.1813493 -1.1430871 -1.1109326 ## HQ(n) -0.9317404 -0.8825582 -0.8091441 -0.7418377 ## SC(n) -0.5466095 -0.4460765 -0.3213116 -0.2026544 ## FPE(n) 0.3031598 0.3077092 0.3200623 0.3309713 訓練データでVARモデルを推定し、予測値を作成します。予測値の作成にはstats::predict()関数を使用します。 # レベルVARモデルの推定（変数は消費、所得の順番） model_var_train &lt;- vars::VAR(y = data_train[, c(&quot;consumption&quot;, &quot;income&quot;)], # 内生変数を含むデータセット p = 5, # ラグ次数 type = &quot;const&quot;, # 確定項（&quot;both&quot;, &quot;const&quot;, &quot;trend&quot;, &quot;none&quot;） exogen = NULL # 外生変数 ) # 予測値を作成 pred_train &lt;- stats::predict(model_var_train, # 予測値を計算するモデルオブジェクト（VAR、vec2varの出力） n.ahead = nrow(data_test), # 何期先まで予測値を作成するか ci = 0.95, # 予測の信頼区間 dumvar = NULL # ダミー変数を含む外生変数（行列形式） ) # 予測値を図示 autoplot(pred_train) 最後に、forecastパッケージのaccuracy()関数で予測精度を評価します。 消費の予測精度をRMSEで評価すると、VARモデル予測は0.849、ナイーブ予測（過去の平均値）は0.847、ナイーブ予測（最後の値）は0.620と、残念ながらナイーブ予測の精度より悪い結果になりました。 # 消費のVARモデル予測値の精度 forecast::accuracy(data_test[, &quot;consumption&quot;], # ホールドアウトしたテストデータ pred_train$fcst$consumption[, &quot;fcst&quot;] # 作成した予測値 ) ## ME RMSE MAE MPE MAPE ## Test set 0.5796194 0.8493349 0.6078776 69.91951 73.23117 # ナイーブ予測（過去の平均値）の精度 pred_meanf &lt;- forecast::meanf(y = data_train[, &quot;consumption&quot;], # 訓練データの目的変数 h = 20, # 何期先まで予測するか level = 95 # 出力する予測値の信頼区間。複数出力するときはc(95, 70)などとベクトルで指定 ) forecast::accuracy(data_test[, &quot;consumption&quot;], # ホールドアウトしたテストデータ pred_meanf$mean ) ## ME RMSE MAE MPE MAPE ACF1 Theil&#39;s U ## Test set 0.576444 0.8472476 0.6107782 69.81901 73.97757 0.6536854 Inf # ナイーブ予測（最後の値）の精度 pred_rwf &lt;- forecast::rwf(y = data_train[, &quot;consumption&quot;], # 訓練データの目的変数 h = 20, # 何期先まで予測するか level = 95 # 出力する予測値の信頼区間。複数出力するときはc(95, 70)などとベクトルで指定 ) forecast::accuracy(data_test[, &quot;consumption&quot;], # ホールドアウトしたテストデータ pred_rwf$mean ) ## ME RMSE MAE MPE MAPE ACF1 ## Test set -0.0009843402 0.62092 0.5068942 -0.3965949 204.2299 0.6536854 ## Theil&#39;s U ## Test set Inf 同様に所得の予測精度をRMSEで評価すると、VARモデル予測は1.097、ナイーブ予測（過去の平均値）は1.101、ナイーブ予測（最後の値）は1.044と、ナイーブ予測（過去の平均値）の精度より良いものの、ナイーブ予測（最後の値）の精度より悪い結果になりました。VARモデルの予測精度が悪い背景として、予測期間（2006-2010年）にGlobal Financial Crisisが発生し、消費、所得ともに外的要因で大きく変動したことがあると考えられます。 # 所得のVARモデル予測値の精度 forecast::accuracy(data_test[, &quot;income&quot;], # ホールドアウトしたテストデータ pred_train$fcst$income[, &quot;fcst&quot;] # 作成した予測値 ) ## ME RMSE MAE MPE MAPE ## Test set 0.3852657 1.097126 0.8229979 48.80035 104.1688 # ナイーブ予測（過去の平均値）の精度 pred_meanf &lt;- forecast::meanf(y = data_train[, &quot;income&quot;], # 訓練データの目的変数 h = 20, # 何期先まで予測するか level = 95 # 出力する予測値の信頼区間。複数出力するときはc(95, 70)などとベクトルで指定 ) forecast::accuracy(data_test[, &quot;income&quot;], # ホールドアウトしたテストデータ pred_meanf$mean ) ## ME RMSE MAE MPE MAPE ACF1 Theil&#39;s U ## Test set 0.3758971 1.100584 0.8268816 48.04421 105.6855 0.1283031 Inf # ナイーブ予測（最後の値）の精度 pred_rwf &lt;- forecast::rwf(y = data_train[, &quot;income&quot;], # 訓練データの目的変数 h = 20, # 何期先まで予測するか level = 95 # 出力する予測値の信頼区間。複数出力するときはc(95, 70)などとベクトルで指定 ) forecast::accuracy(data_test[, &quot;income&quot;], # ホールドアウトしたテストデータ pred_rwf$mean ) ## ME RMSE MAE MPE MAPE ACF1 Theil&#39;s U ## Test set 0.1388272 1.043676 0.757668 25.45754 138.938 0.1283031 Inf "],["付録1-寄与度の計算方法.html", "付録1 寄与度の計算方法 記号法 和・差の指標 積の指標 商の指標 固定加重平均の指標 可変加重平均の指標", " 付録1 寄与度の計算方法 寄与度分解とは、経済・金融指標の変動をその構成項目別に要因分解し、指標全体の動きにどの構成項目がどれくらい影響したかを明らかにする分析手法です。ここでは、様々なタイプの指標の変動（変化率、階差）を構成項目の寄与度に分解する計算方法を紹介します。 記号法 変数\\(X_t\\)の添え字\\(t\\)は、時系列インデックスを表します。 \\(\\Delta\\)記号は変数の階差を表します。例えば、変数\\(X_t\\)について\\(\\Delta{X_t}\\)と表記した場合、\\(\\Delta{X_t} = X_t - X_{t-1}\\)を意味します。したがって、\\(t\\)期の変数\\(X_t\\)は前期の値\\(X_{t-1}\\)と階差\\(\\Delta{X_t}\\)を使って\\(X_t = X_{t-1} + \\Delta{X_t}\\)に変形することができます。なお、ここでは\\(t-1\\)を「前期」としていますが、「前年」や「前年同期」でも計算方法は変わらないため、適宜読み替えてください。 \\(\\Delta{X_t}\\)を前期の値\\(X_{t-1}\\)で割った\\(\\Delta{X_t}/X_{t-1}\\)は、変数\\(X_t\\)の変化率を表します。変化率をパーセント表示する場合は\\(100 \\times \\Delta{X_t}/X_{t-1}\\)とします。 また、等号\\(=\\)の代わりに\\(\\approx\\)を使用することがあります。\\(\\approx\\)は左辺の値が右辺の値で近似できることを示しています。 和・差の指標 複数の構成項目の足し算・引き算で作成する指標の寄与度分解です。 公式 \\[ \\begin{aligned} 元の式： &amp;Y_t = X_{1,t} + X_{2,t} \\\\ \\\\ 寄与度分解： &amp;\\frac{\\Delta{Y_t}}{Y_{t-1}} = \\frac{\\Delta{X_{1,t}}}{Y_{t-1}} + \\frac{\\Delta{X_{2,t}}}{Y_{t-1}} \\end{aligned} \\] ある指標\\(Y_t\\)が、構成項目\\(X_{1,t}\\)、\\(X_{2,t}\\)によって次の式で作成されているとします。 \\[ Y_t = X_{1,t} + X_{2,t} \\] 両辺の階差をとると、 \\[ Y_t - Y_{t-1} = (X_{1,t} - X_{1,t-1}) + (X_{2,t} - X_{2,t-1}) \\] となり、これは次の式に書き換えられます。 \\[ \\Delta{Y_t} = \\Delta{X_{1,t}} + \\Delta{X_{2,t}} \\] ここで、両辺を\\(Y_{t-1}\\)で割ると、 \\[ \\frac{\\Delta{Y_t}}{Y_{t-1}} = \\frac{\\Delta{X_{1,t}}}{Y_{t-1}} + \\frac{\\Delta{X_{2,t}}}{Y_{t-1}} \\] となります。これは、変数\\(Y_t\\)の変化率\\(\\Delta{Y_t}/Y_{t-1}\\)が、\\(X_{1,t}\\)の寄与度\\(\\Delta{X_{1,t}}/Y_{t-1}\\)と、\\(X_{2,t}\\)の寄与度\\(\\Delta{X_{2,t}}/Y_{t-1}\\)に分解できることを示しています。 実例：GDP 「GDP＝消費\\(C_t\\)＋投資\\(I_t\\)＋政府支出\\(G_t\\)＋輸出\\(\\mathit{EX}_t\\)−輸入\\(\\mathit{IM}_t\\)」であることを利用して、GDPの変化率を構成項目の寄与度に分解します。 両辺の階差をとってGDPの前期の値\\(\\mathit{GDP}_{t-1}\\)で割ると、寄与度分解の式は、 \\[ \\frac{\\Delta{\\mathit{GDP}_t}}{\\mathit{GDP}_{t-1}} = \\frac{C_t}{\\mathit{GDP}_{t-1}} + \\frac{I_t}{\\mathit{GDP}_{t-1}} + \\frac{G_t}{\\mathit{GDP}_{t-1}} + \\frac{\\mathit{EX}_t}{\\mathit{GDP}_{t-1}} - \\frac{\\mathit{IM}_t}{\\mathit{GDP}_{t-1}} \\] となります。輸入は控除項目のため、寄与度の符号がマイナスになっている点に注意してください。 なお、GDPには名目原数値、名目季節調整値、実質原数値、実質季節調整値がありますが、上の寄与度分解の式を用いた場合に誤差が発生しないのは名目原数値のみです。季節調整値や実質値では構成項目の合計がGDPに一致しないため、必ず誤差が発生します。 実質GDP変化率の寄与度について、内閣府の公表値では特殊な計算方法が採用されていますので、こちらのページを参照してください。 積の指標 複数の構成項目の掛け算で作成する指標の寄与度分解です。 公式 \\[ \\begin{aligned} 元の式： &amp;Y_t = X_{1,t} X_{2,t} \\\\ \\\\ 寄与度分解： &amp;\\frac{\\Delta{Y_t}}{Y_{t-1}} \\approx \\frac{\\Delta{X_{1,t}}}{X_{1,t-1}} + \\frac{\\Delta{X_{2,t}}}{X_{2,t-1}} \\end{aligned} \\] ある指標\\(Y_t\\)が、構成項目\\(X_{1,t}\\)、\\(X_{2,t}\\)によって次の式で作成されているとします。 \\[ Y_t = X_{1,t} X_{2,t} \\] \\(X_t = X_{t-1} + \\Delta{X_t}\\)を利用すると、上の式は次のように書き換えることができます。 \\[ \\begin{aligned} Y_{t-1} + \\Delta{Y_t} &amp;= (X_{1,t-1} + \\Delta{X_{1,t}}) (X_{2,t-1} + \\Delta{X_{2,t}}) \\\\ \\\\ &amp;= X_{1,t-1} X_{2,t-1} + X_{2,t-1} \\Delta{X_{1,t}} + X_{1,t-1} \\Delta{X_{2,t}} + \\Delta{X_{1,t}} \\Delta{X_{2,t}} \\end{aligned} \\] 両辺を\\(Y_{t-1}\\)で割り、\\(Y_{t-1} = X_{1,t-1} X_{2,t-1}\\)を使って整理すると、 \\[ \\frac{\\Delta{Y_t}}{Y_{t-1}} = \\frac{\\Delta{X_{1,t}}}{X_{1,t-1}} + \\frac{\\Delta{X_{2,t}}}{X_{2,t-1}} + \\frac{\\Delta{X_{1,t}} \\Delta{X_{2,t}}}{X_{1,t-1} X_{2,t-1}} \\] となります。これは、変数\\(Y_t\\)の変化率\\(\\Delta{Y_t}/Y_{t-1}\\)が、\\(X_{1,t}\\)の寄与度\\(\\Delta{X_{1,t}}/X_{1,t-1}\\)と、\\(X_{2,t}\\)の寄与度\\(\\Delta{X_{2,t}}/X_{2,t-1}\\)に分解できることを示しています。 右辺第3項は\\(X_{1,t}\\)と\\(X_{2,t}\\)どちらの階差にも帰属させることができない交差項ですが、通常は小さな値になるため、誤差として処理します。右辺第3項を省略すると、 \\[ \\frac{\\Delta{Y_t}}{Y_{t-1}} \\approx \\frac{\\Delta{X_{1,t}}}{X_{1,t-1}} + \\frac{\\Delta{X_{2,t}}}{X_{2,t-1}} \\] になり、変数\\(Y_t\\)の変化率\\(\\Delta{Y_t}/Y_{t-1}\\)が、構成項目の変化率の足し算で近似できることが確認できます。 実例：貿易指数（金額指数） 貿易統計の貿易指数において、金額指数が「金額指数\\(V_t\\)＝価格指数\\(U_t\\)×数量指数\\(Q_t\\)÷100」で計算できることを利用して、金額指数の変化率を構成項目の寄与度に分解します。 \\(X_t = X_{t-1} + \\Delta{X_t}\\)を利用すると、定義式は、 \\[ V_{t-1} + \\Delta{V_t} = \\frac{(U_{t-1} + \\Delta{U_{t}}) (Q_{t-1} + \\Delta{Q_{t}})}{100} \\] と書き換えることができます。次に、両辺を\\(V_{t-1}\\)で割り、\\(V_{t-1} = U_{t-1}Q_{t-1}/100\\)を利用して整理すると、 \\[ \\frac{\\Delta{V_t}}{V_{t-1}} = \\frac{\\Delta{U_{t}}}{U_{t-1}} + \\frac{\\Delta{Q_{t}}}{Q_{t-1}} + \\frac{\\Delta{U_{t}} \\Delta{Q_{t}}}{U_{t-1}Q_{t-1}} \\] と寄与度分解できます。右辺第3項は価格指数\\(U_t\\)と数量指数\\(Q_t\\)の交差項であり、誤差として処理します。 実例：経常利益 法人企業統計の経常利益が、「経常利益\\(\\mathit{CP}_t\\)＝売上高\\(\\mathit{SL}_t\\)×限界利益率\\(\\alpha_t\\)−人件費\\(\\mathit{HC}_t\\)＋営業外損益\\(\\mathit{NO}_t\\)」で計算できることを利用して、経常利益の変化率を構成項目の寄与度に分解します。なお、「限界利益＝売上高−変動費」、「変動費＝売上原価＋販管費−人件費」です。 まず、和・差の指標の公式を利用すると、経常利益の定義式は、 \\[ \\frac{\\Delta{\\mathit{CP}_t}}{\\mathit{CP}_{t-1}} = \\frac{\\Delta{(\\mathit{SL}_t \\times \\alpha_t})}{\\mathit{CP}_{t-1}} - \\frac{\\Delta{\\mathit{HC}_t}}{\\mathit{CP}_{t-1}} + \\frac{\\Delta{\\mathit{NO}_t}}{\\mathit{CP}_{t-1}} \\] となります。ここで、積の指標の公式から、 \\[ \\frac{\\Delta{(\\mathit{SL}_t \\times \\alpha_t})}{\\mathit{SL}_{t-1} \\times \\alpha_{t-1}} = \\frac{\\Delta{\\mathit{SL}_t}}{\\mathit{SL}_{t-1}} + \\frac{\\Delta{\\alpha_t}}{\\alpha_{t-1}} + \\frac{\\Delta{\\mathit{SL}_t} \\times \\Delta{\\alpha_t}}{\\mathit{SL}_{t-1} \\times \\alpha_{t-1}} \\] であることを利用し、上の式に代入して整理すると、 \\[ \\frac{\\Delta{\\mathit{CP}_t}}{\\mathit{CP}_{t-1}} = \\frac{\\alpha_{t-1}}{\\mathit{CP}_{t-1}}\\Delta{\\mathit{SL}_t} + \\frac{\\mathit{SL}_{t-1}}{\\mathit{CP}_{t-1}}\\Delta{\\alpha_t} - \\frac{\\Delta{\\mathit{HC}_t}}{\\mathit{CP}_{t-1}} + \\frac{\\Delta{\\mathit{NO}_t}}{\\mathit{CP}_{t-1}} + \\frac{\\Delta{\\mathit{SL}_t} \\times \\Delta{\\alpha_t}}{\\mathit{CP}_{t-1}} \\] と寄与度分解できます。右辺第5項は売上高\\(\\mathit{SL}_t\\)と限界利益率\\(\\alpha_t\\)の交差項であり、誤差として処理します。 商の指標 複数の構成項目の割り算で作成する指標の寄与度分解です。 公式 \\[ \\begin{aligned} 元の式： &amp;Y_t = \\frac{X_{1,t}}{X_{2,t}} \\\\ \\\\ 寄与度分解： &amp;\\frac{\\Delta{Y_t}}{Y_{t-1}} \\approx \\frac{\\Delta{X_{1,t}}}{X_{1,t-1}} - \\frac{\\Delta{X_{2,t}}}{X_{2,t-1}} \\end{aligned} \\] ある指標\\(Y_t\\)が、構成項目\\(X_{1,t}\\)、\\(X_{2,t}\\)によって次の式で作成されているとします。 \\[ Y_t = \\frac{X_{1,t}}{X_{2,t}} \\] \\(X_t = X_{t-1} + \\Delta{X_t}\\)を利用すると、上の式は次のように書き換えることができます。 \\[ Y_{t-1} + \\Delta{Y_t} = \\frac{X_{1,t-1} + \\Delta{X_{1,t}}}{X_{2,t-1} + \\Delta{X_{2,t}}} \\] 両辺を\\(Y_{t-1}\\)で割り、\\(Y_{t-1} = X_{1,t-1} X_{2,t-1}\\)を使って整理すると、 \\[ \\begin{aligned} 1 + \\frac{\\Delta{Y_t}}{Y_{t-1}} &amp;= \\frac{X_{1,t-1} + \\Delta{X_{1,t}}}{X_{2,t-1} + \\Delta{X_{2,t}}} \\times \\frac{X_{2,t-1}}{X_{1,t-1}} \\\\ \\\\ &amp;= \\frac{1 + \\Delta{X_{1,t}}/X_{1,t-1}}{1 + \\Delta{X_{2,t}}/X_{2,t-1}} \\end{aligned} \\] となります。ここで、 \\[ \\Bigl( 1 + \\frac{\\Delta{X_{1,t}}}{X_{1,t-1}} - \\frac{\\Delta{X_{2,t}}}{X_{2,t-1}} \\Bigr) \\Bigl( 1 + \\frac{\\Delta{X_{2,t}}}{X_{2,t-1}} \\Bigr) = 1 + \\frac{\\Delta{X_{1,t}}}{X_{1,t-1}} + \\frac{\\Delta{X_{1,t}}}{X_{1,t-1}}\\frac{\\Delta{X_{2,t}}}{X_{2,t-1}} - \\Bigl( \\frac{\\Delta{X_{2,t}}}{X_{2,t-1}} \\Bigr)^2 \\] の両辺を\\(1+\\Delta{X_{2,t}}/X_{2,t-1}\\)で割って整理すると、 \\[ \\frac{1 + \\Delta{X_{1,t}}/X_{1,t-1}}{1 + \\Delta{X_{2,t}}/X_{2,t-1}} = 1 + \\frac{\\Delta{X_{1,t}}}{X_{1,t-1}} - \\frac{\\Delta{X_{2,t}}}{X_{2,t-1}} - \\frac{X_{2,t-1}\\Delta{X_{1,t}}\\Delta{X_{2,t}} - X_{1,t-1} (\\Delta{X_{2,t}})^2}{X_{1,t-1}X_{2,t-1}X_{2,t}} \\] となることから、この結果を前の式に代入すると、\\(\\Delta{Y_t}/Y_{t-1}\\)は、 \\[ \\frac{\\Delta{Y_t}}{Y_{t-1}} = \\frac{\\Delta{X_{1,t}}}{X_{1,t-1}} - \\frac{\\Delta{X_{2,t}}}{X_{2,t-1}} - \\frac{X_{2,t-1}\\Delta{X_{1,t}}\\Delta{X_{2,t}} - X_{1,t-1} (\\Delta{X_{2,t}})^2}{X_{1,t-1}X_{2,t-1}X_{2,t}} \\] となります。これは、変数\\(Y_t\\)の変化率\\(\\Delta{Y_t}/Y_{t-1}\\)が、\\(X_{1,t}\\)の寄与度\\(\\Delta{X_{1,t}}/X_{1,t-1}\\)と、\\(X_{2,t}\\)の寄与度\\(-\\Delta{X_{2,t}}/X_{2,t-1}\\)に分解できることを示しています。 右辺第3項は\\(X_{1,t}\\)と\\(X_{2,t}\\)どちらの階差にも帰属させることができない交差項ですが、通常は小さな値になるため、誤差として処理します。右辺第3項を省略すると、 \\[ \\frac{\\Delta{Y_t}}{Y_{t-1}} \\approx \\frac{\\Delta{X_{1,t}}}{X_{1,t-1}} - \\frac{\\Delta{X_{2,t}}}{X_{2,t-1}} \\] になり、変数\\(Y_t\\)の変化率\\(\\Delta{Y_t}/Y_{t-1}\\)が、構成項目の変化率の引き算で近似できることが確認できます。 実例：貿易指数（数量指数） 貿易統計の貿易指数において、数量指数が「数量指数\\(Q_t\\)＝100×金額指数\\(V_t\\)÷価格指数\\(U_t\\)」で計算できることを利用して、数量指数の変化率を構成項目の寄与度に分解します。 \\(X_t = X_{t-1} + \\Delta{X_t}\\)を利用すると、定義式は、 \\[ Q_{t-1} + \\Delta{Q_t} = 100 \\times \\frac{V_{t-1} + \\Delta{V_{t}}}{U_{t-1} + \\Delta{U_{t}}} \\] と書き換えることができます。次に、両辺を\\(Q_{t-1}\\)で割り、\\(Q_{t-1} = 100 \\times V_{t-1}/U_{t-1}\\)を利用して整理すると、 \\[ \\frac{\\Delta{Q_t}}{Q_{t-1}} = \\frac{\\Delta{V_{t}}}{V_{t-1}} - \\frac{\\Delta{U_{t}}}{U_{t-1}} - \\frac{U_{t-1}\\Delta{V_{t}}\\Delta{U_{t}} - V_{t-1} (\\Delta{U_{t}})^2}{V_{t-1}U_{t-1}U_{t}} \\] と寄与度分解できます。右辺第3項は金額指数\\(V_t\\)と価格指数\\(U_t\\)の交差項であり、誤差として処理します。 実例：失業率 労働力調査の失業率が、「失業率\\(u_t\\)＝1−就業者数\\(E_t\\)÷(15歳以上人口\\(N_t\\)×労働力率\\(\\alpha_t\\))」で計算できることを利用して、失業率の階差を構成項目の寄与度に分解します。一般的な指標では変化率を寄与度分解しますが、失業率は割合のため変化率ではなく階差を寄与度分解します。 なお、失業率には様々な寄与度分解の方法がありますが、この分解方法は厚生労働省の『令和元年版 労働経済の分析』のコラム１－１図に基づいています。 \\(X_t = X_{t-1} + \\Delta{X_t}\\)を利用すると、定義式は、 \\[ \\begin{aligned} u_{t-1} + \\Delta{u_t} &amp;= 1 - \\frac{E_{t-1} + \\Delta{E_{t}}}{\\alpha_{t-1}N_{t-1} + \\Delta{(\\alpha_{t}N_t)}} \\\\ \\\\ &amp;= 1 - \\frac{E_{t-1}}{\\alpha_{t-1}N_{t-1}} \\times \\frac{1 + \\Delta{E_t}/E_{t-1}}{1 + \\alpha_{t-1}N_{t-1}/\\Delta{(\\alpha_{t}N_t)}} \\end{aligned} \\] と書き換えることができます。ここで商の指標の公式を利用すると、 \\[ u_{t-1} + \\Delta{u_t} = 1 - \\frac{E_{t-1}}{\\alpha_{t-1}N_{t-1}} \\times \\Bigl( 1 + \\frac{\\Delta{E_t}}{E_{t-1}} - \\frac{\\Delta{(\\alpha_{t}N_t)}}{\\alpha_{t-1}N_{t-1}} - \\frac{\\alpha_{t-1}N_{t-1}\\Delta{E_t}\\Delta{(\\alpha_tN_t)}-E_{t-1}(\\Delta{(\\alpha_tN_t)})^2}{E_{t-1}\\alpha_{t-1}N_{t-1}\\alpha_{t}N_{t}} \\Bigr) \\] となり、さらに\\(\\Delta{(\\alpha_tN_t)}\\)に対して積の公式を利用して、右辺の括弧内の第3項を分割すると、 \\[ u_{t-1} + \\Delta{u_t} = 1 - \\frac{E_{t-1}}{\\alpha_{t-1}N_{t-1}} \\times \\Bigl( 1 + \\frac{\\Delta{E_t}}{E_{t-1}} - \\frac{\\Delta{\\alpha_t}}{\\alpha_{t-1}} - \\frac{\\Delta{N_t}}{N_{t-1}} - \\frac{\\Delta{\\alpha_{t}}\\Delta{N_t}}{\\alpha_{t-1}N_{t-1}} - \\frac{\\alpha_{t-1}N_{t-1}\\Delta{E_t}\\Delta{(\\alpha_tN_t)}-E_{t-1}(\\Delta{(\\alpha_tN_t)})^2}{E_{t-1}\\alpha_{t-1}N_{t-1}\\alpha_{t}N_{t}} \\Bigr) \\] となります。定義式\\(u_{t-1} = 1 - E_{t-1}/(\\alpha_{t-1}N_{t-1})\\)を用いて整理すると、 \\[ \\Delta{u_t} = - \\frac{1}{\\alpha_{t-1}N_{t-1}}\\Delta{E_t} + \\frac{E_{t-1}}{\\alpha^2_{t-1}N_{t-1}}\\Delta{\\alpha_t} + \\frac{E_{t-1}}{\\alpha_{t-1}N^2_{t-1}}\\Delta{N_t} + \\frac{E_{t-1}}{\\alpha_{t-1}N_{t-1}} \\times \\Bigl( \\frac{\\Delta{\\alpha_{t}}\\Delta{N_t}}{\\alpha_{t-1}N_{t-1}} - \\frac{\\alpha_{t-1}N_{t-1}\\Delta{E_t}\\Delta{(\\alpha_tN_t)}-E_{t-1}(\\Delta{(\\alpha_tN_t)})^2}{E_{t-1}\\alpha_{t-1}N_{t-1}\\alpha_{t}N_{t}} \\Bigr) \\] と寄与度分解できます。右辺第1・2・3項がそれぞれ就業者数\\(E_t\\)、労働力率\\(\\alpha_t\\)、15歳以上人口\\(N_t\\)の寄与度であり、就業者数が失業率に対しマイナス要因であること、労働力率と15歳以上人口が失業率に対しプラス要因であることを示しています。 また、右辺第4項は3つの構成項目の交差項であり、誤差として処理します。 固定加重平均の指標 複数の構成項目を、時間で変化しない固定ウェイトで加重平均して作成する指標の寄与度分解です。 公式 \\[ \\begin{aligned} 元の式： &amp;Y_t = w_1 X_{1,t} + w_2 X_{2,t} \\\\ \\\\ 寄与度分解： &amp;\\frac{\\Delta{Y_t}}{Y_{t-1}} = w_1 \\frac{\\Delta{X_{1,t}}}{Y_{t-1}} + w_2 \\frac{\\Delta{X_{2,t}}}{Y_{t-1}} \\end{aligned} \\] ある指標\\(Y_t\\)が、構成項目\\(X_{1,t}\\)、\\(X_{2,t}\\)と、時間によって変化しない固定ウェイト\\(w_1\\)、\\(w_2\\)によって、次の式で作成されているとします。なお、ウェイトの合計は1とします（\\(w_1 + w_2 = 1\\)）。 \\[ Y_t = w_1 X_{1,t} + w_2 X_{2,t} \\] 両辺で階差をとると、 \\[ Y_t - Y_{t-1} = w_1 (X_{1,t} - X_{1,t-1}) + w_2 (X_{2,t} - X_{2,t-1}) \\] となり、これは次の式に書き換えられます。 \\[ \\Delta{Y_t} = w_1 \\Delta{X_{1,t}} + w_2 \\Delta{X_{2,t}} \\] ここで、両辺を\\(Y_{t-1}\\)で割ると、 \\[ \\frac{\\Delta{Y_t}}{Y_{t-1}} = w_1 \\frac{\\Delta{X_{1,t}}}{Y_{t-1}} + w_2 \\frac{\\Delta{X_{2,t}}}{Y_{t-1}} \\] となります。これは、変数\\(Y_t\\)の変化率\\(\\Delta{Y_t}/Y_{t-1}\\)が、\\(X_{1,t}\\)の寄与度\\(w_1\\Delta{X_{1,t}}/Y_{t-1}\\)と、\\(X_{2,t}\\)の寄与度\\(w_2\\Delta{X_{2,t}}/Y_{t-1}\\)に分解できることを示しています。 実例：ラスパイレス指数 消費者物価指数や企業物価指数のように、ウェイトを基準年で固定して計算するラスパイレス方式の指標について、変化率を構成項目の寄与度に分解します。 定義式を、 $$ I_t = $$ とします。\\(I_t\\)は物価総合指数、\\(P_{i,t}\\)と\\(P_{0,t}\\)はそれぞれ比較年と基準年における個別品目\\(i\\)の物価指数（例えば生鮮食品、エネルギー、コア物価指数など）、\\(w_i\\)は個別品目\\(i\\)のウェイトです。 ここで、全ての品目\\(i\\)について基準年の物価指数\\(P_{i,0}\\)が1であるとすると、定義式は、 \\[ wI_t = \\sum_{i=1}{w_iP_{i,t}} \\times 100 \\] と書き換えられます。\\(w\\)は個別品目のウェイトの合計\\(\\sum_{i=1}{w_i}\\)を表します。 固定加重平均の指標の公式を利用すると、 \\[ \\frac{\\Delta{I_t}}{I_{t-1}} = \\sum_{i=1}{\\frac{w_i}{w}\\frac{\\Delta{P_{i,t}}}{I_{t-1}}} \\times 100 \\] と寄与度分解できます。なお、消費者物価指数（2020年基準）の詳細な作成方法はこちらを、企業物価指数（2020年基準）の詳細な作成方法はこちらのリンク先を参照してください。 可変加重平均の指標 複数の構成項目を、時間で変化する可変ウェイトで加重平均して作成する指標の寄与度分解です。 公式 \\[ \\begin{aligned} 元の式： &amp;Y_t = w_{1,t} X_{1,t} + w_{2,t} X_{2,t} \\\\ \\\\ 寄与度分解： &amp;\\frac{\\Delta{Y_t}}{Y_{t-1}} \\approx w_{1,t-1}\\frac{\\Delta{X_{1,t}}}{Y_{t-1}} + w_{2,t-1}\\frac{\\Delta{X_{2,t}}}{Y_{t-1}} + (X_{1,t-1} - X_{2,t-1})\\frac{\\Delta{w_{1,t}}}{Y_{t-1}} \\end{aligned} \\] ある指標\\(Y_t\\)が、構成項目\\(X_{1,t}\\)、\\(X_{2,t}\\)と、時間によって変化する可変ウェイト\\(w_{1,t}\\)、\\(w_{2,t}\\)によって、次の式で作成されているとします。なお、ウェイトの合計は常に1とします（\\(w_{1,t} + w_{2,t} = 1\\)）。 \\[ Y_t = w_{1,t} X_{1,t} + w_{2,t} X_{2,t} \\] \\(X_t = X_{t-1} + \\Delta{X_t}\\)、\\(w_{2,t} = 1 - w_{1,t}\\)、\\(\\Delta{w_{2,t}} = -\\Delta{w_{1,t}}\\)を利用すると、上の式は次のように書き換えることができます。 \\[ \\begin{aligned} Y_{t-1} + \\Delta{Y_t} &amp;= (w_{1,t-1} + \\Delta{w_{1,t}}) (X_{1,t-1} + \\Delta{X_{1,t}}) + (w_{2,t-1} + \\Delta{w_{2,t}}) (X_{2,t-1} + \\Delta{X_{2,t}}) \\\\ \\\\ &amp;= w_{1,t-1}X_{1,t-1} + w_{2,t-1}X_{2,t-1} + w_{1,t-1}\\Delta{X_{1,t}} + w_{2,t-1}\\Delta{X_{2,t}} \\\\ &amp;+ X_{1,t-1}\\Delta{w_{1,t}} + X_{2,t-1}\\Delta{w_{2,t}} + \\Delta{w_{1,t}}\\Delta{X_{1,t}} + \\Delta{w_{2,t}}\\Delta{X_{2,t}} \\\\ \\\\ &amp;= Y_{t-1} + w_{1,t-1}\\Delta{X_{1,t}} + (1-w_{1,t-1})\\Delta{X_{2,t}} + (X_{1,t-1} - X_{2,t-1})\\Delta{w_{1,t}} + \\Delta{w_{1,t}}(\\Delta{X_{1,t}} - \\Delta{X_{2,t}}) \\end{aligned} \\] 両辺を\\(Y_{t-1}\\)で割り、\\(Y_{t-1} = X_{1,t-1} X_{2,t-1}\\)を使って整理すると、 \\[ \\frac{\\Delta{Y_t}}{Y_{t-1}} = w_{1,t-1}\\frac{\\Delta{X_{1,t}}}{Y_{t-1}} + (1-w_{1,t-1})\\frac{\\Delta{X_{2,t}}}{Y_{t-1}} + (X_{1,t-1} - X_{2,t-1})\\frac{\\Delta{w_{1,t}}}{Y_{t-1}} + \\frac{\\Delta{w_{1,t}}(\\Delta{X_{1,t}} - \\Delta{X_{2,t}})}{Y_{t-1}} \\] となります。これは、変数\\(Y_t\\)の変化率\\(\\Delta{Y_t}/Y_{t-1}\\)が、\\(X_{1,t}\\)の寄与度\\(w_1\\Delta{X_{1,t}}/Y_{t-1}\\)と、\\(X_{2,t}\\)の寄与度\\((1-w_{1,t-1})\\Delta{X_{2,t}}/Y_{t-1}\\)、可変ウェイト\\(w_{1,t}\\)の寄与度\\((X_{1,t-1} - X_{2,t-1})/\\Delta{w_{1,t}}{Y_{t-1}}\\)に分解できることを示しています。右辺第1・2項にかかるウェイト\\(w_{t-1}\\)が\\(t\\)時点ではなく\\(t-1\\)時点であることに注意してください。 右辺第4項は\\(X_{1,t}\\)、\\(X_{2,t}\\)、\\(w_{1,t}\\)のどの階差にも帰属させることができない交差項ですが、通常は小さな値になるため、誤差として処理します。右辺第4項を省略すると、次の近似式が成立します。 \\[ \\frac{\\Delta{Y_t}}{Y_{t-1}} \\approx w_{1,t-1}\\frac{\\Delta{X_{1,t}}}{Y_{t-1}} + (1-w_{1,t-1})\\frac{\\Delta{X_{2,t}}}{Y_{t-1}} + (X_{1,t-1} - X_{2,t-1})\\frac{\\Delta{w_{1,t}}}{Y_{t-1}} \\] 実例：現金給与総額 毎月勤労統計の現金給与総額\\(W_t\\)の変化率を、一般労働者の現金給与総額\\(FT_t\\)、パートタイム労働者の現金給与総額\\(PT_t\\)、パートタイム比率\\(p_t\\)、の3つの構成項目の寄与度に分解します。 定義式を、 \\[ W_t = (1-p_t)FT_t + p_tPT_t \\] とすると、可変加重平均の指標の公式より、 \\[ \\frac{\\Delta{W_t}}{W_{t-1}} = (1-p_{t-1})\\frac{\\Delta{FT_{t}}}{W_{t-1}} + p_{t-1}\\frac{\\Delta{PT_{t}}}{W_{t-1}} + (PT_{t-1} - FT_{t-1})\\frac{\\Delta{p_{t}}}{W_{t-1}} + \\frac{\\Delta{p_{t}}(\\Delta{PT_{t}} - \\Delta{FT_{t}})}{W_{t-1}} \\] と寄与度分解できます。右辺第1・2・3項がそれぞれ一般労働者の現金給与総額\\(FT_t\\)、パートタイム労働者の現金給与総額\\(PT_t\\)、パートタイム比率\\(p_t\\)の寄与度です。 右辺第4項は3つの構成項目の交差項であり、誤差として処理します。 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
