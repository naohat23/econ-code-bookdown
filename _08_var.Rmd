# 時系列分析

第8章「時系列分析」では、時系列データを対象とする分析手法について解説します。

時系列分析は、観測値と観測時点がセットになった時系列データについて、観測時点間のデータの関係性（データの並び順や前後関係に関する性質）を明らかにしたり、過去のデータの特徴から将来の値を予測したりすることを目的としています。

なお、この章の説明は、Nielsen（2021）、西山 他（2019）、村尾（2019）、横内・青木（2014）を参考にしています。詳細は各参考文献を参照してください。

## 第8章の準備

### パッケージのインポート {.unnumbered}

```{r}
library(estimatr)
library(forecast)
library(ggfortify)
library(ggplot2)
library(magrittr)
library(tidyverse)
library(tseries)
library(vars)
library(urca)
```

### 外部データセットの取得 {.unnumbered}

この章では、西山 他（2019）の実証例のデータセットを使用します。[西山 他（2019）のサポートウェブサイト](http://www.yuhikaku.co.jp/books/detail/9784641053854)からデータファイルを取得し、各自の実行環境のワーキングディレクトリ直下に`data_nishiyama`フォルダを作成して、その中に格納してください。

## 定常性

時系列分析の目的は時系列データの特徴を明らかにし、将来の予測に役立てることです。そのためには、分析対象である時系列データの特徴が時間を通じて安定している必要があります。そうした時系列データの安定性を担保する前提条件が**定常性（stationarity）**です。

### 定常性の定義 {.unnumbered}

本書では、定常性を次のように「平均$E(Y_t)$、分散$Var(Y_t)$、自己共分散$Cov(Y_t, Y\_{t-j})$が時間$t$を通じて変わらないこと」と定義します。このような性質をもつ時系列データを**定常過程（stationary process）**といいます。

特に、原系列に何ら変換を加えない状態で定常である時系列データを**レベル定常過程**と呼びます。レベル定常過程のデータは、ある水準の周りで安定的に推移し、一定の幅をもって変動します。

$$
E(Y_t) = \mu \lt \infty \\
Var(Y_t) = E[(Y_t - \mu)^2] = \sigma^2 \lt \infty \\
Cov(Y_t, Y_{t-j}) = E[(Y_t - \mu)(Y_{t-j} - \mu)] = \gamma_j, \quad j \gt 0
$$

なお、定常性にはいくつかの定義があり、本書では「弱定常」や「共分散定常」と呼ばれる定常性の定義を採用しています。定常性の詳細については、Nielsen（2021）P.77、西山 他（2019）P.476、村尾（2019）P.36、横内・青木（2014）P.82を参照してください。

### ホワイトノイズ {.unnumbered}

定常な時系列データの中でも、最も簡単で重要な系列が**ホワイトノイズ（white noise）**です。次のように「平均が0、分散がある一定の値、自己共分散が0」が成立する時系列データ$u_t$をホワイトノイズと定義します。ホワイトノイズの厳密な定義については、村尾（2019）P.32を参照してください。

$$
E(u_t) = 0 \\
Var(u_t) = E(u_t u_t) = \sigma^2 \lt \infty \\
Cov(u_t, u_s) = E(u_t, u_s) = 0, \quad t \ne s
$$

例えば、平均が0の正規分布（標準正規分布など）に従う時系列データはホワイトノイズの一種です。ホワイトノイズを生成する場合、`rnorm()`関数などを用いて正規乱数を生成し、ホワイトノイズとして利用することが一般的です。

```{r}
# 標準正規分布に従うサンプル数400の乱数を生成し折れ線グラフをプロット
rnorm(n = 400, mean = 0, sd = 1) %>% 
  plot(type = "l")
```

## 非定常過程

定常性を満たさない時系列データを**非定常過程（non-stationary process）**といいます。

ここでは、定常過程と非定常過程を区別する方法である**単位根**や、代表的な非定常過程の例である**階差定常過程（和分過程）**と**トレンド定常過程**について解説します。

### 単位根 {.unnumbered}

時系列データの安定性を示す定常性とコインの裏表の関係にある概念が、**単位根（unit root）**です。時系列データが定常か非定常かを判断するうえでは、単位根の有無が重要です。

ある時系列データ$Y_t$の当期の値$Y_t$と1期前の値$Y_{t-1}$について、次のような関係を考えます。なお、$u_t$はホワイトノイズです。

$$
Y_t = \rho Y_{t-1} + u_t 
$$

ここで、ある時に変数$u_t$に生じたショックがその後の時系列データ$Y_t$に与える波及効果は、データの時間を通じた影響力を表す係数$\rho$の絶対値$|\rho|$が取りうる範囲によって、次の3つに分類できます。

1.  $|\rho| \lt 1$のケース

    データの時間を通じた影響力が1より小さく、変数$u_t$に生じたショックは時間の経過に伴って減衰します。すなわち、[波及効果は過渡的・一時的であり、このケースは定常過程になります]{.underline}。

2.  $|\rho| = 1$のケース

    データの時間を通じた影響力が1であり、変数$u_t$に生じたショックは時間の経過に伴って減衰することなく一定で永遠に続きます。すなわち、[波及効果は恒久的であり、このケースは非定常過程になります]{.underline}。このように$|\rho| = 1$となる$\rho$を**単位根**といいます。このケースは**ランダムウォーク（random walk）**とも呼ばれます。

3.  $|\rho| \gt 1$のケース

    データの時間を通じた影響力が1より大きく、変数$u_t$に生じたショックは時間の経過に伴って拡大します。すなわち、[波及効果は爆発的（発散）であり、このケースは非定常過程になります]{.underline}。

本書では2のケースと3のケースが成立するとき、すなわち$|\rho| \ge 1$の場合に、当該データが**単位根過程（unit root process）**であると呼びます。単位根過程は非定常過程の十分条件です。また、単位根過程はこの後に示す階差定常過程（和分過程）と同義です。

ケース1〜3のデータ例をRで作成して図示すると、次のようになります。

定常過程のケース1（$Y^1_t = 0.1 \times Y^1_{t-1} + u_t$）では$Y=0$から離れてもすぐに回帰し、$Y=0$の周りで安定的に変動しています。一方、ケース2（$Y^2_t = Y^2_{t-1} + u_t$）ではデータがすぐ$Y=0$に戻らずに一定期間プラスやマイナスの領域で推移し続け、不安定な千鳥足的変動になっています。最後のケース3（$Y^3_t = 1.03 \times Y^3_{t-1} + u_t$）ではデータが$Y=0$に戻ることなく、加速度的に$Y=0$から離れていきます。

```{r}
# 時系列データを格納する要素数100の数値ベクトル
y1 <- vector(mode = "numeric", length = 100)
y2 <- vector(mode = "numeric", length = 100)
y3 <- vector(mode = "numeric", length = 100)

# 要素数100のホワイトノイズ（標準正規乱数）
set.seed(0)
u <- rnorm(n = 100, mean = 0, sd = 1)

# y1〜y3の初期値（t=1）にu[1]を代入しておく
y1[1] <- u[1]
y2[1] <- u[1]
y3[1] <- u[1]

# forループでt=2以降のデータを作成
for (t in 2:100) {
  y1[t] <- 0.1 * y1[t-1] + u[t] # ケース1
  y2[t] <- 1.0 * y2[t-1] + u[t] # ケース2
  y3[t] <- 1.03 * y3[t-1] + u[t] # ケース3
} 

# 折れ線グラフをプロット
tibble::tibble(
  X = 1:100,
  Y1 = y1,
  Y2 = y2,
  Y3 = y3
) %>% 
  tidyr::pivot_longer(cols = -X, values_to = "Y", names_to = "series") %>% 
  ggplot(mapping = aes(x = X, y = Y, color = series)) +
  geom_line()
```

上の例において、$t=25$時点で一時的なショックを発生させると次のような結果になります。ケース1ではショックが次第に消失し$y=0$に回帰します。一方、ケース2ではショックが消失せずに残り、ショック発生後に水準がシフトします。ケース3ではショックが増幅され、ショックがない場合に比べ増加ペースが加速します。

```{r}
# 時系列データを格納する要素数100の数値ベクトル
y1 <- vector(mode = "numeric", length = 100)
y2 <- vector(mode = "numeric", length = 100)
y3 <- vector(mode = "numeric", length = 100)

# 要素数100のホワイトノイズ（標準正規乱数）
set.seed(0)
u <- rnorm(n = 100, mean = 0, sd = 1)

# y1〜y3の初期値（t=1）にu[1]を代入しておく
y1[1] <- u[1]
y2[1] <- u[1]
y3[1] <- u[1]

# forループでt=2以降のデータを作成。t=25にshockが発生
for (t in 2:100) {
  if (t == 25) {shock <- 10} else {shock <- 0}
  y1[t] <- 0.1 * y1[t-1] + u[t] + shock # ケース1
  y2[t] <- 1.0 * y2[t-1] + u[t] + shock # ケース2
  y3[t] <- 1.03 * y3[t-1] + u[t] + shock # ケース3
} 

# 折れ線グラフをプロット
tibble::tibble(
  X = 1:100,
  Y1 = y1,
  Y2 = y2,
  Y3 = y3
) %>% 
  tidyr::pivot_longer(cols = -X, values_to = "Y", names_to = "series") %>% 
  ggplot(mapping = aes(x = X, y = Y, color = series)) +
  geom_line()
```

### 階差定常過程・和分過程 {.unnumbered}

非定常過程のデータは平均や分散といった性質が時間を通じて変化するため、そのままでは定常過程のデータと同様の分析を行うことができません。しかし、一定の処置を行なって定常過程に変換（定常化）すれば、定常過程のデータと同じように扱うことができます。

そうした特定の方法で定常化できる非定常過程の代表例として、原系列の階差を取ることで定常になる**階差定常過程（和分過程）**と、タイムトレンドを除去することで定常になる**トレンド定常過程**があります。

**階差定常過程（difference stationary）**は、原系列の階差を取ることで定常になる非定常過程であり、$|\rho| \ge 1$のケースに該当します。

階差定常過程は「何回階差を取れば定常過程になるか」が重要です。非定常過程$Y_t$が1次の階差変換で定常になるとき、$Y_t$は1次の**和分過程（integrated process）**であるといい、$Y_t \sim I(1)$と表記します。同様に2次の階差変換で定常になる場合は2次の和分過程と呼び、$Y_t \sim I(2)$と表記します。なお、原系列のままで定常なレベル定常過程は$I(0)$過程に分類されます。

経済・金融分野では一般的に多くの時系列データが$I(0)$過程もしくは$I(1)$過程であり、$I(2)$過程はきわめて稀です。したがって実務的には$I(0)$過程と$I(1)$過程を見分けることが重要になります。

### ランダムウォーク {.unnumbered}

階差定常過程の代表例がランダムウォークです。ランダムウォークは$Y_t = c + Y_{t-1} + u_t$の形を取ります。$c$はドリフト項といい、ランダムウォークなどの$I(1)$過程ではタイムトレンドを表現します（村尾（2019）P.66）。

次の例はドリフト項がない最も基本的なランダムウォーク$Y_t = Y_{t-1} + u_t$であり、$y=0$の周りで不安定な千鳥足的変動になっています。

```{r}
# 時系列データを格納する要素数100の数値ベクトル
y <- vector(mode = "numeric", length = 100)

# 要素数100のホワイトノイズ（標準正規乱数）
set.seed(0)
u <- rnorm(n = 100, mean = 0, sd = 1)

# yの初期値（t=1）にu[1]を代入しておく
y[1] <- u[1]

# forループでt=2以降のデータを作成
for (t in 2:100) {y[t] <- y[t-1] + u[t]}

# 折れ線グラフにプロット
plot(y, type = "l")
```

一方、次の例はドリフト項があるランダムウォーク$Y_t = c + Y_{t-1} + u_t$であり、タイムトレンド（ここでは$c=0.2$）の周りで不安定な千鳥足的変動になっています。一見すると次に示すトレンド定常過程と似ていますが、ランダムウォークはタイムトレンドを除去しても非定常過程のままであり、それだけでは定常過程に変換できません。

```{r}
# 時系列データを格納する要素数100の数値ベクトル
y <- vector(mode = "numeric", length = 100)

# 要素数100のホワイトノイズ（標準正規乱数）
set.seed(0)
u <- rnorm(n = 100, mean = 0, sd = 1)

# yの初期値（t=1）にu[1]を代入しておく
y[1] <- u[1]

# forループでt=2以降のデータを作成
for (t in 2:100) {y[t] <- 0.2 + y[t-1] + u[t]}

# 折れ線グラフにプロット
plot(y, type = "l")
```

### トレンド定常過程 {.unnumbered}

**トレンド定常過程（trend stationary）**は、タイムトレンド（時間のみの関数）を取り除くことによって定常になる非定常過程です。タイムトレンドを除去するには、1階の階差変換を行なったり、線形トレンドを当てはめてトレンドラインからの乖離幅を求めたりする方法があります。

トレンド定常過程は、レベル定常過程にタイムトレンド項を加えた構造をしています。平均値が時間を通じて一定でないため定常性の条件を満たしませんが、タイムトレンドの周りで一定の幅をもって安定的に変動するため、定常過程に近い性質があります。

トレンド定常過程のデータを扱う際の注意点は次のとおりです。

-   [単位根の有無だけでは、定常過程とトレンド定常過程を区別できません]{.underline}。定常過程とトレンド定常過程はどちらも単位根をもたず、ともに$I(0)$過程に分類されます。定常過程とトレンド定常過程を見分けるには、データを図示してトレンドの有無をチェックするか、次に示すADF検定フローで「トレンド項がある$I(0)$過程」と判断されることを確認する必要があります。

-   [トレンド定常過程のデータに定常性を前提とする時系列分析手法は使えません]{.underline}。トレンド定常過程は単位根を持たない$I(0)$過程である点が定常過程と共通していますが、平均値が一定でなく定常性の条件を満たしていないため、後述するBox-jenkins法のARMAモデルといった定常性を前提とする時系列分析手法を使うことはできません。階差変換を行うARIMAモデルを用いる必要があります。

-   [1階の階差変換による定常化だけでは、階差定常過程（1次の和分過程）とトレンド定常過程を区別できません]{.underline}。1階の階差変換はタイムトレンドを除去する手段でもあるため、1階の階差変換により階差定常過程（1次の和分過程）とトレンド定常過程をどちらも定常化することができます。一方、それ以外のタイムトレンド除去方法（例えば線形トレンドの当てはめなど）を階差定常過程データに用いても定常化することはできません。$I(1)$過程データと$I(0)$過程データは、後述する共和分の有無の確認などで取り扱いが異なるため、次に示すADF検定フローを通じて区別する必要があります。

最も単純なトレンド定常過程の例が、ホワイトノイズ$u_t$にタイムトレンド項$\delta t$を加えた$Y_t = \delta t + u_t$です。ここでは例として$\delta = 0.1$とします。

```{r}
# 時系列データを格納する要素数100の数値ベクトル
y <- vector(mode = "numeric", length = 100)

# 要素数100のホワイトノイズ（標準正規乱数）
set.seed(0)
u <- rnorm(n = 100, mean = 0, sd = 1)

# forループでタイムトレンド項1〜100のデータを作成
for (t in 1:100) {y[t] <- 0.1 * t + u[t]}

# 折れ線グラフにプロット
plot(y, type = "l")
```

また、$|\rho| \lt 1$のケースのレベル定常過程データにタイムトレンド項を加えた$Y_t = \rho Y_{t-1} + \delta t + u_t \; (|\rho| \lt 1)$もトレンド定常過程になります。ここでは例として$\rho = 0.8$、$\delta = 0.1$とします。

```{r}
# 時系列データを格納する要素数100の数値ベクトル
y <- vector(mode = "numeric", length = 100)

# 要素数100のホワイトノイズ（標準正規乱数）
set.seed(0)
u <- rnorm(n = 100, mean = 0, sd = 1)

# yの初期値（t=1）にu[1]を代入しておく
y[1] <- u[1]

# forループでt=2以降のデータを作成
for (t in 2:100) {y[t] <- 0.8 * y[t-1] + 0.1 * t + u[t]}

# 折れ線グラフにプロット
plot(y, type = "l")
```

上記式にドリフト項$c$を加えた$Y_t = c + \rho Y_{t-1} + \delta t + u_t \; (|\rho| \lt 1)$も、トレンド定常過程になります。ここでは例として$c = 5$とします。トレンド定常過程におけるドリフト項$c$は期初時点の一時的なショックを表現し、ショックの消化に伴いトレンド定常過程全体が$y=0$からレベルシフトします。ショックの消化に要する期間は$Y_{t-1}$の係数$\rho$に、レベルシフトの幅はドリフト項$c$と$Y_{t-1}$の係数$\rho$両方に依存します。

```{r}
# 時系列データを格納する要素数100の数値ベクトル
y <- vector(mode = "numeric", length = 100)

# 要素数100のホワイトノイズ（標準正規乱数）
set.seed(0)
u <- rnorm(n = 100, mean = 0, sd = 1)

# yの初期値（t=1）にu[1]を代入しておく
y[1] <- u[1]

# forループでt=2以降のデータを作成
for (t in 2:100) {y[t] <- 5　+　0.8 * y[t-1] + 0.1 * t + u[t]}

# 折れ線グラフにプロット
plot(y, type = "l")
```

## 単位根検定（ADF検定） {.unnumbered}

分析対象の時系列データに単位根があるかどうかを判断するための方法が**単位根検定（unit root test）**です。単位根検定にはいくつかの種類がありますが、ここでは最も一般的な拡張ディッキー＝フラー検定（Augmented Dickey-Fuller test、ADF検定）を用います。ADF検定の最終的な目的は、単位根の有無をもとに、分析対象である時系列データの和分次数（$I(0)$、$I(1)$、$I(2)$など）を判断することです。

ADF検定では、分析対象の時系列データが定数項やタイムトレンド項をもつ可能性を考慮し、次の3種類のモデルについて「単位根がある（$\rho = 1$）」ことを帰無仮説とする検定を行います。ここで、定数項は$\beta_1$、タイムトレンド項は$\beta_2t$です。また、$\sum^{p-1}_{i=1}{\gamma_iY_{t-i}}$は系列相関の影響を制御（除去）する役割があります。

1.  trendモデル

    trendモデルは定数項$\beta_1$とタイムトレンド項$\beta_2t$の両方を含む回帰モデルです。

$$
\Delta{Y_t} = \beta_1 + \beta_2t + (\rho - 1)Y_{t-1} + \sum^{p-1}_{i=1}{\gamma_iY_{t-i}} + u_t
$$

2.  driftモデル

    driftモデルは定数項$\beta_1$を含む回帰モデルです。

$$
\Delta{Y_t} = \beta_1 + (\rho - 1)Y_{t-1} + \sum^{p-1}_{i=1}{\gamma_iY_{t-i}} + u_t
$$

3.  noneモデル

    noneモデルは定数項もタイムトレンド項も含まない回帰モデルです。

$$
\Delta{Y_t} = (\rho - 1)Y_{t-1} + \sum^{p-1}_{i=1}{\gamma_iY_{t-i}} + u_t
$$

ADF検定の具体的なフローは次のとおりです。

まず、時系列データの原系列に対し、trendモデル、driftモデル、noneモデルの順番で「単位根がある（$\rho = 1$）」ことを帰無仮説とする検定を行います。帰無仮説が棄却できればそこでADF検定が終了し、時系列データが$I(0)$過程であると判断します。

一方、3つのモデル全てにおいて帰無仮説が棄却できなければ、時系列データを階差変換し、再びtrendモデル、driftモデル、noneモデルの順番で検定を繰り返します。1次の階差変換で帰無仮説が棄却できれば時系列データは$I(1)$過程、2次の階差変換で帰無仮説が棄却できれば時系列データは$I(2)$過程と判断します。

なお、上記のフローは単純化したもので、実際には3つのモデルそれぞれについて複数の帰無仮説を検定し、その結果によって手順が変わる枝分かれのフロー構造になっています。ADF検定フローの詳細については、村尾（2019）P.137を参照してください。

### ADF検定フロー自作関数 {.unnumbered}

このように、ADF検定を用いた和分次数の判断はシステマチックに実施することができます。本書では、このADF検定フローを分析対象の時系列データに自動で適用する自作関数`adf_test_flow()`を定義します。

`adf_test_flow()`自作関数の引数は次のとおりです。

-   `y`：ADF検定を適用する時系列データ。数値型ベクトルもしくはts型データを指定します。
-   `selectlags`：ラグ次数の選択基準。`"AIC"`、`"BIC"`、`"Fixed"`の3つのうち1つを指定します（デフォルト値は`"AIC"`）。`"AIC"`もしくは`"BIC"`を指定すると、それぞれ赤池情報量基準（Akaike Information Criterion）とベイズ情報量基準（Bayesian Information Criterion）に基づき最適なラグ次数が自動で選択されます（このとき`lags`引数に指定した値が自動で選択されるラグ次数の上限になります）。`"Fixed"`を指定する場合は、適用するラグ次数を`lags`引数に手動で指定します。
-   `lags`：検定するモデルのラグ次数（正の整数、デフォルト値は1）。`selectlags`引数に`"AIC"`か`"BIC"`を指定した場合は`lags`が自動的に選択されるラグ次数の最大値になります。`selectlags`引数に`"Fixed"`を指定した場合は、`lags`に指定したラグ次数がそのまま使用されます。
-   `sig_level`：有意水準。0.01、0.05、0.1の3つのうち1つを指定します。

これらの引数を設定して`adf_test_flow()`自作関数を実行すると、ADF検定フローの結果を記載したtibble形式のデータフレームが返ります。trendモデル3種類、driftモデル3種類、noneモデル1種類の、合計7種類のADF検定を実施し、各検定の結果に基づく判断経路を「＊」で示します。最終的に`y`に指定した時系列データが$I(0)$過程、$I(1)$過程、$I(2)$過程のどれに従うかが判断できるまで、階差次数を増やして検定を継続します。

```{r}
# 時系列データに対しADF検定フローを自動で適用する関数
adf_test_flow <- function(y, selectlags = "AIC", lags = 1, sig_level = 0.05) {
  
  '
  y：ADF検定を適用する時系列データ（数値型ベクトル、ts型データ）
  selectlags：ラグ次数選択基準（"Fixed", "AIC", "BIC"）。デフォルト値は"AIC"
  lags：検定するモデルのラグ次数（正の整数）。デフォルト値は1
  sig_level：有意水準（0.01, 0.05, 0.1）
  '
  
  # 引数sig_levelをurca::ur_df()関数用に文字列へ変換
    sig_level_str <- str_c(sig_level * 100, "pct")
  
  # 出力用tibble
    res <- tibble::tibble() %>% 
      dplyr::mutate(`フロー` = NA_real_,
                    `階差次数` = NA_real_,
                    `モデル` = NA_character_,
                    `ラグ次数` = NA_real_,
                    `帰無仮説` = NA_character_,
                    `検定統計量` = NA_character_,
                    `棄却点の分布` = NA_character_,
                    `棄却点` = NA_real_,
                    `検定値` = NA_real_,
                    `結果` = NA_character_,
                    `判断` = NA_character_,
                    `判断経路` = NA_character_,
                    )
  
  # フローカウンタ
  i <- 0
  
  # 判断経路記入用ベクトル（最大21要素＝7フロー × 3ループ）
  flow_vec <- rep(FALSE, times = 21)
  flow_vec[1] <- TRUE
  #flow_vec[c(1, 8, 15)] <- TRUE
  
  # 判断経路記入可否フラグ
  flow_cont_flag <- TRUE

  # 階差次数dのループ（0〜2次）
  for (d in 0:2) {
    
    # データの階差変換
    if (d >= 1) {y <- diff(y, lag = 1)}
    
    # 正規分布の棄却点
    norm_cval <- qnorm(p = sig_level, lower.tail = TRUE)
    
    # ADF検定
    urdf_trend <- urca::ur.df(y, type = "trend", lags = lags, selectlags = selectlags)
    urdf_drift <- urca::ur.df(y, type = "drift", lags = lags, selectlags = selectlags)
    urdf_none <- urca::ur.df(y, type = "none", lags = lags, selectlags = selectlags)
    
    # ループ継続判断用ベクトル
    continue_vec <- rep(FALSE, 3)
    
    # ADF検定フロー1 trendモデル H0：ρ - 1 = 0
    i <- i + 1
    res %<>% tibble::add_row(`フロー` = i,
                             `階差次数` = d,
                             `モデル` = "trend 第1検定",
                             `ラグ次数` = nrow(urdf_trend@testreg$coefficients) - 3,
                             `帰無仮説` = "単位根あり",
                             `検定統計量` = "tau3",
                             `棄却点の分布` = "tau3",
                             `棄却点` = urdf_trend@cval["tau3", sig_level_str],
                             `検定値` = urdf_trend@teststat[, "tau3"],
                             `判断経路` = if_else(flow_vec[i], "＊", "")
                             )
    if (urdf_trend@teststat[, "tau3"] < urdf_trend@cval["tau3", sig_level_str]) {
      res$結果[i] <- "帰無仮説を棄却"
      res$判断[i] <- str_c("単位根なし：I(", d, ")過程")
      if (flow_vec[i]) {flow_cont_flag <- FALSE}
    } else {
      res$結果[i] <- "帰無仮説を棄却できない"
      res$判断[i] <- str_c("単位根の判断保留：フロー", 2 + 7 * d, "へ")
      if (flow_cont_flag) {flow_vec[i + 1] <- TRUE}
    }
    
    # ADF検定フロー2 trendモデル H0：β2 = 0 and ρ - 1 = 0
    i <- i + 1
    res %<>% tibble::add_row(`フロー` = i,
                             `階差次数` = d,
                             `モデル` = "trend 複合検定",
                             `ラグ次数` = nrow(urdf_trend@testreg$coefficients) - 3,
                             `帰無仮説` = "トレンド項なし＆単位根あり",
                             `検定統計量` = "phi3",
                             `棄却点の分布` = "phi3",
                             `棄却点` = urdf_trend@cval["phi3", sig_level_str],
                             `検定値` = urdf_trend@teststat[, "phi3"],
                             `判断経路` = if_else(flow_vec[i], "＊", "")
                             )
    if (urdf_trend@teststat[, "phi3"] > urdf_trend@cval["phi3", sig_level_str]) {
      res$結果[i] <- "複合帰無仮説を棄却"
      res$判断[i] <- str_c("トレンド項あり：フロー", 3 + 7 * d, "へ")
      if (flow_cont_flag) {flow_vec[i + 1] <- TRUE}
    } else {
      res$結果[i] <- "複合帰無仮説を棄却できない"
      res$判断[i] <- str_c("トレンド項なし：フロー", 4 + 7 * d, "へ")
      if (flow_cont_flag) {flow_vec[i + 2] <- TRUE}
    }
  
    # ADF検定フロー3 trendモデル H0：ρ - 1 = 0
    i <- i + 1
    res %<>% tibble::add_row(`フロー` = i,
                             `階差次数` = d,
                             `モデル` = "trend 第2検定",
                             `ラグ次数` = nrow(urdf_trend@testreg$coefficients) - 3,
                             `帰無仮説` = "単位根あり",
                             `検定統計量` = "tau3",
                             `棄却点の分布` = "正規分布",
                             `棄却点` = norm_cval,
                             `検定値` = urdf_trend@teststat[, "tau3"],
                             `判断経路` = if_else(flow_vec[i], "＊", "")
                             )
    if (urdf_trend@teststat[, "tau3"] < norm_cval) {
      res$結果[i] <- "帰無仮説を棄却"
      res$判断[i] <- str_c("単位根なし：I(", d, ")過程")
      if (flow_vec[i]) {flow_cont_flag <- FALSE}
    } else {
      res$結果[i] <- "帰無仮説を棄却できない"
      res$判断[i] <- "階差変換し再検定"
      if (flow_cont_flag & (d <= 1)) {flow_vec[(d + 1) * 7 + 1]}
      continue_vec[1] <- TRUE
    }
  
    # ADF検定フロー4 driftモデル H0：ρ - 1 = 0
    i <- i + 1
    res %<>% tibble::add_row(`フロー` = i,
                             `階差次数` = d,
                             `モデル` = "drift 第1検定",
                             `ラグ次数` = nrow(urdf_drift@testreg$coefficients) - 2,
                             `帰無仮説` = "単位根あり",
                             `検定統計量` = "tau2",
                             `棄却点の分布` = "tau2",
                             `棄却点` = urdf_drift@cval["tau2", sig_level_str],
                             `検定値` = urdf_drift@teststat[, "tau2"],
                             `判断経路` = if_else(flow_vec[i], "＊", ""),
                             )
    if (urdf_drift@teststat[, "tau2"] < urdf_drift@cval["tau2", sig_level_str]) {
      res$結果[i] <- "帰無仮説を棄却"
      res$判断[i] <- str_c("単位根なし：I(", d, ")過程")
      if (flow_vec[i]) {flow_cont_flag <- FALSE}
    } else {
      res$結果[i] <- "帰無仮説を棄却できない"
      res$判断[i] <- str_c("単位根の判断保留：フロー", 5 + 7 * d, "へ")
      if (flow_cont_flag) {flow_vec[i + 1] <-TRUE}
    }
  
    # ADF検定フロー5 driftモデル H0：β1 = 0 and ρ - 1 = 0
    i <- i + 1
    res %<>% tibble::add_row(`フロー` = i,
                             `階差次数` = d,
                             `モデル` = "drift 複合検定",
                             `ラグ次数` = nrow(urdf_drift@testreg$coefficients) - 2,
                             `帰無仮説` = "定数項なし＆単位根あり",
                             `検定統計量` = "phi1",
                             `棄却点の分布` = "phi1",
                             `棄却点` = urdf_drift@cval["phi1", sig_level_str],
                             `検定値` = urdf_drift@teststat[, "phi1"],
                             `判断経路` = if_else(flow_vec[i], "＊", "")
                             )
    if (urdf_drift@teststat[, "phi1"] > urdf_drift@cval["phi1", sig_level_str]) {
      res$結果[i] <- "複合帰無仮説を棄却"
      res$判断[i] <- str_c("定数項あり：フロー", 6 + 7 * d, "へ")
      if (flow_cont_flag) {flow_vec[i + 1] <- TRUE}
    } else {
      res$結果[i] <- "複合帰無仮説を棄却できない"
      res$判断[i] <- str_c("定数項なし：フロー", 7 + 7 * d, "へ")
      if (flow_cont_flag) {flow_vec[i + 2] <- TRUE}
    }
  
    # ADF検定フロー6 driftモデル H0：ρ - 1 = 0
    i <- i + 1
    res %<>% tibble::add_row(`フロー` = i,
                             `階差次数` = d,
                             `モデル` = "drift 第2検定",
                             `ラグ次数` = nrow(urdf_drift@testreg$coefficients) - 2,
                             `帰無仮説` = "単位根あり",
                             `検定統計量` = "tau2",
                             `棄却点の分布` = "正規分布",
                             `棄却点` = norm_cval,
                             `検定値` = urdf_drift@teststat[, "tau2"],
                             `判断経路` = if_else(flow_vec[i], "＊", "")
                             )
    if (urdf_drift@teststat[, "tau2"] < norm_cval) {
      res$結果[i] <- "帰無仮説を棄却"
      res$判断[i] <- str_c("単位根なし：I(", d, ")過程")
      if (flow_vec[i]) {flow_cont_flag <- FALSE}
    } else {
      res$結果[i] <- "帰無仮説を棄却できない"
      res$判断[i] <- "階差変換し再検定"
      if (flow_cont_flag & (d <= 1)) {flow_vec[(d + 1) * 7 + 1]}
      continue_vec[2] <- TRUE
    }
  
    # ADF検定フロー7 noneモデル H0：ρ - 1 = 0
    i <- i + 1
    res %<>% tibble::add_row(`フロー` = i,
                             `階差次数` = d,
                             `モデル` = "none",
                             `ラグ次数` = nrow(urdf_none@testreg$coefficients) - 1,
                             `帰無仮説` = "単位根あり",
                             `検定統計量` = "tau1",
                             `棄却点の分布` = "tau1",
                             `棄却点` = urdf_none@cval["tau1", sig_level_str],
                             `検定値` = urdf_none@teststat[, "tau1"],
                             `判断経路` = if_else(flow_vec[i], "＊", "")
                             )
    if (urdf_none@teststat[, "tau1"] < urdf_none@cval["tau1", sig_level_str]) {
      res$結果[i] <- "帰無仮説を棄却"
      res$判断[i] <- str_c("単位根なし：I(", d, ")過程")
      if (flow_vec[i]) {flow_cont_flag <- FALSE}
    } else {
      res$結果[i] <- "帰無仮説を棄却できない"
      res$判断[i] <- "階差変換し再検定"
      if (flow_cont_flag & (d <= 1)) {flow_vec[(d + 1) * 7 + 1]}
      continue_vec[3] <- TRUE
    }
    
    # ループ継続を判断（continue_vecの中に一つでもTRUEがあればbreakしない）
    if (!any(continue_vec)) {break}
  }

  # 結果
  return(res %>% 
           dplyr::select(-`検定統計量`, -`棄却点の分布`)
         )
}
```

### 実証例 村尾 6.10 ADF検定 {.unnumbered}

定義した自作関数`adf_test_flow()`を用い、`vars`パッケージの`Canada`データセットに含まれるカナダの実質労働生産性（`prod`）データに対して実際にADF検定フローを適用します（村尾（2019）P.141）。ラグ次数選択基準の`selectlags`には赤池情報量基準を意味する`"AIC"`を指定しておきます。

`adf_test_flow()`自作関数を適用した結果、実質労働生産性の原系列（階差次数＝0）では単位根の存在が示唆され、1次の階差系列では単位根がないことが確認できました。したがって実質労働生産性は$I(1)$過程であると判断できます。

```{r, paged.print=TRUE}
# Canadaデータセットを呼び出し
data(Canada)

# 実質労働生産性データを変数yに格納しプロット
y <- Canada[, "prod"]
plot(y)

# yにadf_test_flow()関数を適用
adf_prod <- adf_test_flow(y = y, # 検定対象の時系列データ
                          selectlags = "AIC", # ラグ次数選択基準
                          lags = 10, # AICで自動選択する最大ラグ次数
                          sig_level = 0.05 # 有意水準（％）
                          )

# 結果をコンソールに出力
# 実際に使用する際は、View(adf_prod) で別ウィンドウに結果を表示する方が結果が確認しやすい
adf_prod
```

## 系列相関

時系列データ$Y$について、現在の値$Y_t$と過去の値$Y_{t-1}$の間の相関を**系列相関（serial correlation）**もしくは**自己相関（autocorrelation）**といいます。

多くのマクロ経済データでは、正の系列相関（前期が正であれば今期も正である可能性が高い）が観察されます。系列相関がないデータは何らかのショックで平均値から乖離してもすぐにまた平均値へ戻りますが、正の系列相関があるデータは一度平均値を離れると戻るまでに一定期間を要する傾向があります。マクロ経済の活動が活発になる期間と停滞する期間が交互に繰り返される「景気循環」の現象は、マクロ時系列データ上では正の系列相関として表現されます（西村 他（2019）P.471）。

系列相関の有無やその構造を確認することは、分析対象である時系列データのモデリングやラグ次数の選択を行う上で重要です。また、最小2乗法（OLS）では誤差項に系列相関があると最小2条推定量の望ましい性質が得られないため、誤差項に対し系列相関の検定が行われます。これは後述するベクトル自己回帰（VAR）モデルでも同様です。

### 自己相関（ACF） {.unnumbered}

ラグ$j$の（標本）**自己相関係数**$\hat{\rho}_j$は、標本平均$\bar{Y} = (1/T)\Sigma^{T}_{t=1}{Y_t}$として、次のように計算できます。

$$
\hat{\rho}_{j} = \frac{\hat{\gamma}_{j}}{\hat{\gamma}_{0}} = \frac{\Sigma^{T}_{t=j+1}{(Y_t-\bar{Y})(Y_{t-j}-\bar{Y})}}{\Sigma^{T}_{t=1}{(Y_t-\bar{Y})^2}}
$$

**コレログラム（自己相関プロット）**は、横軸にラグ次数$j$、縦軸に自己相関係数の値をプロットした図で、`stats::acf()`関数で作成します。ACFはAuto Correlation Function（自己相関関数）を意味します。

コレログラム内に表示されている波線は「自己相関係数が0である」との帰無仮説をラグ次数$j = 1$から順番に逐次検定するための95％信頼区間を示しています。いずれかのラグ次数$j$において自己相関係数の値が信頼区間の外側にあれば、系列相関があると判断できます。

ここでは、西山 他（2019）P.478の図10-14で使用されている日本のGDPギャップ（内閣府、1980〜2016年）のデータでコレログラムを作成します。

```{r}
# XLSXデータを読み込み
data <- readxl::read_excel(path = "data_nishiyama/ch10/Fig_12_GDPgap_quarterly.xlsx", # ファイルパス（拡張子が必要、URLは不可）
                           sheet = NULL, # シートインデックス／シート名
                           col_names = c("year", "quarter", "date", "cao", "boj"), # ヘッダー（列名データ）の有無／列名指定
                           col_types = NULL, # 各列の型の指定（c：文字列型、d：数値型、D：日付型、l：論理値型）
                           skip = 1 # 読み込み時に上からスキップする行数
                           )

data %<>% 
  dplyr::select(date, cao, boj) %>% 
  dplyr::mutate(date = lubridate::date(zoo::as.yearqtr(date, format = "%YQ%q"))) %>% 
  dplyr::filter(date <= "2016-12-31")
```

`stats::acf()`関数でコレログラムを作成すると、ラグ次数が5次以下の低次の自己相関係数が有意であることが確認できます。このように、ラグ次数が低次の自己相関係数が正であり、ラグ次数1次をピークに単調に減少するのは、正の系列相関がある典型的なマクロ経済変数のコレログラムの例です。

しかし、これをもって、5四半期前までのGDPギャップが全て当該四半期のGDPギャップに関係しているとは判断できません。自己相関係数の計算では、ラグの積み重ねによる間接的な関係が含まれているためです。そうした間接的な影響を排除して、過去のデータと当月のデータの直接的な関係を調べる方法が、次の偏自己相関です。

```{r}
stats::acf(data$cao, plot = TRUE)
```

### 偏自己相関（PACF） {.unnumbered}

偏自己相関プロットを作成するには、`stats::pacf()`関数を使用します。PACFはPartial Auto Correlation Function（偏自己相関関数）を意味します。

自己相関プロットと同様に、日本のGDPギャップ（内閣府、1980〜2016年）のデータで偏自己相関プロットを作成すると、ラグ次数1次の偏自己相関係数が有意である一方、2〜5次の偏自己相関係数は有意ではなくなりました。この結果は、当該四半期のGDPギャップと関係しているのは1四半期前のGDPギャップのみであることを示しています。

```{r}
stats::pacf(data$cao, plot = TRUE)
```

### Ljung-Box検定 {.unnumbered}

自己相関プロットや偏自己相関プロットは系列相関の有無を視覚的に判断できる便利な方法ですが、プロット上に表示される95％信頼区間は逐次検定用であり、自己相関の数が大きくなると多重検定の問題が生じます（t検定を複数回行うと設定された有意水準5％よりも高い頻度で第1種の過誤が発生する）。つまり、実際には系列相関がないにも関わらず、どこかのラグ次数で帰無仮説（自己相関係数が0）を棄却してしまう確率が増加します（西村 他（2019））。

この問題を避けるには「複数の自己相関係数が0である」という結合帰無仮説

$$
H_0 : \rho_1 = \rho_2 = \cdots = \rho_m = 0
$$

を一度に検定する必要があります。対立仮説は「少なくとも一つの自己相関係数が0ではない」です。この結合帰無仮説の検定が、**リュン＝ボックス検定（Ljung-Box test）**です。なお、過去には「ダービン・ワトソン比」で系列相関の有無を判断する方法が用いられていましたが、現在ではLjung-Box検定を用いるのが一般的です（西村 他（2019）P.484）。

Ljung-Box検定を行うには`stats::Box.test()`関数を使用し、`type`引数に`"Ljung-Box"`を、`lag`引数にラグ次数を指定します。ここでは、偏自己相関プロット（PACF）の結果に基づきラグ次数に1を指定します。

実行すると、`X-squared`に修正Q統計量、`p-value`にp値が出力されます。この例ではp値が5％を下回り、ラグ次数が1のとき「系列相関がない」との帰無仮説が5％の有意水準で棄却されます。すなわち、日本のGDPギャップはラグ次数が1次のとき系列相関を持つと判断できます。

```{r}
stats::Box.test(data$cao, # 帰無仮説「自己相関なし」を検定する系列
                lag = 1, # ラグ次数
                type = "Ljung-Box"
                )
```

一方、`rnorm()`関数で標準正規分布に従うホワイトノイズを生成してLjung-Box検定を実行すると、p値が5％を上回り「系列相関がない」との帰無仮説が5％の有意水準で棄却されません。この結果は、ホワイトノイズには有意な系列相関が確認できないことを意味しています（なお、これは帰無仮説の「系列相関がない」を採択しているわけではありません）。

```{r}
stats::Box.test(rnorm(n = 400, mean = 0, sd = 1), # 帰無仮説「自己相関なし」を検定する系列
                lag = 1, # ラグ次数
                type = "Ljung-Box"
                )
```

## Box-Jenkins法

**Box-Jenkins法**はARMAモデルやARIMAモデルといった最も基本的な時系列分析の手法です。

Box-Jenkins法には、原系列のままで定常な「レベル定常データ」に適用できる**ARMAモデル**と、階差変換によって定常になる「トレンド定常過程」や「階差定常過程」（単位根過程、和分過程）に適用する**ARIMAモデル**があります。

### ARMAモデル {.unnumbered}

ARMAモデルには、**ARモデル**、**MAモデル**、ARモデルとMAモデルを組み合わせた**ARMAモデル**があります。ARモデルとMAモデルはARMAモデルの一部です。ARMAモデルを適用可能な時系列データは、原系列のままで定常な「レベル定常過程」データです。

#### ARモデル {.unnumbered}

**自己回帰（Autoregressive、AR）モデル**は、過去の自分のデータを説明変数とする回帰モデルです。$p$時点前までのデータを使う自己回帰モデルを$AR(p)$と表記します。

$$
Y_t = c + \phi_1Y_{t-1} + \phi_2Y_{t-2} + \phi_3Y_{t-3} + \dots + \phi_pY_{t-p} + u_t
$$

なお、$u_t$は平均が0、分散が$\sigma^2$の正規分布$N(0,\sigma^2)$に従うホワイトノイズ（平均が0、分散がある一定の値、自己共分散が0）です。

#### MAモデル {.unnumbered}

**移動平均（Moving Average、MA）モデル**は、過去の$q$時点前までの誤差項$u_t$を説明変数とする回帰モデルで、$MA(q)$と表記します。

$$
Y_t = c + \theta_1u_{t-1} + \theta_2u_{t-2} + \theta_3u_{t-3} + \dots + \theta_qu_{t-q} + u_t
$$

#### ARMAモデル {.unnumbered}

**自己回帰移動平均（Autoregressive Moving Average、ARMA）モデル**は、ARモデルとMAモデルを組み合わせたモデルです。$p$次のARモデルと$q$次のMAモデルを組み合わせたARMAモデルを$ARMA(p,q)$と表記します。

\$\$ Y_t = c + \phi\*1Y*{t-1} +* \phi\*2Y{t-2} + \phi\*3Y*{t-3} +* \dots + \phi\*pY{t-p} + \theta\*1u*{t-1} +* \theta\*2u{t-2} + \theta\*3u*{t-3} +* \dots + \theta\*qu{t-q} + u_t \\

Y_t= c + \Sigma\^p\_{i=1}(\phi\*iY*{t-i}) +* \Sigma\^p\_{j=1}(\theta\*ju{t-j}) + u_t \$\$

#### AR・MAモデルの比較 {.unnumbered}

時系列データの特徴を観察することで、ARモデルとMAモデルのどちらを適用すべきか判断することができます。ここではサンプルとして、$AR(1)$過程に従うデータ$Y^{AR}_t = 0.7Y^{AR}_{t-1} + u_t$、$MA(1)$過程に従うデータ$Y^{MA}_t = 0.7u_{t-1} + u_t$、$ARMA(1,1)$過程に従うデータ$Y^{ARMA}_t=0.7Y^{ARMA}_{t-1} + 0.7u_{t-1} + u_t$を生成し、折れ線グラフと自己相関・偏自己相関プロットを作成します。

プロットした結果を見ると、次のような特徴があることが確認できます（馬場（2018）P.46、村尾（2019）P.63、Nielsen（2021）P.180）。

-   AR過程（ラグ次数$p$）：自己相関（ACF）が緩やかに減衰　＆　偏自己相関（PACF）がラグ$p+1$以降はゼロ
-   MA過程（ラグ次数$q$）：自己相関（ACF）がラグ$q+1$以降はゼロ　＆　偏自己相関（PACF）が緩やかに減衰
-   ARMA過程（ラグ次数$p,q$）：自己相関（ACF）、偏自己相関（PACF）どちらも緩やかに減衰（プロットからはラグ次数が決定できない）

```{r}
#stats::arima.sim()関数でts形式のサンプルデータを生成
y_ar <- stats::arima.sim(model = list(ar = c(0.7)), n = 100)
y_ma <- stats::arima.sim(model = list(ma = c(0.7)), n = 100)
y_arma <- stats::arima.sim(model = list(ar = c(0.7), ma = c(0.7)), n = 100)

forecast::ggtsdisplay(y_ar, main = "AR(1)過程")
forecast::ggtsdisplay(y_ma, main = "MA(1)過程")
forecast::ggtsdisplay(y_arma, main = "ARMA(1)過程")
```

### ARIMAモデル {.unnumbered}

ARIMAモデルは、データの階差をとってARMAモデルを適用する手法です。基本になるARIMAモデルに加え、季節変動があるデータに用いるSARIMAモデルや、外生変数を組み込むARIMAXモデルなどがあります。

ARIMAモデルを適用する時系列データは、階差変換によって定常化できる「トレンド定常過程」や「階差定常過程（単位根過程、和分過程）」です。

#### ARIMAモデル

**自己回帰和分移動平均（Autoregressive Integrated Moving Average、ARIMA）モデル**は、時系列データを階差変換してARMAモデルを適用します。$d$階の階差をとって$ARMA(p,q)$を適用するARIMAモデルを、$ARIMA(p,d,q)$と表記します。

ここで、ラグ演算子（lag operator）$L$を$LY_t = Y_{t-1}$、階差演算子（difference operator）$\Delta$を$\Delta Y_t = Y_t - Y_{t-1}$と定義します。例えば2次のラグは$L(LY_t) = L^2Y_t = Y_{t-2}$、2階の階差は$\Delta(\Delta Y_t) = \Delta^2 Y_t = \Delta(Y_t - Y_{t-1}) = (Y_t - Y_{t-1}) - (Y_{t-1} - Y_{t-2}) = Y_t - 2Y_{t-1} + Y_{t-2}$になります。

この表記法を用いると、$ARIMA(p,d,q)$は、

$$
\Delta^d Y_t = \Sigma^p_{i=1}(\phi_iL^i\Delta^d Y_t) + \Sigma^q_{j=1}(\theta_jL^ju_t) + u_t
$$

と表記でき、これを変形すると次のように定式化できます。

$$
\bigl( 1-\Sigma^p_{i=1}(\phi_iL^i) \bigr) \Delta^d Y_t  = \bigl( 1+\Sigma^q_{j=1}(\theta_jL^j) \bigr) u_t
$$

#### SARIMAモデル

**季節性ARIMA（Seasonal ARIMA、SARIMA）モデル**は、ARIMAモデルに季節成分を入れたモデルです。

例えば月次の時系列データの場合、通常のARIMAでは当月と1カ月前、2カ月前、3カ月前・・・の関係を$ARIMA(p,d,q)$でモデル化しますが、SARIMAモデルではそれに加えて、当月と1年前の同月、2年前の同月、3年前の同月・・・の関係を季節性の次数$(P,D,Q)$でモデル化します。

1周期（年間）が$s$のデータ（月次データであれば$s=12$）について、ARIMAの次数$(p,d,q)$、季節性の次数$(P,D,Q)$をモデル化したSARIMAモデルを、$SARIMA(p,d,q)(P,D,Q)[s]$と表記します。

ここで、1周期が$s$である時系列データについて季節階差演算子$\Delta_s$を$\Delta_s Y_t = Y_t - Y_{t-s}$と定義します。例えば2階の季節階差は$\Delta_s(\Delta_s Y_t) = \Delta_s^2 Y_t = \Delta_s(Y_t - Y_{t-s}) = (Y_t - Y_{t-s}) - (Y_{t-s} - Y_{t-2s}) = Y_t - 2Y_{t-s} + Y_{t-2s}$になります。

この表記法を用いると、$SARIMA(p,d,q)(P,D,Q)[s]$は次のように定式化できます。

$$
\bigl( 1-\Sigma^p_{i=1}(\phi_iL^i) \bigr) \bigl( 1-\Sigma^P_{I=1}(\Phi_IL^{sI}) \bigr) \Delta^d \Delta^D_s Y_t  = \bigl( 1+\Sigma^q_{j=1}(\theta_jL^j) \bigr) \bigl( 1+\Sigma^Q_{J=1}(\Theta_JL^{sJ}) \bigr) u_t
$$

#### ARIMAXモデル

**外生変数付きARIMA（ARIMA with eXogenous variables、ARIMAX）モデル**は、ARIMAに外生変数を加えたモデルです。

単変量時系列モデルであるARIMAモデルに回帰の要素を導入したものであり、主にイベント効果、曜日・祝日効果、異常値の補正などを考慮するために用いられます。

ここで、分析対象の非説明変数（目的変数、応答変数ともいう）を$Y_t$、外性変数として導入する説明変数を$X_t$とします。$r$個の説明変数があり、時点$t$における$k$番目の説明変数を$X_{k,t}$と表記すると、$Y_t$を階差変換しない$ARIMAX(p,0,q)$モデルは次のように定式化できます。

$$
Y_t = c + \Sigma^p_{i=1}(\phi_iY_{t-i}) + \Sigma^q_{j=1}(\theta_iu_{t-j}) + \Sigma^r_{k=1}(\beta_kX_{k,t}) + u_t
$$

### Box-Jenkins法の分析フロー {.unnumbered}

Box-Jenkins法は、次のような一連のフローに基づいて分析を実施します。

1.  探索的データ分析

    データの折れ線グラフや自己相関・偏自己相関プロットを作成し、季節性やトレンドといったデータの特徴を把握します。

2.  データ変換

    データの特徴に応じてデータの変換を行います。特に、変動幅が徐々に拡大していくデータでは対数変換を、増加トレンドと変動幅の拡大が同時に見られるデータでは対数階差（変化率）変換などを行います。

3.  単位根検定

    ADF検定を行い、データの定常性を確認します。原系列のままで定常なレベル定常過程か、単位根はないがトレンドをもつトレンド定常過程か、単位根をもつ階差定常過程（和分過程）か判断します。階差定常過程（和分過程）の場合は、何階の階差変換で定常になるか（和分次数）を確認します。

4.  データ分割

    モデルの予測精度を評価するため、データを訓練データとテストデータに分割します。

    一般的に、計量経済学の回帰分析では全てのデータを対象にモデルの推定が行われます。分析の目的がモデルの推定そのもの、すなわちデータの背景にあるメカニズムの把握にあればそれで構いませんが、分析の目的が将来予測の場合は、全てのデータを対象にモデルを推定すると未知の値に対する予測精度を評価することができません。

    Box-Jenkins法の目的はほとんどの場合将来予測であることから、分割した訓練データでモデルを推定し、残しておいた未知のテストデータ（）で予測精度を評価します。これは特に教師あり機械学習において一般的な分析手順であり、ホールドアウト法とも呼ばれます。

5.  モデル同定・推定

    ARIMAモデルの同定と推定を行います。同定とは$ARIMA(p,d,q)$のラグ・階差次数$p,d,q$といったモデルの構造を決めることを言います（機械学習でいうパイパーパラメータの決定）。推定とは、モデルの構造を決めた後に係数を求めることを指します。RではARIMAモデルの同定・推定を自動で実施する関数を用います。

    なお、モデルを同定する際の指標の一つに**尤度（likelihood）**があります。尤度とは「パラメータが与えられた時に、手持ちのデータが得られる確率」であり、手持ちのデータに対するモデルの当てはまりの良さを定量化した指標です。通常、尤度は小さな値になるため、対数変換した**対数尤度（log likelihood）**が用いられます。なお、この尤度（対数尤度）を最大化するようにパラメータを決める方法を**最尤推定法**といいます。

    パラメータの数（ARIMAモデルの場合はラグ次数の$p,q$）を増やしてモデルを複雑化するほど尤度が大きくなるため、モデルを同定する際にはパラメータの数に対し一定の制約が必要です。その制約が**赤池情報量基準（AIC）**です。AICは「パラメータを増加させた以上に尤度が改善しているか」を示す指標であり、小さいほどモデルとして優れていることを意味します。

6.  残差チェック

    モデルを推定した結果として得られる残差の自己相関検定（Ljung-Box検定）と正規性検定（Jarque-Bera検定）を行います。ARIMAモデルを正しく推定した場合は残差がホワイトノイズになるため、自己相関検定では残差系列に自己相関が見られないこと、異常値がないことを確認します。また、時系列モデルの残差項として正規分布に従うホワイトノイズを仮定しているため、正規性検定では残差系列が正規分布と異なっていないことを確認します。

7.  予測精度評価

    分割しておいたテストデータで予測値を作成し、実績と比較して予測精度を評価します。評価指標にはRMSE（Root Mean Square Error）などが用いられます。加えて、過去の平均値や前期の値を予測値として用いるナイーブ予測（ARIMAなどの複雑なモデルを使わない予測）の精度を上回ることを確認します。

### 実証例 馬場 第2部第7章 {.unnumbered}

ここではBox-Jenkins法を実際のデータに適用した例として、馬場（2018）第2部第7章（P.95〜116）の実証例を再現します。

#### 1. 探索的データ分析

Rの`Seatbelts`データセットに含まれる英国の交通事故死傷者数（前席）の月次時系列データ`front`を読み込み、`forecast`パッケージの`ggtsdisplay()`関数で折れ線グラフと自己相関・偏自己相関プロットを作成します。

折れ線グラフに明らかな周期性が見られます。自己相関プロット（ACF）で12・24・36カ月ラグに大きな自己相関があること、偏自己相関プロット（PACF）で12カ月ラグの前後に大きな自己相関があることから、1年周期の変動があると判断できます。

また、時間を通じて値が取る範囲が変化していることから、単位根をもつことが示唆されます。

```{r}
front <- Seatbelts[, "front"]

forecast::ggtsdisplay(front, main = "原系列")
```

#### 2. データ変換 {.unnumbered}

個数や人数といったデータは対数変換するとうまくモデル化できる傾向があることから、`front`を`log()`関数で対数変換したデータについても同様に`ggtsdisplay()`関数でプロットします。

```{r}
front_log <- log(front)
forecast::ggtsdisplay(front_log, main = "対数系列")
```

原系列（対数変換した系列も含む）は単位根を持っている可能性があるため、対数系列を`diff()`関数で差分をとって対数差分系列に変換し、定常化できるか確認します。

対数差分系列は長期にわたって平均値が変化せず、単位根が無くなっている可能性が示唆されます。短期の自己相関（ACF）は減りましたが、12・24・36カ月ラグではプラスの、6・18・30カ月ラグではマイナスの自己相関が残っており、対数差分系列にも周期性があることが確認できます。

```{r}
front_log_diff <- diff(front_log)
forecast::ggtsdisplay(front_log_diff, main = "対数差分系列")
```

12カ月単位の自己相関があるため、季節成分をもっていると判断できます。そこで、`forecast`パッケージの`ggsubseriesplot()`関数を用いて、原系列を月ごとに分けたグラフを作成します。

12月が最も交通事故死亡者数が多く、2月が最も少ない傾向があることが確認できます。

```{r}
ggsubseriesplot(front)
```

季節成分の影響を除去するため、先ほど作成した対数差分系列にさらに季節差分（ここでは12カ月前差）をとり、プロットします。季節階差は`diff()`関数の`lag`引数に周期（ここでは12）を指定して計算します。

自己相関プロット（ACF）、偏自己相関プロット（PACF）ともに12カ月単位の自己相関が残っており、季節階差をとっても季節成分の影響を全て除去することはできませんでした。

```{r}
front_log_diff_seasdiff <- diff(front_log_diff, lag = 12)

forecast::ggtsdisplay(front_log_diff_seasdiff, main = "対数差分系列の季節階差系列")
```

#### 3. 単位根検定 {.unnumbered}

折れ線グラフのプロットから原系列が単位根を持っていることはほぼ明らかですが、念のため`adf_test_flow()`自作関数で`front`単位根検定と和分次数の確認を行います。

原系列（階差次数0）に対しADF検定を適用したフロー1〜7では「単位根あり」の帰無仮説を棄却できず、最終的に1階の階差系列に対しtrend第1検定を行ったフロー8で$I(1)$過程であると判断されました。これは折れ線グラフのプロットと整合的な結果です。

```{r}
adf_test_flow(y = front, # 検定対象の時系列データ
                          selectlags = "AIC", # ラグ次数選択基準
                          lags = 12, # AICで自動選択する最大ラグ次数
                          sig_level = 0.05 # 有意水準（％）
                          )
```

#### 4. データ分割 {.unnumbered}

モデルの予測精度を評価するため、データを訓練データとテストデータに分割します。

まず、Rの`Seatbelts`データセットから、前席における死傷者数（`front`）、ガソリン価格（`PetrolPrice`）、前席のシートベルト着用を義務付ける法律の施行有無を表すフラグ（`law`）を抽出し、対数変換したデータを作成します。

```{r}
Seatbelts_log <- Seatbelts[, c("front", "PetrolPrice", "law")]
Seatbelts_log[, "front"] <- log(Seatbelts_log[, "front"])
Seatbelts_log[, "PetrolPrice"] <- log(Seatbelts_log[, "PetrolPrice"])
```

データ期間（1969年1月〜1984年12月）のうち最後の1年（1984年）をテストデータ、それ以前を訓練データとして分割します。ts形式のデータを分割するには`stats::window()`関数を使用して`start`引数と`end`引数に時点を指定します。

```{r}
# 訓練データ（1983年12月以前）
data_train <- stats::window(Seatbelts_log, end = c(1983, 12))

# テストデータ（1984年1月以降）
data_test <- stats::window(Seatbelts_log, start = c(1984, 1))
```

#### 5. モデル同定・推定 {.unnumbered}

分割した訓練データを用い、モデルの同定・推定を行います。

まず、`forecast`パッケージの`Arima()`関数を用いて手動でモデルを同定します。ここでは暫定的に、モデルを$SARIMA(1,1,1)(1,0,0)$と同定してパラメータを推定します。

モデルの推定結果の係数を見ると、外生変数の`PetrolPrice`と`law`の係数がともにマイナスになっています。これは、ガソリン価格が上がるか、シートベルト着用義務化法が施行されると交通事故死傷者数が減少することを意味しています。

```{r}
library(forecast)

model_sarimax <- Arima(y = data_train[, "front"], # 分析対象のデータ
                       order = c(1, 1, 1), # ラグ・階差次数(p,d,q)
                       seasonal = list(order = c(1, 0, 0)), # 季節成分のラグ・階差次数(P,D,Q)
                       xreg = data_train[, c("PetrolPrice", "law")] # 外生変数
                       )

model_sarimax
```

次に、`forecast`パッケージの`auto.arima()`関数を用いて自動でモデルを同定します。実行すると数十秒計算が行われ（PCの性能により変わります）、その後結果が出力されます。

`auto.arima()`関数では$SARIMA(p,d,q)(P,D,Q)$の6つのラグ・階差次数を全て自動で選択することができますが、階差次数（和分次数）はADF検定で事前に確認できるため、引数`d`に直接指定します。ここでは、先に実施したADF検定の結果に基づき`d = 1`を指定します。また、1階の階差変換で単位根がなくなり定常化できることから、季節成分の階差次数`D`は`D = 0`を指定します。

`auto.arima()`関数を実行した結果、$SARIMA(2,1,4)(2,0,0)[12]$が選択されました。パラメータの推定結果を見ると、外生変数の`PetrolPrice`と`law`の係数はやはり両方マイナスになっています。また、モデル選択基準であるAICは-326.9と先ほど手動で同定・推定した$SARIMA(1,1,1)(1,0,0)$の-318.7より小さく、自動選択のモデルが優れていることを意味しています。

```{r}
library(forecast)

model_sarimax_auto <- auto.arima(y = data_train[, "front"], # 分析対象のデータ
                                 xreg = data_train[, c("PetrolPrice", "law")], # 外生変数
                                 ic = "aic", # モデル選択に使用する情報量基準
                                 d = 1, # 階差次数dを指定
                                 D = 0, # 季節成分の階差次数Dを指定
                                 max.order = 10, # SARIMA(p,d,q)(P,D,Q)におけるラグ次数の合計p+q+P+Qの最大値
                                 stepwise = FALSE, # TRUEにするとステップワイズ法を使用して計算を省略
                                 approximation = FALSE, # 
                                 parallel = TRUE, # TRUEにすると並列化演算で高速化
                                 num.cores = 4 # 並列化演算に使用するCPUのコア数（PCの環境に合わせて指定）
                                 )

model_sarimax_auto
```

#### 6. 残差チェック {.unnumbered}

モデルの同定・推定ができたら、残差系列に自己相関が見られないこと、正規分布と異なっていないことを確認します。

まず、Ljung-Box検定で残差に有意な自己相関がないことを確認します。Ljung-Box検定を行うには`forecast`パッケージの`checkresiduals()`関数や`stats`パッケージの`Box.test()`関数を使用します。`checkresiduals()`関数はLjung-Box検定の結果に加えて残差の折れ線グラフ、自己相関プロット（ACF）、ヒストグラムが出力されるので便利です。

どちらの結果もp値が有意水準の0.05を上回っており、残差に有意な自己相関が見られないことが確認できます。

```{r}
forecast::checkresiduals(model_sarimax_auto)
```

```{r}
stats::Box.test(model_sarimax_auto$residuals, # 帰無仮説「自己相関なし」を検定する系列
                lag = 24, # ラグ次数
                type = "Ljung-Box"
                )
```

次に、Jarque-Bera検定で残差の正規性を確認します。Jarque-Bera検定を行うには`tseries`パッケージの`jarque.bera.test()`関数を使用します。

`jarque.bera.test()`関数の実行結果を見ると、p値が有意水準の0.05を上回り、正規分布と有意に異なっていないと判断できます。これは、先ほど`checkresiduals()`関数で出力された残差のヒストグラムの見た目とも整合的です。

```{r}
tseries::jarque.bera.test(model_sarimax_auto$residuals)
```

#### 7. 予測精度評価 {.unnumbered}

最後にモデルの予測精度評価として、テストデータを使った予測値のRMSEを計算し、ナイーブ予測の精度を上回ることを確認します。

まず、`forecast`パッケージの`forecast()`関数でテストデータを使って予測値を作成します。

```{r}
library(forecast)

forecast_sarimax_auto <- forecast(model_sarimax_auto, # 推定したモデルオブジェクト
                                  xreg = data_test[, c("PetrolPrice", "law")], # 外生変数
                                  h = 12, # 何期先まで予測するか
                                  level = 95 # 出力する予測値の信頼区間。複数出力するときはc(95, 70)などとベクトルで指定
                                  )

forecast_sarimax_auto
```

予測結果をプロットするには、`ggplot2`パッケージの`autoplot()`関数に予測オブジェクトを指定して実行します。テストデータを使った予測期間は、予測値に加えて`forecast()`関数で出力した信頼区間（ここでは95％）も図示されています。

```{r}
ggplot2::autoplot(forecast_sarimax_auto)
```

予測値のRMSE（Root Mean Squared Error）は、`forecast`パッケージの`accuracy()`関数で計算できます。訓練データ（Training set）のRMSEは0.088、テストデータ（Test set）のRMSEは0.104と、テストデータの予測精度（テストスコア）が訓練データの予測精度（訓練スコア）に比べやや悪いことが確認できます。

```{r}
forecast::accuracy(forecast_sarimax_auto, # 予測オブジェクト
                   x = data_test[, "front"] # テストデータの目的変数
                   )
```

ここではテストデータの外生変数を使って予測値を計算しましたが、実際にこのモデルを運用して将来予測を行う場合は外生変数の`PetrolPrice`が将来どのような値をとるか不明なため、予測対象期間の`PetrolPrice`の値を何らか想定する必要があります（`law`は法律の施行フラグであり1983年2月以降は1の値をとり続けます）。

そこで、予測対象期間の`PetrolPrice`の簡易的な想定として、訓練データにおける`PetrolPrice`の平均値と最後の値を用い、`front`の予測値を計算してRMSEを求めます。

`PetrolPrice`の訓練データの平均値を用いて計算した`front`の予測値のRMSEは0.083になります。

```{r}
# テストデータのPetrolPriceを訓練データの平均値で置換
data_test_exmean <- data_test %>% 
  data.frame() %>% 
  dplyr::mutate(PetrolPrice = mean(data_train[, "PetrolPrice"])) %>% 
  ts(frequency = 12, start = c(1984, 1))

# 予測値を計算
forecast_sarimax_auto_exmean <- forecast(model_sarimax_auto, # 推定したモデルオブジェクト
                                         xreg = data_test_exmean[, c("PetrolPrice", "law")], # 外生変数
                                         h = 12, # 何期先まで予測するか
                                         level = 95 # 出力する予測値の信頼区間。複数出力するときはc(95, 70)などとベクトルで指定
                                         )

ggplot2::autoplot(forecast_sarimax_auto_exmean)

# RMSEを計算
forecast::accuracy(forecast_sarimax_auto_exmean, # 予測オブジェクト
                   x = data_test[, "front"] # テストデータの目的変数
                   )
```

`PetrolPrice`の訓練データの最後の値を用いて計算した`front`の予測値のRMSEは0.108になります。

```{r}
# テストデータのPetrolPriceを訓練データの最後の値で置換
data_test_exlatest <- data_test %>% 
  data.frame() %>% 
  dplyr::mutate(PetrolPrice = tail(data_train[, "PetrolPrice"], n = 1)) %>% 
  ts(frequency = 12, start = c(1984, 1))

# 予測値を計算
forecast_sarimax_auto_exlatest <- forecast(model_sarimax_auto, # 推定したモデルオブジェクト
                                           xreg = data_test_exlatest[, c("PetrolPrice", "law")], # 外生変数
                                           h = 12, # 何期先まで予測するか
                                           level = 95 # 出力する予測値の信頼区間。複数出力するときはc(95, 70)などとベクトルで指定
                                           )

ggplot2::autoplot(forecast_sarimax_auto_exlatest)

# RMSEを計算
forecast::accuracy(forecast_sarimax_auto_exlatest, # 予測オブジェクト
                   x = data_test[, "front"] # テストデータの目的変数
                   )
```

最後に、予測精度の比較対象としてナイーブ予測による予測値を計算します。ここではナイーブ予測として、過去の平均値を予測値として用いるモデルと、前時点の値（訓練データの最後の値）を予測値として用いるモデルの2つを考え、RMSEを計算します。

過去の平均値による予測は`forecast`パッケージの`meanf()`関数を、前時点の値による予測は同じく`forecast`パッケージの`rwf()`関数を使用します。

ナイーブ予測では、過去の平均値による予測モデルのテストデータRMSEは0.395、前時点の値による予測モデルのテストデータRMSEは0.150になりました。それに比べると$SARIMA(2,1,4)(2,0,0)[12]$モデルでは、`PetrolPrice`の訓練データの平均値を用いて計算した予測値のRMSEが0.083、同様に`PetrolPrice`の訓練データの最後の値を用いて計算した予測値のRMSEが0.108と、どちらもナイーブ予測モデルのRMSEより良好であり、$SARIMA(2,1,4)(2,0,0)[12]$モデルによる予測に付加価値があることが確認できます。

```{r}
# 過去の平均値による予測
forecast_mean <- forecast::meanf(y = data_train[, "front"], # 訓練データの目的変数
                                 h = 12, # 何期先まで予測するか
                                 level = 95 # 出力する予測値の信頼区間。複数出力するときはc(95, 70)などとベクトルで指定
                                 )

ggplot2::autoplot(forecast_mean)

forecast::accuracy(forecast_mean, # 予測オブジェクト
                   x = data_test[, "front"] # テストデータの目的変数
                   )
```

```{r}
# 前時点の値（訓練データの最後の値）による予測
forecast_latest <- forecast::rwf(y = data_train[, "front"], # 訓練データの目的変数
                                 h = 12, # 何期先まで予測するか
                                 level = 95 # 出力する予測値の信頼区間。複数出力するときはc(95, 70)などとベクトルで指定
                                 )

ggplot2::autoplot(forecast_latest)

forecast::accuracy(forecast_latest, # 予測オブジェクト
                   x = data_test[, "front"] # テストデータの目的変数
                   )
```

以上で、Box-Jenkins法による分析の流れは終了です。

## 共和分

### 見せかけの回帰 {.unnumbered}

2つの無関係な$I(1)$過程データを用いて回帰分析を行うと、データ間に関係がないにも関わらず回帰係数のt値や決定係数が大きくなり、「2つのデータに関係がある」と誤って判断してしまうことがあります。これを**見せかけの回帰（spurious regression）**といいます。

見せかけの回帰を判断するポイントの一つは、回帰分析の結果として得られる残差の系列相関に関する診断です。見せかけの回帰では残差が大きな系列相関を持つ症状があります。偏自己相関プロットやLjung-Box検定で残差が系列相関を持つことが確認できれば、見せかけの回帰であることを疑うべきです。

### 共和分関係 {.unnumbered}

一方、見せかけの回帰とは異なり、複数の$I(1)$過程のデータの間に意味のある関係を見出すことができる場合があります。そうした関係を**共和分（cointegration）**といいます。

共和分は、$I(1)$過程の2つのデータ$Y_t \sim I(1)$と$X_t \sim I(1)$の線型結合が$I(0)$過程に従うこと、すなわち、

$$
\beta_1 Y_t + \beta_2 X_t \sim I(0)
$$

を満たす$\beta_1$と$\beta_2$が存在することと定義されます。これは、次の回帰式

$$
Y_t = \mu + \theta X_t + u_t
$$

を最小2乗法（OLS）で推定して得られる残差$\hat{u}_t = Y_t - \hat{\mu} - \hat{\theta} X_t$が$I(0)$過程に従うことを意味します。したがって、見せかけの回帰と共和分は「残差が$I(1)$過程であれば見せかけの回帰」、「残差が$I(0)$過程であれば共和分」と区別できます。

### Engle-Grangerの方法 {.unnumbered}

見せかけの回帰では回帰残差が単位根をもつ$I(1)$過程、共和分関係では回帰残差が定常（$I(0)$過程）になるため、残差に単位根検定を行なって「単位根あり」の帰無仮説を棄却できなければ「見せかけの回帰」、「単位根あり」の帰無仮説を棄却できれば「共和分関係」であると判断できます。

このように、回帰残差に単位根検定を適用して「見せかけの回帰」か「共和分関係」かを判断する方法を、**エングル＝グレンジャー（Engle-Granger）の方法**と言います。

なお、残差に対する単位根検定では通常のADF検定が使えないため、**Phillips-Ouliaris検定（PO検定）**と呼ばれる単位根検定の手法を用います（馬場（2018）P.142）。

### 実証例 西村 12.3 見せかけの回帰 {.unnumbered}

次の例は、西山 他（2019）P.592に掲載されている実証例12.3を参考に、日本の実質GDPデータを南極のペンギンの数データに回帰したものです。データ年次で、期間は1982〜2014年です。

```{r}
# XLSXデータを読み込み
data <- readxl::read_excel(path = "data_nishiyama/ch12/Fig_2_penguin.xlsx", # ファイルパス（拡張子が必要、URLは不可）
                           sheet = "Figure", # シートインデックス／シート名
                           col_names = c("date", "gdp", "penguin"), # ヘッダー（列名データ）の有無／列名指定
                           col_types = NULL, # 各列の型の指定（c：文字列型、d：数値型、D：日付型、l：論理値型）
                           skip = 1 # 読み込み時に上からスキップする行数
                           )

# ts()関数でtibble形式のデータをts形式に変換
data_ts <- ts(data %>% dplyr::select(-date),
              start = min(data$date),
              end = max(data$date),
              frequency = 1
              )
```

`forecast::ggtsdisplay()`関数で折れ線グラフと自己相関・偏自己相関プロットを図示します。

```{r}
forecast::ggtsdisplay(data_ts[, "gdp"], main = "日本の実質GDP")
```

```{r}
forecast::ggtsdisplay(data_ts[, "penguin"], main = "南極のペンギン数")
```

まず、実質GDPデータとペンギン数データそれぞれについて`adf_test_flow()`自作関数で単位根検定を行うと、どちらも$I(1)$過程と判断できます。

```{r}
# 日本の実質GDPデータのの単位根検定
adf_gdp <- adf_test_flow(y = data$gdp, # 検定対象の時系列データ
                          selectlags = "AIC", # ラグ次数選択基準
                          lags = 10, # AICで自動選択する最大ラグ次数
                          sig_level = 0.05 # 有意水準（％）
                          )

adf_gdp
```

```{r}
# ペンギン数データの単位根検定
adf_penguin <- adf_test_flow(y = data$penguin, # 検定対象の時系列データ
                          selectlags = "AIC", # ラグ次数選択基準
                          lags = 10, # AICで自動選択する最大ラグ次数
                          sig_level = 0.05 # 有意水準（％）
                          )

adf_penguin
```

次に、実質GDPをペンギン数に回帰すると、両者には関係がないと考えられるにも関わらず、ペンギン数の回帰係数は5％の有意水準で有意に正であり、自由度修正済み決定係数は0.50とまずまずの大きさになっています。

```{r}
# stats::lm()関数でgdpをpenguinに回帰
model_lm <- stats::lm(formula = gdp ~ penguin, 
                      data = data
                      )

# 回帰した結果を出力
summary(model_lm)

# ホワイトの標準誤差で係数の仮設検定を実施
lmtest::coeftest(model_lm,
                 vcov. = sandwich::vcovHC(model_lm, type = "HC1"))
```

回帰分析の結果を回帰診断プロットで視覚的に検証します。Residuals vs Fittedでは予測値に対し残差が一様に分布しておらず、残差に系列相関があることが示唆されます。

```{r}
# 回帰診断プロット
par(mfrow = c(2, 2))
model_lm %>% plot()
```

確認のために残差系列の偏自己相関プロット（PACF）を作成すると、ラグ次数1次の偏自己相関係数が有意に正であり、正の系列相関があることがわかります。

また、Ljung-Box検定（ラグ次数1）の結果を見るとp値が0.05を下回り、5％の有意水準で残差が系列相関をもつと判断できます。これらの残差に関する診断は、日本の実質GDPと南極のペンギン数の関係が見せかけの回帰であることを支持しています。

```{r}
resid <- model_lm$residuals

# 残差の偏自己相関プロット
stats::pacf(resid, plot = TRUE)

# 残差のLjung-Box検定
stats::Box.test(resid, # 帰無仮説「自己相関なし」を検定する系列
                lag = 1, # ラグ次数
                type = "Ljung-Box"
                )
```

最後に、Engle-Grangerの方法（PO検定）で残差の単位根検定を行います。PO検定を行うには`urca`パッケージの`ca.po()`関数を使用します。

`ca.po()`関数の第一引数`z`には時系列データを行列形式で指定します。このとき、行列の列の順番に注意してください。第1列が回帰分析における非説明変数、第2列が説明変数に該当します。順番が異なるとPO検定の結果が異なる場合があります。

`demean`引数には、モデルがトレンド項と定数項の両方を含む場合は`trend`、定数項のみ含む場合は`constant`、トレンド項も定数項も含まない場合は`none`を指定します。

PO検定の結果、検定統計量（`Value of test-statistic`）が2.812であり、有意水準5％の棄却点48.8439より小さいことから、「単位根あり」の帰無仮説を棄却することができません。したがって日本の実質GDPと南極のペンギン数の関係は「共和分関係」ではなく「見せかけの回帰」であることが示唆されます。

```{r}
# データセットからgdp、penguinの順で選択して行列形式に変換
data_mat <- data %>% 
  dplyr::select(gdp, penguin
                ) %>% 
  as.matrix()

# PO検定
urca::ca.po(z = data_mat, # データを格納した行列（非説明変数、説明変数の順）
            demean = "trend" # 検定モデル（"trend", "constant", "none"）
            ) %>% 
  summary()
```

### 実証例 西村 12.4 共和分 {.unnumbered}

次の例は、西山 他（2019）P.597に掲載されている実証例12.4を参考に、長期金利を短期金利に回帰したものです。データ月次で、期間は1986年7月〜1995年9月です。

長期金利と短期金利はともに単位根過程ですが、その差である長短金利スプレッドは、リスク・プレミアムが定数である限り定常であることが示唆されます。この時、長期金利と短期金利の間に共和分関係が存在します。

```{r}
# XLSXデータを読み込み
data <- readxl::read_excel(path = "data_nishiyama/ch12/Fig_1_longshortspread.xlsx", # ファイルパス（拡張子が必要、URLは不可）
                           sheet = 1, # シートインデックス／シート名
                           col_names = c("date", "call", "yield10", "spread"), # ヘッダー（列名データ）の有無／列名指定
                           col_types = NULL, # 各列の型の指定（c：文字列型、d：数値型、D：日付型、l：論理値型）
                           skip = 1# 読み込み時に上からスキップする行数
                           )

# ts()関数でtibble形式のデータをts形式に変換
data_ts <- ts(data %>% dplyr::select(-date),
              start = c(1986, 7),
              end = c(1995, 9),
              frequency = 12
              )
```

`forecast::ggtsdisplay()`関数で折れ線グラフと自己相関・偏自己相関プロットを図示します。

```{r}
forecast::ggtsdisplay(data_ts[, "call"], main = "短期金利（コールレート）")
```

```{r}
forecast::ggtsdisplay(data_ts[, "yield10"], main = "長期金利（10年物国債利回り）")
```

短期金利データと長期金利データそれぞれについて`adf_test_flow()`自作関数で単位根検定を行うと、どちらも$I(1)$過程と判断できます。

```{r}
# 短期金利データのの単位根検定
adf_call <- adf_test_flow(y = data_ts[, "call"], # 検定対象の時系列データ
                          selectlags = "AIC", # ラグ次数選択基準
                          lags = 12, # AICで自動選択する最大ラグ次数
                          sig_level = 0.05 # 有意水準（％）
                          )

adf_call
```

```{r}
# 長期金利データのの単位根検定
adf_yield10 <- adf_test_flow(y = data_ts[, "yield10"], # 検定対象の時系列データ
                             selectlags = "AIC", # ラグ次数選択基準
                             lags = 12, # AICで自動選択する最大ラグ次数
                             sig_level = 0.05 # 有意水準（％）
                             )

adf_yield10
```

次に、長期金利を短期金利に回帰すると、短期金利の回帰係数は5％の有意水準で有意に正であり、自由度修正済み決定係数は0.82と高い水準です。

```{r}
# stats::lm()関数でgdpをpenguinに回帰
model_lm <- stats::lm(formula = yield10 ~ call, 
                      data = data
                      )

# 回帰した結果を出力
summary(model_lm)

# ホワイトの標準誤差で係数の仮設検定を実施
lmtest::coeftest(model_lm,
                 vcov. = sandwich::vcovHC(model_lm, type = "HC1"))
```

残差系列の偏自己相関プロット（PACF）を作成すると、ラグ次数1〜3次の偏自己相関係数が有意であり、系列相関があることがわかります。また、Ljung-Box検定（ラグ次数3）の結果を見るとp値が0.05を下回り、5％の有意水準で残差が系列相関をもつと判断できます。

```{r}
resid <- model_lm$residuals

# 残差の偏自己相関プロット
stats::pacf(resid, plot = TRUE)

# 残差のLjung-Box検定
stats::Box.test(resid, # 帰無仮説「自己相関なし」を検定する系列
                lag = 3, # ラグ次数
                type = "Ljung-Box"
                )
```

最後に、Engle-Grangerの方法（PO検定）で残差の単位根検定を行います。PO検定の結果、検定統計量（`Value of test-statistic`）が57.429であり、有意水準5％の棄却点48.844より大きいことから、「単位根あり」の帰無仮説を棄却します。したがって長期金利と短期金利の関係は「共和分関係」であると判断できます。

```{r}
# データセットからyield10、callの順に選択して行列形式に変換
data_mat <- data %>% 
  dplyr::select(yield10, call
                ) %>% 
  as.matrix()

# PO検定
urca::ca.po(z = data_mat, # データを格納した行列（非説明変数、説明変数の順）
            demean = "trend" # 検定モデル（"trend", "constant", "none"）
            ) %>% 
  summary()
```

## VARモデル

**ベクトル自己回帰（Vector AutoRegression、VAR）モデル**は、多変量時系列モデルの一種です。VARモデルでは、将来予測やシミュレーションに加えて、複数の時系列データ間の相互作用を分析することができます。

### レベルVARモデル {.unnumbered}

変数$Y_t$と$X_t$をモデル化したラグ次数1のVARモデル$VAR(1)$は、次のように表すことができます。$t$期の変数$Y_t$と$X_t$が、それぞれ$t-1$期の$Y_{t-1}$と$X_{t-1}$によって説明されています。

$$
Y_t = c_1 + \phi_{11}Y_{t-1} + \phi_{12}X_{t-1} + u_{1t} \\
X_t = c_2 + \phi_{21}Y_{t-1} + \phi_{22}X_{t-1} + u_{2t}
$$

撹乱項$u_{1t}$と$u_{2t}$は定常なホワイトノイズであり、過去の自身の撹乱項とは相関していません。ただし、同時点の撹乱項同士の相関は許容します。

このとき、変数$Y_t$と$X_t$はどちらも$I(0)$過程であることが前提になっています。定常過程の線型結合は必ず定常過程になるため（すなわち、ここでは$\beta_1Y_t + \beta_2X_t \sim I(0)$）、変数が全て定常であればVARモデルの各式の左辺・右辺がどちらも$I(0)$になり、VARモデル全体が定常になります（村尾（2019）P.94）。このように、定常な時系列データを原系列のまま用いて構築するVARモデルを**レベルVARモデル**といいます。

一般的に、$n$個の変数を持つラグ次数$p$のVARモデルを$VAR(p)$と表記します。ここで、$\boldsymbol{Y}_t$は$n$個の変数ベクトル、$\boldsymbol{c}$は$n \times 1$の定数ベクトル、$\boldsymbol{\phi}_t$は$n \times n$個の係数行列です。

このレベルVARモデル$VAR(p)$が定常になるには、$\boldsymbol{Y}_t$の$n$個の変数が全て定常であることが必要です。

$$
\boldsymbol{Y}_t = \boldsymbol{c} + \boldsymbol{\phi}_1\boldsymbol{Y}_{t-1} + \dots + \boldsymbol{\phi}_{p}\boldsymbol{Y}_{t-p} + \boldsymbol{u}_t
$$

### 階差VARモデル {.unnumbered}

変数$Y_t$と$X_t$でレベルVARモデルを構築する際、変数が非定常な$I(1)$過程のケースを考えます。

-   1つの変数が$I(1)$のケース

    　このケースでは、変数そのものが$I(1)$であることに加え、変数の線型結合$\beta_1Y_t + \beta_2X_t$も必ず$I(1)$過程になります。

-   全ての変数が$I(1)$過程のケース

        このケースでは、変数$Y_t$と$X_t$がどちらも$I(1)$過程であり、さらに両者の間に共和分の関係がない場合（すなわち「見せかけの回帰」の場合）には変数の線形結合$\beta_1Y_t + \beta_2X_t$も$I(1)$過程になります。

VARモデルの各式の項に一つでも$I(1)$過程の項があれば、VARモデル全体が非定常になります。非定常なVARモデルには標準的な推定法が使えないため、VARモデル全体を定常化する必要があります（村尾（2019）P.101）。

上記の2つのケースでVARモデルを定常化するには、Box-Jenkins法におけるARMAモデルとARIMAモデルの関係と同様に、変数を階差変換します。変数を階差変換すると、変数$Y_t$、$X_t$に加えて、線形結合$\beta_1\Delta{Y_t} + \beta_2\Delta{X_t}$が$I(0)$になり、VARモデル全体が定常になります（村尾（2019）P.96）。このように、時系列データを階差変換して構築するVARモデルを**階差VARモデル**といいます。

### ベクトル誤差修正モデル {.unnumbered}

変数$Y_t$と$X_t$がどちらも$I(1)$過程であり、かつ両者の間に共和分の関係がある場合は、変数の線型結合$\beta_1Y_t + \beta_2X_t$が$I(0)$過程になります。こうした共和分の関係は、変数$Y_t$と$X_t$の間に経済学的に意味がある均衡関係が成立していると解釈できます。

このとき、変数$Y_t$と$X_t$そのものは$I(1)$変数なので、レベルVARモデルは定常になりません。しかし、定常性を確保するために階差変換して階差VARモデルを構築すると、共和分による均衡関係の情報が失われてしまいます。

このようなケースにおいて、共和分の関係を活かしつつモデルを定常化する方法が、**ベクトル誤差修正（Vector Error Correction、VEC）モデル**です（村尾（2019）P.101）。

まず、$n$個の変数を持つラグ次数2のVARモデル：$VAR(2)$を考えます。

$$
\boldsymbol{Y}_t = \boldsymbol{B}_1\boldsymbol{Y}_{t-1} + \boldsymbol{B}_{2}\boldsymbol{Y}_{t-2} + \boldsymbol{u}_t
$$

この両辺から$\boldsymbol{Y}_{t-1}$を引いて式を整理すると、

$$
\Delta \boldsymbol{Y}_t = \boldsymbol{\Pi}\boldsymbol{Y}_{t-1} + \boldsymbol{\Gamma}_{1}\Delta\boldsymbol{Y}_{t-1} + \boldsymbol{u}_t
$$ となります。なお、$\boldsymbol{\Pi} = - \boldsymbol{I}_n + \boldsymbol{B}_1 + \boldsymbol{B}_2$、$\boldsymbol{\Gamma}_1 = - \boldsymbol{B}_2$であり、また$\boldsymbol{I}_n$は$n \times n$の単位行列です。

ここで、変数ベクトルを階差変換した$\boldsymbol{Y}_t$と$\boldsymbol{Y}_{t-1}$は$I(0)$であり、レベル型の線形結合$\boldsymbol{\Pi}\boldsymbol{Y}_{t-1}$も$I(0)$になります。したがって、このモデル全体で定常化が達成されています。

さらに、係数行列$\boldsymbol{\Pi}$が$\boldsymbol{\Pi} = \boldsymbol{\alpha}\boldsymbol{\beta'}$に分解できるため、上の式は次のように書き換えることができ、

$$
\Delta \boldsymbol{Y}_t = \boldsymbol{\alpha}\boldsymbol{\beta'}\boldsymbol{Y}_{t-1} + \boldsymbol{\Gamma}_{1}\Delta\boldsymbol{Y}_{t-1} + \boldsymbol{u}_t
$$

これをベクトル誤差修正（VEC）モデルと呼びます。

この場合はラグ次数1のVECモデルのため、$VEC(1)$と表記します。このように、非定常なラグ次数2のレベルVARモデル：$VAR(2) \sim I(1)$を、共和分関係を利用することで、定常なラグ次数1次のVECモデル：$VEC(1) \sim I(0)$に変換することができます。

ベクトル誤差修正モデルには3つの要素があります。

-   $\boldsymbol{\beta'}\boldsymbol{Y}_{t-1}$：**誤差修正項（error correction term）**は、共和分関係で表される均衡関係からの一時的な乖離を表していると解釈できます。

-   $\boldsymbol{\beta}$：**共和分行列（cointegration matrix）**は、$n$個の内生変数から共和分関係（均衡関係）を選び出す役割があります。

* $\boldsymbol{\alpha}$：**調整行列（adjusting matrix）**は、一時的な乖離が均衡へ向かうスピードを表しています。

### 構造VARモデル {-}

**構造VAR（Structural VAR、SVAR）モデル**は、通常のVARモデルに理論的な制約を加え、モデルで発生する外性的なショックを「実物ショック」や「政策ショック」のように経済学的に解釈可能な「構造ショック」として識別するための方法です（村尾（2019）P.88、宮尾（2006）P.17）。
