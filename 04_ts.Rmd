# 時系列データ操作

第4章「時系列データ操作」では、時系列データに特有のデータ操作方法について解説します。時系列データ特有の操作とは、変化率やラグなどのデータ変換、日次・月次などの頻度変換、季節調整、トレンド推定などです。

Rで時系列データを扱う方法には、主に次の2つがあります。

まず、第3章で使用した**tibble形式**です。tibble形式は複数の列を含むデータフレームの形状をしており、データそのものを格納する列と、日付型データを格納する列を組み合わせることで、時系列データを扱うことができます。

もう一つは**ts型**です。ts型はデータと日付があらかじめセットになった一次元のデータ構造で、季節調整を行う`seasonal`パッケージなどで使用されます。

## 第4章の準備

### パッケージのインポート {.unnumbered}

```{r}
library(forecast)
library(mgcv)
library(pbapply)
library(seasonal)
library(seasonalview)
library(tidyquant)
library(tidyverse)
library(zoo)
```

### 外部データセットの取得 {.unnumbered}

この章では、外部データセットとして以下のデータを使用します。第1章のコードを使用してあらかじめウェブからデータセットを取得してください。

-   OWIDのCOVID-19データセット：　`data_owid`
-   日本の産業別就業者数：　`data_labor`

## データ変換

ここでは、時系列データのラグ・リード系列、変化率、移動平均を計算する方法を解説します。

まず、Our World in Dataの`data_owid`データセットから使用するサンプルデータを作成します。

```{r}
data_owid_jp <- data_owid %>% 
  dplyr::select(location, date, new_cases, new_deaths) %>% 
  dplyr::filter(location == "Japan",
                date >= "2022-01-01")
```

### ラグ・リード系列の作成 {.unnumbered}

`dplyr::lag()`関数と`dplyr::leag()`関数で、既存の列のラグ・リード系列を作成します。

```{r}
# 1期ラグの系列を追加
data_owid_jp %>% 
  dplyr::mutate(new_cases_lag = dplyr::lag(new_cases, n = 1)) 
```

### 変化率系列の作成 {.unnumbered}

`dplyr::lag()`関数で、既存の列の変化率系列を作成します。

```{r}
# 前期比変化率（％表示）の系列を追加
data_owid_jp %>% 
  dplyr::mutate(new_cases_chg = 100 * (new_cases / dplyr::lag(new_cases, n = 1) - 1)) 
```

### 移動平均系列の作成 {.unnumbered}

`zoo`パッケージの`rollmean()`関数で、移動平均系列を作成します。

```{r}
# 後方7日移動平均の系列を追加
data_owid_jp %>% 
  dplyr::mutate(new_cases_7dma = zoo::rollmean(new_cases, # 移動平均を作成するもとの系列名
                                               k = 7, # 移動平均の期間
                                               na.pad = TRUE, # 系列の先端部分で移動平均を計算できない箇所をNAで埋めるか
                                               align = "right")) # left：前方移動平均、center：中央移動平均、right：後方移動平均
```

## 時系列データの頻度変換

`tidyverse`と整合性がある金融時系列データ分析用の`tidyquant`パッケージに含まれる`tq_transmute()`関数を用いて、時系列データの頻度変換（高頻度データから低頻度データへの変換）を行います。

なお、`tidyquant`は頻度変換以外にも様々な分析機能があります。詳しくは[公式ウェブサイト](https://business-science.github.io/tidyquant/)を参照してください。

まず、Our World in Dataの`data_owid`データセットから、使用するサンプルデータを作成します。`tq_transmute()`関数に入力する時系列データは、原則として横型データである点に留意してください。

```{r}
# サンプルデータ（日次）
data_owid_cases_wide <- data_owid %>% 
  dplyr::select(location, date, new_cases) %>% 
  dplyr::filter(date >= "2021-01-01") %>% 
  dplyr::arrange(date) %>% 
  tidyr::pivot_wider(id_cols = "date", names_from = "location", values_from = "new_cases")
```

### 日次データを週次データに変換 {.unnumbered}

`tidyquant::tq_transmute()`関数を使用して日次データを週次データに変換すると、月曜～日曜のデータが`FUN`に指定した関数で集計され、日曜の日付で記録されます。

```{r}
data_owid_cases_wide %>% 
  tidyquant::tq_transmute(select = -date, mutate_fun = apply.weekly, FUN = mean, na.rm = TRUE)
```

なお、日曜～土曜のデータを集計し日曜の日付で記録したい場合は、`rollmean()`関数を使用して前方7日移動平均を計算し、日曜の値を抽出します。

```{r}
data_owid_cases_wide %>%
  dplyr::mutate(across(-date, rollmean, k = 7, na.pad = TRUE, align = "left")) %>% 
  dplyr::filter(lubridate::wday(date) == 1)
```

### 日次データを月次データに変換 {.unnumbered}

`tidyquant::tq_transmute()`関数を使用して日次データを月次データに変換すると、月初～月末のデータが`FUN`に指定した関数で集計され、月末の日付で記録されます。

```{r}
data_owid_cases_wide %>% 
  tidyquant::tq_transmute(select = -date, mutate_fun = apply.monthly, FUN = mean, na.rm = TRUE)
```

### 日次データを四半期データに変換 {.unnumbered}

`tidyquant::tq_transmute()`関数を使用して日次データを四半期データに変換すると、期初～期末のデータが`FUN`に指定した関数で集計され、期末の日付で記録されます。

```{r}
data_owid_cases_wide %>% 
  tidyquant::tq_transmute(select = -date, mutate_fun = apply.quarterly, FUN = mean, na.rm = TRUE)
```

## 季節調整（X-13）

ここでは、`seasonal`パッケージを用いた時系列データへの季節調整方法について解説します。

`seasonal`パッケージでは、米国商務省センサス局が開発したX-13ARIMA-SEATSを用いて、ts型の月次データ、四半期データ、半期データに対し季節調整を適用することができます。

`seasonal`パッケージや、X-13ARIMA-SEATSの詳細については、Sax & Eddelbuettel（2018）や、奥本（2016）を参照してください。

サンプルデータとして、ts型データである`seasonal::unemp`データセットを用い、`seasonal`パッケージの使用方法を確認します。

### ts型データの可視化 {-}

季節調整の前に、データを確認します。ts型データは`plot()`関数でグラフを作成できます。

```{r}
plot(seasonal::unemp)
```

### X-13ARIMA-SEATSの実施方法 {.unnumbered}

`seasonal`パッケージでは、`seas()`関数を使用してts型データにX-13ARIMA-SEATSを適用します。`seas()`関数は、季節調整の結果を格納したseas型のオブジェクトを返します。

```{r}
# seas()関数で季節調整を実行
m <- seasonal::seas(x = seasonal::unemp)

# 季節調整の結果を出力
summary(m)
```

`plot()`関数で、原数値と季節調整値のグラフを作成できます。黒色の線が原数値、赤色の線が季節調整値です。外れ値がある場合はグラフ中に外れ値が表示されます。

```{r}
plot(m)
```

季節調整値を出力するには、`seasonal::final()`関数を使用します。

```{r}
# 季節調整値をunemp_saに格納
unemp_sa <- seasonal::final(m)

# unemp_saの折れ線グラフを作成
plot(unemp_sa, type = "l")
```

`monthplot()`関数を使用すると、季節変動（Seasonal component）と不規則変動（Seasonal irregular component）を月別に確認することができます。

```{r}
monthplot(m)
```

### 実例：月次データの季節調整 {.unnumbered}

ここでは、日本の産業別就業者数データセット`data_labor`に対し、月次の季節調整を適用します。`data_labor`はtibble形式のデータフレームの中に、日付型の列と、複数の数値型のデータの列を格納したものです。

1.  tibble形式のデータフレームに格納されている数値型データをts型データに変換する
2.  tibble形式のデータフレームをリスト形式に変換する
3.  `pblapply()`で一括して`seas()`関数の季節調整を適用する。季節調整エラーは`try()`関数で処理する
4.  季節調整エラーを取得する
5.  季節調整値を取得する
6.  エラーが生じた系列は原数値を取得する

```{r}
# data_laborから一部を抽出
data_labor_nsa <- data_labor %>% 
  dplyr::select(date, `総数`, `製造`, `情報通信`)

# 数値型データをts型データに変換
data_labor_ts <- data_labor_nsa %>% 
  dplyr::select(-date) %>% 
  ts(frequency = 12, # 月次データの場合は12を指定
     start = c(lubridate::year(data_labor$date[1]), lubridate::month(data_labor$date[1]))) # データ開始年月を指定

# tibble形式をリスト形式に変換
data_labor_ts %<>% 
  as.list()

# pblapply()関数で一括してseas()関数の季節調整を適用
# 季節調整でエラーが発生する可能性があるため、try()関数でエラー処理を行う
result <- pblapply(data_labor_ts,
                   function(e) try(suppressMessages(seas(e, transform.function = "auto")),
                                   silent = TRUE))

# 季節調整エラーを取得
result_iserror <- sapply(result, class) == "try-error"

# 季節調整値を取得し、tibble形式データフレームのdata_labor_saに格納
data_labor_sa <- do.call(cbind, lapply(result[!result_iserror], final)) %>% 
  tibble::as_tibble() %>% 
  dplyr::bind_cols(data_labor[, 1], .)

# エラーが生じた系列は原数値を取得してdata_labor_saに追加
for (col in which(result_iserror)) {
  data_labor_sa <- data_labor[, names(result[col])] %>% 
    dplyr::bind_cols(data_labor_sa)
}

# data_labor_saの列をdata_labor_nsaの列順で並べ替え
data_labor_sa %<>% 
  dplyr::select(all_of(names(data_labor_nsa)))
```

個別系列の季節調整結果を可視化するには、`plot()`関数を使用します。一部の系列は季節性がないと判断されるため、季節調整が行われていません。

```{r}
plot(result[[which(names(result) == "総数")]])

plot(result[[which(names(result) == "製造")]])

plot(result[[which(names(result) == "情報通信")]])
```

## 季節調整（STL分解）

上で紹介したX-13ARIMA-SEATSは、月次データ、四半期データ、半期データに適用できますが、週次データや日次データには適用できません。そこで、週次データや日次データの季節調整を行うために`stats`パッケージの`stl()`関数による**STL分解**を使用します。

STL分解とは、Seasonal Decomposition of Time Series by Loessの略で、時系列データを季節変動、トレンド変動、不規則変動に分解する手法です。

### STL分解の実施方法 {-}

STL分解を行うには、ts型の時系列データに対し、`stats`パッケージの`stl()`関数を適用します。`stl()`関数はSTL分解の結果を格納したstl型のオブジェクトを返します。

```{r}
# stl()関数でSTL分解を実行
m <- stats::stl(x = seasonal::unemp,
                s.window = "periodic")

# STL分解の結果を可視化
plot(m)
```

季節調整値を出力するには、`stl()`関数が返すstl型オブジェクトに格納されている`time.series`にアクセスします。`time.series`には、季節変動（`seasonal`）、トレンド変動（`trend`）、不規則変動（`remainder`）の順番にデータが格納されています。

```{r}
# 季節調整値（トレンド）をunemp_saに格納
unemp_sa <- m$time.series[, "trend"]

# unemp_saの折れ線グラフを作成
plot(unemp_sa, type = "l")
```

### 週次データのSTL分解 {.unnumbered}

STL分解は、`seasonal::unemp`のような月次データだけでなく、より高頻度なデータにも適用できるのが特徴です。

ここでは、週次データである`fpp2::gasoline`データセットに対し、STL分解を適用します。週次データは`ts()`関数における`frequency`引数が52.18となっています。これは、うるう年を考慮した1年の平均日数365.25日を7で割った値であり、週次データは一周期が52.18週であることを示しています。

```{r}
# データの確認
head(fpp2::gasoline)
```

```{r}
# データの可視化
plot(fpp2::gasoline)
```

```{r}
# stl()関数でSTL分解を実行
m <- stats::stl(x = fpp2::gasoline,
                s.window = "periodic")

# STL分解の結果を可視化
plot(m)
```

### 日次データのSTL分解（単一周期） {.unnumbered}

さらに、STL分解は日次データにも適用可能です。日次データの周期性は通常、週、月、年の3種類あると考えられます。ただし、ts型では一種類の周期性しか指定できないため、ここでは週を一周期に設定し、`ts()`関数の`frequency`引数に7を指定して、STL分解を行います。

使用するデータは、Our World in Dataの`data_owid`データセットにおける、日本の新規感染者数です。

```{r}
# data_owidから日本の新規感染者数（日次）を抽出
data_cases_jp <- data_owid %>% 
  dplyr::filter(location == "Japan") %>% 
  tidyr::drop_na(new_cases) %>%
  dplyr::pull(new_cases)

# 1週＝7日の周期を設定したts型データに変換
data_cases_jp_ts <- data_cases_jp %>% 
  ts(start = c(2020, 1, 22),
     frequency = 7)

# STL分解の結果をmに格納
m <- stats::stl(x = data_cases_jp_ts,
                s.window = "periodic")

# STL分解の結果を可視化
plot(m)
```

### 日次データのSTL分解（複数周期） {.unnumbered}

データに複数の周期性を設定したい場合は、ts型の拡張版であるmsts型データを用います。msts型データは`forecast`パッケージの`msts()`関数で作成できます。

ここでは、日次データに対し、1週間＝7日と、1年＝365.25日の2種類の周期性を設定しています。

msts型データに対するSTL分解は、`stats::stl()`関数ではなく、`forecast`パッケージの`mstl()`関数を使用します。

```{r}
# data_owidから日本の新規感染者数（日次）を抽出
data_cases_jp <- data_owid %>% 
  dplyr::filter(location == "Japan") %>% 
  tidyr::drop_na(new_cases) %>%
  dplyr::pull(new_cases)

# 1週間＝7日と1年＝365.25日の周期を設定したmsts型データに変換
data_cases_jp_msts <- data_cases_jp %>%  
  forecast::msts(seasonal.period = c(7, 365.25),
                 start = c(2020, 1, 22))

# STL分解の結果をmに格納
m <- forecast::mstl(x = data_cases_jp_ts, s.window = "periodic")

# STL分解の結果を可視化
plot(m)
```

## トレンド推定

`seasonal`パッケージのサンプルデータセット`unemp`（米国の失業者数、原数値）を対象に、トレンド推定を行う方法を解説します。

### HPフィルタ {.unnumbered}

HPフィルタ（Hodrick Prescott Filter）は、時系列データのトレンド成分と循環成分を推定する手法です。

具体的には、時系列データがトレンド成分$g_t$と循環成分$c_t$で構成されると仮定し、次の式のように、全期間を通して「循環成分の2乗の総和」と「トレンド成分の2階階差の2乗の総和」の加重和が最小になるような$g_t$を計算します。

ここで、$\lambda$はトレードオフの関係にある2つの項にウェイトをつける調整パラメータです。$\lambda$が大きいほどトレンド成分が直線に近く、$\lambda$が小さいほどトレンド成分が元のデータに近くなります。一般的に、四半期データには$\lambda = 1600$が、月次データには$\lambda = 14400$が使用されます。

$$
\min \Bigg\{ \sum_{t=1}^{T}{c_t^2} + \lambda \sum_{t=1}^{T}[(g_t - g_{t-1})-(g_{t-1}-g_{t-1})]^2 \Bigg\}
$$

HPフィルタを適用するには、時系列データに対し、`mFilter`パッケージの`hpfilter()`関数を使用します。データ型は数値型ベクトル、ts型どちらでもOKです。

推定したトレンド成分、循環成分は、推定結果を格納したオブジェクトに、それぞれ`trend`、`cycle`の名称で格納されています。

```{r}
result <- mFilter::hpfilter(x = seasonal::unemp, # HPフィルタを適用する時系列データ（数値型ベクトル、ts型）
                            type = "lambda",
                            freq = 14400, # ラムダの値（四半期：1600、月次：14400）
                            drift = FALSE) # ドリフト項の有無 

summary(result)
```

```{r, eval=FALSE}
# トレンド推定結果を可視化（コンソールでEnterを押すと表示）
plot(result)
```

### 一般化加法モデル（GAM） {.unnumbered}

一般化加法モデル（Generalizes Additive Model、GAM）は、線形モデルがもつ解釈性の良さを保ったまま、非線形モデルのような高い説明力を両立させる手法です。

具体的には次のように、被説明変数$Y$を、説明変数$X$を要素とする非線形関数$f(X)$の和として説明するモデルです。非線形関数を用いることにより、説明変数と被説明変数の間の複雑な関係を説明できる一方で、被説明変数がそれぞれの非線形関数の和になっていることで、被説明変数の変動要因を説明変数毎に分解することができ、高い解釈性を維持している点が特徴です。

$$
Y_i = \beta_0 + f_1(X_{1i}) + f_2(X_{2i}) + \dots + f_k(X_{ki}) + u_i
$$

ここでは$X$として時系列インデックスを用いることでトレンドを推定しますが、GAMはトレンド推定以外にも様々な用途に使用可能です。例えば、[服部直樹（2020）「新型コロナウイルス感染症（COVID-19）の感染拡大要因は何か」](https://www.mizuho-rt.co.jp/publication/mhri/research/pdf/report/report20-1021.pdf)では、GAMの拡張版である「交互作用項付き一般化加法モデル（GA2M）」を使用して、新型コロナウイルス感染者数の変動要因の説明を試みています（ただし、同レポートではPythonによるGA2Mの実装を用いています）。

一般化加法モデルは、`mgcv`パッケージの`gam()`関数で推定します。事前に時系列インデックスを作成し、`gam()`関数の`formula`引数で`被説明変数 ~ s(時系列インデックス)`と指定します。説明変数である時系列インデックスを`s()`に配置することにより、非線形関数を適用します（`s()`を用いなければ、通常の線形回帰と同じ結果になります）。

`gam()`関数の使用方法の詳細については、[こちら](https://logics-of-blue.com/%E5%B9%B3%E6%BB%91%E5%8C%96%E3%82%B9%E3%83%97%E3%83%A9%E3%82%A4%E3%83%B3%E3%81%A8%E5%8A%A0%E6%B3%95%E3%83%A2%E3%83%87%E3%83%AB/)のウェブサイトを参照してください。

```{r}
# unempデータをベクトル形式に変換してdataに格納
data <- tibble::tibble(
  date = seq(from = as.Date("1990-01-01"), to = as.Date("2016-11-01"), by = "1 month"),
  unemp = seasonal::unemp %>% as.vector()
  )

# dataに時系列インデックスtimeを追加
data %<>% 
  dplyr::mutate(time = seq_along(date))

# GAMでトレンド推定
result <- mgcv::gam(formula = unemp ~ s(time),
                    family = gaussian(), # 分布関数
                    sp = NULL, # 平滑度を決めるパラメータ（大きいほど直線に近く、小さいほど元データに近い。NULLで自動最適化）
                    data = data)

# 推定結果を出力
summary(result)
```

推定したトレンド系列は、推定結果を格納したオブジェクトに`fitted.values`の名称で格納されています。

```{r}
data %>%
  dplyr::mutate(unemp_fitted = result$fitted.values)
```

```{r}
# トレンド推定結果を可視化
plot(result, # gam()関数で推定した結果を格納したオブジェクト
     residuals = TRUE, # 元データを表示するか
     se = TRUE, # 信頼区間を表示するか
     pch = "*") # 元データのマーカー
```

```{r}
# 推定したモデルのチェック
mgcv::gam.check(result)
```
